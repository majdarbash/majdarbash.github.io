[
{
	"uri": "https://majdarbash.github.io/blockchain/blockchain-foundation/",
	"title": "1 - Blockchain Foundation",
	"tags": [],
	"description": "",
	"content": "Consensys developer program\nacademy.consensys.net/developer\nHistory of blockchain\n1990s Stuart Haber / Scott Stornetta\nHow to keep the past data secure and keep digital information safe and resistant to tampering\n 1991 - First paper outlined the use of a chain of cryptographically secured blocks to preserve the integrity of past information and protect it. 1993 - Spam countermeasures 2008 - Bitcoin is born\nSatoshi Nakamoto\nReleased whitepaper:\n“Bitcoin: A Peer-to-Peer Electornic Cash System” 2014 - Ethereum\nCurrency transactions and can run computations.\nDistributed world computer running on a blockchain.\n“Ethereum Virtual Machine” (EVM) 2015-2017 - Financial interest in Bitcoin / Blockchain  From Bitcoin white paper\n\u0026ldquo;A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution.”\nTriple Entry Accounting\nIn addition to standard entries in both books of transaction credit / transaction debit it includes additional entry of recording a transaction on distributed blockchain (similar to auditing).\nLedgers\nBook of transactions\nIf you want to confirm that transaction took place you have to look into books of 2 businesses - credit to one and debit to another. Here comes the concept of double entry accounting.\nDistributed Ledgers\nKeeping copies of ledgers across the internet with specific rules to manipulate them.\nConsensus: Consensus mechanisms ensure that the distributed ledgers of a blockchain stay synchronized.\nBlockchain\nIs a set of linearly connected information-containing blocks secured with cryptography.\nNodes: the computers that run the blockchain software.\nEthereum - free to download blockchain, can be programmed. These applications run across al the nodes of the blockchain - these applications are DApps - Decentralized Applications.\nTokenization - provides a set of instructions for creating digital representation of everything from physical objects to ideas.\nEther (ETH) is the token for Ethereum. Ether allows a user to pay for transactions on the Ethereum blockchain.\n"
},
{
	"uri": "https://majdarbash.github.io/aws-csap/data-stores/",
	"title": "Data Stores",
	"tags": [],
	"description": "",
	"content": " Amazon S3 Amazon EBS (Elastic Block Storage) Amazon EFS (Elastic File System)  EFS Performance Considerations   Amazon Storage Gateway Amazon WorkDocs Database on EC2 Amazon RDS Amazon DynamoDB Amazon Redshift Amazon Neptune Amazon Elasticache Other Database Options  Amazon Athena Amazon Quantum Ledger Database Amazon Managed Blockchain Amazon Timestream Database Amazon DocumentDB Amazon ElasticSearch   Database Comparison   Amazon S3 \u0026ldquo;Secure, durable, highly scalable object storage at a very low cost.1 You can store and retrieve any amount of data, at any time, from anywhere on the web through a simple web service interface. You can write, read, and delete objects containing from zero to 5 TB of data.\u0026rdquo;\nRead More\u0026hellip;\nAmazon EBS (Elastic Block Storage)  To be used with EC2 Bound to a single AZ Use snapshots for backup Snapshots can be shared across different accounts Change AZ by launching a volume in different AZ from snapshot Convert from unencrypted to encrypted volume through snapshot Snapshots consume storage incrementally  AWS reorganized necessary data on snapshot deletion, to guarantee restore-ability of all the snapshots Snapshot lifecycle policies help automate the creation and deletion of the snapshots    Amazon EFS (Elastic File System)  Based on NFS (Network File System) Multi-AZ storage Pay based on the usage Mount points in single or many AZs EFS in not an encrypted protocol, use in caution of mounting over the Internet EFS is as durable and availble as S3  Amazon DataSync - good alternative for syncrhonizing on-premise storage with EFS / S3   Beware of the cost!  EFS is about 3 times more expensive than EBS and about 20 times more expensive than S3    EFS Performance Considerations  Burst credits are allocated over time to control throughput  starts from 2.1TiB with baseline rate of 50MiB/s and a burst rate of 100MiB/s Defined by BurstCreditBalance metric in AWS CloudWatch   Supports different 2 performance modes:  General purpose - default mode for FS with workload up to 7000 IOPS Max I/O - workloads demanding higher than 7000 IOPS, optimized for applications where tens / hundreds / thousands of EC2 instances are accessing the file system  Systems scale to higher level of aggregate throughput Tradeoff on slightly higher latencies for file operations     If application can handle async writes, you can tradeoff consistency for speed, through enabling asynchronous writes.  Read More\u0026hellip;\nAmazon Storage Gateway  Virtual Machine that can run on premise or EC2 Provides local storage resources backed by AWS S3 and Glacier Contains logic to synchronize data back-and-forth to S3 Useful in cloud migrations  Running Modes\n File Gateway  Allow on-prem or EC2 instances to store objects in S3 via NFS or SMB mount point   Volume Gateway Stored Mode / Gateway-stored Volumes  Async replicaiton of data from on-prem to S3, uses iSCSI interface   Volume Gateway Cached Mode / Gateway-cached volumes  Primary data stored in S3 with frequent access data cached locally on-prem, uses iSCSI interface   Tape Gateway / Gateway-Virtual Tape Library  Virtual media changer and tape library for use with existing backup software, uses iSCSI interface    Cost Model Following cost model components should be considered when using AWS Storage Gateway:\n gateway usage snapshot storage usage volume storage usage virtual tape shelf storage virtual tape library storage retrieval from virtual tape shelf data transfer out  Amazon WorkDocs  Amazon\u0026rsquo;s alternative to Dropbox / Google Drive Secure, fully managed file collaboration service Can integrate with AD for SSO Web, mobile and native clients (no Linux client yet) HIPAA, PCI DSS and ISO complant Available SDK for creation complementary apps  Database on EC2  Run any database with full control and ultimate flexibility Self-managed backups, redundancy, patching, scale Good option to run databases not supported by RDS yet  Amazon RDS  Managed database service Supports most-popular database engines Structured, relational databases Automated backups and patching in pre-defined maintenance windows Push-button scaling, replicaiton and redundancy Multi-AZ RDs  Standby instance replication is Synchronous Masters can be promoted at any point of time without data loss   Read replication is asynchronous Read-replicas service regional users MariaDB is open-source fork of MySQL   Note: Non-transactional storage enginers like MyISAM don\u0026rsquo;t support replication; you must use InnoDB (XtraDB on MariaDB)  RDS Anti-Patterns\n Large BLOBS - use S3 Automated scalability - use DynamoDB Name/Value data structure - use DynamoDB Data is not well structure or unpredictable - use DynamoDB Unsupported by RDS database - use EC2 Complete control over the database - use EC2  Amazon DynamoDB  Key-value store Managed multi-AZ NoSQL data store Cross-Region Replication option Defaults to eventual consistency reads SDK supports strong read consistncy via a parameter  May slow down read in case of outages in the write AZ   Priced on throughput  Read/Write Capacity Units   Autoscale capacity adjusts per configured min/max levels  DynamoDB won\u0026rsquo;t scale down   On-Demand Capacity provides flexible capacity at a small premium cost Achieve ACID compliance with DynamoDB Transactions  Relational vs NoSQL\n Relational - structured data NoSQL - self-contained records  NoSQL Indexes\n Primary Key is used to create internal hash Composite Primary Key key consists of partition key and sort key  Can have duplicate of partition keys as long as the sort key is different   Global Secondary Index  If you want a fast query of attributes outside the primary key   Local Secondary Index  You know the partition key adn want to quickly query on some other attibute   There is a limit to the number of indexes and attributes per index Indexes take up storage space  Amazon Redshift  Fully managed, clustered peta-byte scale data warehouse Extremely cost-effective as compared to some other on-premises data warehouse platforms PostgreSQL compatible with JDBC and ODBC drivers available; comptiable with most BI tools out of the box Features parallel processing and columnar data stores which are optimized for complex queries Option to query directly form data files on S3 via Redshift Spectrum  Data Lake\n Large repository for a variety of data Query raw data without extension pre-processing Lessen time from data collection to data value Identify correlations between disparate data sets Data can be located on AWS S3 and queried from BI tools using Amazon Redshift Spectrum  Amazon Neptune  Fully-managed Graph database Optimized to deal with relationships between objects  Allows to store interrelationships and query them in very effective manner   Supports open graph APIs for both Gremlin and SPARQL  Amazon Elasticache  Fully managed implementations of 2 popular in-memory data stores - Redis and Memcached Push-button scalability for memory, writes and reads In Memory key/value store - not persisten in the traditional sense\u0026hellip; Billed by node size and hours of use  Use Cases\n Web Session Store  Stateless application   Database Caching  Offload load from database servers, return results faster to users   Leaderboards  Provide live leaderboard for millions of users in your mobile app   Streaming Data Dashboards  Provide a landing spot for streaming sensor data on the factory floor, providing live real-time dashboard displays.    Redis vs Memcached\nMemcached\n Simple, no-frills, straight-forward You need to scale out and in as demand changes You need to run multiple CPU corers and threads You need to cache objects (i.e. database queries)  Redis\n You need encryption You need HIPAA compliance Support for clustering You need complex datatypes You need high-availability (replication) Pub/Sub capability Geospacial Indexing Backup and Restore  Other Database Options Amazon Athena  SQL Engine overlaid on S3 base on Presto Query raw data objects as they sit in an S3 bucket Use or convert your data to Parquet format if possible for a big performance jump Similar in concept to Redshift Spectrum  Amazon Athena vs Amazon Redshift Spectrum\n Athena: Data lives mostly on S3 without the need to perform joins with other data sources Redshift Spectrum: Want to join S3 data with existing RedShift tables or create union products Supports Apache Parquet, JSON and Apache ORC formats  Amazon Quantum Ledger Database  Based on blockchain concepts Provides immutable and transparent journal as a service without having to setup and maintain an entire blockchain framework Centralized design (as opposed to decentralized consensus-based design for common blockchain frameworks) allows for higher performance and scalability Append-only concept where each record contributes to the integrity of the chain  Amazon Managed Blockchain  Fully managed blockchain framework supporting open source frameworks of Hyperledger Fabric and Ethereum Distributed consensus-based concept consisting of a network members (other AWS accounts), nodes (instances) and potentially applications  Amazon Timestream Database  Fully managed database service specifically built for storing and analyzing time-series data Alternative to DynamoDB or Redshift and includes some built-in analytics like interpolation and smoothing  Use Cases\n Industrial Machinery Sensor Networks Equipment Telemetry  Amazon DocumentDB  with MongoDB compatibility AWS\u0026rsquo;s investion that emulates the MongoDB API so it acts like MongoDB to existing clients and drivers Fully managed with all the good stuff (multi-AZ, HA, scalability, integrated with KMS, S3 backups) An option if you currently use MongoDB and want to get out of the server management  Amazon ElasticSearch  Stores and indexes documents (JSON) Usually referred to as ELK stack:  ElasticSearch - search and storage Kibana - analytics LogStash - intake    Other intake solutions:\n CloudWatch Firehose IoT  Database Comparison  Database on EC2  Ultimate control over database Preferred DB not available under RDS   Amazon RDS  Need traditional database for OLTP Your data is well-formed and structured   Amazon DynamoDB  Name/value pair data or unpredictable data structure In-memory performance with persistence   Amazon Redshift  Massive amounts of data Primary OLAP workloads   Amazon Neptune  Relationships between objects a major portion of data value   Amazon Elasticache  Fast temporary storage for small amounts of data Highly volatile data    "
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/2019-11-7-elements-of-data-science/",
	"title": "Elements of Data Science",
	"tags": [],
	"description": "",
	"content": " What is Data Science?  Types of Learning Key Issues in ML Supervised Methods  Linear Methods Logistic Regression and Linear Separability Linear Separability     Problem Formulation and Exploratory Data Analysis  Data collection  Data Sampling Data Labeling   Exploratory Data Analysis  Domain Knowledge Data Schema Data Statistics Correlations Data Issues     Data Processing and Feature Engineering  Encoding Categorical Variables  Encoding Ordinals Encoding Nominals   Handling Missing Values Feature Engineering  Filtering and Scaling Transformation Text-Based Features     Model Training, Tuning, and Debugging  Supervised Learning: Neural Networks Supervised Learning: K-Nearest Neighbors Supervised Learning: Linear and Non-Linear Support Vector Machines Supervised Learning: Decision Trees and Random Forests Model Training: Validation Set  Splitting Data: Training, Testing, Validation**   Model Training: Bias Variance Tradeoff Model Debugging: Error Analysis Model Tuning: Regularization  Regularization Techniques   Model Tuning: Hyperparameter Tuning Model Tuning  Training Data Tuning Feature Set Tuning   Model Tuning: Feature Extraction Model Tuning: Bagging/Boosting   Model Evaluation and Model Productionizing  Using ML Models in Production Model Evaluation Metrics  Confusion Matrix Metrics Cross Validation K-Fold Cross Validation Leave-one Out Cross Validation Stratified K-fold Cross Validation   Metrics for Linear Regression Using ML Models in Production: Storage  Model and Pipeline Persistence Model Deployment   Using ML Models in Production: Monitoring and Maintenance Using ML Models in Production: Using AWS Common Mistakes    What is Data Science? Data Science Definition Processes and systems to extract knowledge or insights from data, either structured on unstructured. (Wikipedia)\nMachine Learning Artificial Intelligence machines that improve their predictions by learning from large amounts of input data.\nLearning Is the process of estimating underflying function $f$ by mapping data attributes to some target value.\nTraining Set Is a set of labeled examples $(x, f(x))$ where $x$ is the input variarbles and $f(x)$ is the observed target truth.\nGoal Given a training set, find approximation $f^`$ of $f$ that best generalizes, or predicts, labels for new measures. Results are measured by quality, e.g. error rate, sum squared error.\nFeatures Can be defined as Features, Attributes, Independent Variables, Predictors.\nLabel Can be defined as Label, Target, Outcome, Class, Dependent Variable, Response.\nDimensionality Refers to the number of Features.\nTypes of Learning  Supervised Learning  Labeled data Types:  Regression - target type is a continous numerical value Classification - categorical type     Unusupervised Learning  Unlabeled data Grouping / clustering the data   Semi-Supervised Learning  Some data is labeled, some is not   Reinforcement Learning  Apply reward and penalty for each step of ML Algorithm Example, teach ML algorithm to play video games    Key Issues in ML Data Quality\n Consistency of data  Consider the business problem Select subset of data and do some adjustments if required   Accuracy of data Noisy data Missing data  Missing values should be recovered for ML algorithms to work with   Outliers in data  Errors, typos should be refined Outliers should be replaced or removed   Bias Variance, etc.  Model Quality\n Underfitting  Failure to capture the important patterns Model is to simple or there are too few explanatory variables Not flexible enough to model real patterns Correponds to high bias (results show systematic lack of fit in certain regions)   Overfitting  Failure to generalize Indicates that the Model is too flexible Overreacting to the noise Correponds to high variance (small change in training data correponds to big change in the results)    Computation Speed and Scalability AWS SageMaker\n Increase speed Solve prediction time complexity Solve space complexity  Supervised Methods Linear Methods  $f(x) = \\Phi(W^T X)$ where $\\Phi()$\tis some activation function. Weights are optimized by applying (stochastic) gradient descent to minimize the Loss Function $\\sum_{}\\left\\lvert{\\hat{Y}_i - Y_i}\\right\\rvert ^ 2 $ Methods  Linear regression for numeric target outcome Logistic regiression for categorical target outcome    Univariate Linear Regression\n Simplest model One explanatory variable $X$ One target variable $Y$ Goal is to find a line that minimizes the Sum of Squared Errors (SSE) Finding a line is basically finding an intercept and slope, represented by $w_0$ and $w_1$  Multivariate Linear Regression\n Expansion of Univariate Linear Regression Includes $N$ explanatory variables Sensitive to correlation between features, resulting in high variance in coefficients  Logistic Regression and Linear Separability  Logistic Regression is a ML algorithm with the binary result (0/1) Estimates the probability of the input belonging to one or two classes: positive and negative Logistic Regression is vulnerable to outliers Sigmoid curve is a representation of probability $\\sigma(x) = \\frac{1}{1+e^-x}$  Value can be any value, but the output is always between 0 and 1 The threshold should be defined, e.g. 0.5 for predicting the label to the new observation    Intermediary variable $z$ will be a linear combination of features and used with the sigmoid function\n$$z = W^T = w_0 + w_1 x_1 + w_2 x_2 + w_n x_n $$\n$$\\sigma(z) = \\frac{1}{1 + e^-z}$$\nLogistic regression finds the best weight vector by fitting the training data\n$$ logit(p(y = 1 \\vert x)) = z $$\nwhere\n$$ logit = log ( \\frac{p}{1 - p} ) $$\nLinear Separability  Data set is considered as linearly separable if the features can be separated in a linear fashion Logistic regression can be easily applied in linearly separable data set   Problem Formulation and Exploratory Data Analysis  What is the problem we need to solve? What is the business metric?  Measure quality Measure impact of the solution   Is ML the appropriate approach?  Can the problem be solved with rules or standard coding? Are the patterns too difficult to capture algorithmically? Is there a lot of data available from which to induce patterns?   What data is available?  What are the data sources? What is the gap between desired data and actual data that is available? How to add more data?   What type of ML problem is it?  Characterize the ML model according to the dimensions Decompose the business model into a few models   What are your goals?  Technical ML goals Define the criteria for successful outcome of the project    Data collection AWS provides a comprehensive tool kit for sharing and analyzing data at any scale\n Open Data on AWS  you can discover and share data sets   Data Sampling  Selecting a subset of instances, examples, data points for training and testing   Data Labeling  Obtaining a gold-standard answers for supervised learning    Data Sampling  Representatively  Sample needs to be representative of the expected production population Sample be unbiased    Sampling Methods\n Random Sampling  Each data point has an equal probability of being selected Disadvantage: rare populations may miss, and lead to being under-represented   Stratified Sampling  Apply Random Sampling to each subpopulation separately    Issues with Sampling\n Seasonality (time of the day, day of the week, time of the year, etc..)  Stratified Sampling can minimize bias Visualizations may help   Trends (patterns can shift over time, new patterns can emerge)  To detect, try comparing models trained over different time periods Visualizations may help   Leakage: Train/Test Bleed  Inadvertent overlap of training and test data when sampling to create data sets Running tests on test set will end up showing better results then if the test set was independent   Leakage: Using information during training or validation that is not available in production  Consider using validation data that was gathered after your training data was gathered.\nData Labeling Labeling Components\n Labeling Guidelines  Instructions to Labelers Critical to get right Minimize ambiguity   Labeling Tools  Technology  Excel Spreadsheets Amazon Mechanical Turk Custom-built tools     Questions  Human Intelligence Tasks (HITs) should be:  Simple Unambiguous      Amazon Mechanical Turk\n Provides Human Intelligence on Demand Global, on-demand, 24x7 workforce Use for labeling  Managing Labelers\n Motivation Plurality  Assign each HIT to multiple labelers to identify difficult or ambigous cases, or problematic labelers (lazy, confused, biases)   Gold Standard HITs  HITs with known labels mixed in to identify problematic labelers   Auditors Labeler Incentives  Compensation Rewards Voluntary Gamification   Quality and productivity metrics  Sampling and Treatment Assignment\n Random Sampling  Random Assignment  Ideal experiments: Causal conclusion can be generalized   No Random Assignment  Typical survey or observation studies: Cannot establish causation but can establish correlation and can be generalized     No Random Sampling  Random Assignment  Most Experiments: Causal conclusion for the sample only   No Random Assignment  Badly-designed survey or pooled studies: Cannot establish neither causation nor correlation, cannot generalize to larger population      Exploratory Data Analysis Domain Knowledge  Understand how domains works, important dynamics and relationship constraints, how data is generated, etc. Better understanding of domain leads to better features, better debugging, better metrics, etc.  Amazon ML Solutions Lab\n Developing Machine Learning skills through collaboration and education  Brainstorming Custom Modeling Training On-Site with Amazon experts    Data Schema  Types of Data:  Categorical Ordinal Numerical Date Vector Text Image Un-structured    Pandas DataFrame Merge/Join\nimport pandas as pd df = pd.DataFrame({\u0026quot;Name\u0026quot;: [\u0026quot;John\u0026quot;, \u0026quot;Bob\u0026quot;, \u0026quot;Jim\u0026quot;, \u0026quot;Kate\u0026quot;], \u0026quot;Job\u0026quot;: [\u0026quot;Accountant\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Marketing\u0026quot;]}) df_1 = pd.DataFrame({\u0026quot;VP\u0026quot;: [\u0026quot;Tom\u0026quot;, \u0026quot;Andy\u0026quot;, \u0026quot;Kate\u0026quot;], \u0026quot;Job\u0026quot;: [\u0026quot;Accountant\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Marketing\u0026quot;]}) df_merged = df.merge(df_1, on=\u0026quot;Job\u0026quot;, how=\u0026quot;inner\u0026quot;) print(df_merged) Data Statistics  Look into each Feature one at a time Assess Interactions between the Features (relationships)  Descriptive Statistics\n Overall statistics  Number of instances (rows) Number of attributes (columns)   Attribute statistics (univariate)  Statistics for numeric attributes (mean, variance, etc.) - df.describe() Statistics for categorical attributes (histogram, mode, most/least frequent values, percentage, number of unique values)  Histogram of values: df[].value_counts() or seaborn\u0026rsquo;s distplot()   Target Statistics  Class distribution: E.g. df[].value_counts() or np.bincount(y)     Multivariate Statistics  Correlation Contingency Tables/Cross Tabulation    import pandas as pd import seaborn as sb import matplotlib.pyplot as plt df = pd.DataFrame({\u0026quot;Name\u0026quot;: [\u0026quot;John\u0026quot;, \u0026quot;Bob\u0026quot;, \u0026quot;Jim\u0026quot;, \u0026quot;Kate\u0026quot;], \u0026quot;Job\u0026quot;: [\u0026quot;Accountant\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Marketing\u0026quot;], \u0026quot;Salary\u0026quot;: [1000, 2500, 2750, 1800]}) df_1 = pd.DataFrame({\u0026quot;VP\u0026quot;: [\u0026quot;Tom\u0026quot;, \u0026quot;Andy\u0026quot;, \u0026quot;Kate\u0026quot;], \u0026quot;Job\u0026quot;: [\u0026quot;Accountant\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Marketing\u0026quot;]}) df_merged = df.merge(df_1, on=\u0026quot;Job\u0026quot;, how=\u0026quot;inner\u0026quot;) print(df_merged[\u0026quot;Job\u0026quot;].value_counts()) sb.distplot(df_merged[\u0026quot;Job\u0026quot;].value_counts()) plt.show() Basic Plots\n Density Plot Histogram Plot Scatter Plot Scatter Matrix Plot  import pandas as pd import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer dataset = load_breast_cancer() cols = [ 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39' ] df = pd.DataFrame(dataset['data'], columns=cols) df['target'] = dataset.target # show first a few rows print(df.head()) # show data type for each column print(df.info()) # show summary statistics for each columns print(df.describe()) # check the target variable properties print(df['target'].value_counts()) # Density Plot df['V11'].plot.kde() plt.show() # Histogram df['V11'].plot.hist() plt.show() # Box Plot df.boxplot(['V11']) plt.show() # Scatter Plots (detecting relationship between variables) df.plot.scatter(x='V11', y='V12') plt.show() # Scatter Matrix Plot pd.plotting.scatter_matrix(df[['V11', 'V21', 'V31']], figsize=(15,15)) plt.show() Correlations Correlation values are between -1 and 1.\n 0 means there\u0026rsquo;s no linear relationship -1 means negative correlation 1 means positive correlation  Correlation Matrices Measure the linear dependence between features; can be visualized with heat maps\nCorrelation Matrix Heatmap Apply color coding to the correlation matrix for easy detection of correlation among the attributes\nGenerating Heatmap\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer dataset = load_breast_cancer() cols = [ 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39' ] df = pd.DataFrame(dataset['data'], columns=cols) col = ['V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19'] heatmap = np.corrcoef(df[col].values.T) fig, ax = plt.subplots(figsize=(15, 15)) im = ax.imshow(heatmap, cmap='PiYG', vmin=1) fig.colorbar(im) ax.grid(False) [[ax.text(j, i, round(heatmap[i, j], 2), ha=\u0026quot;center\u0026quot;, va=\u0026quot;center\u0026quot;, color=\u0026quot;w\u0026quot;) for j in range(len(heatmap))] for i in range(len(heatmap))] ax.set_xticks(np.arange(len(col))) ax.set_yticks(np.arange(len(col))) ax.set_xticklabels(col) ax.set_yticklabels(col) plt.show() Generating Heatmap Using Seaboarn\nimport pandas as pd import numpy as np import seaborn import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer dataset = load_breast_cancer() cols = [ 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39' ] df = pd.DataFrame(dataset['data'], columns=cols) col = ['V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19'] heatmap = np.corrcoef(df[col].values.T) seaborn.heatmap(heatmap, yticklabels=col, xticklabels=col, cmap='PiYG', annot=True) plt.show() Data Issues  Messy Data Noisy Data Biased Data  The data itself is biased and non-representative of the truth For example: randomly populated data by the user in the survey   Imbalanced Data  Response variation in sample dataset does not represent the production data For example: the model returns 0 only in 1% of the cases in the sample dataset   Outliers Correlated Data  Highly correlated features can cause colinearity problems and numerical instability     Data Processing and Feature Engineering Encoding Categorical Variables  Categorical Variables are also called Discrete Categories are often represented by text Many algorithms required numerical input Special encoding is required to convert categories into numerical representations  Encoding Ordinals Types of Categorical Variables\n Ordinal  Ordered by certain numerical measurements E.g. clothes size, shoe size: large, medium, small   Nominal  No information about the order E.g. color: red, green, blue \u0026hellip;    Pandas support special dtype=\u0026quot;category\u0026quot;\nEncoding Categorical Variables Example\n Encoding ordinals using the map function, e.g. garden_size Encoding using sklearn\u0026rsquo;s LabelEncoder for labels  Cannot be used if there\u0026rsquo;s no relationship between categories with more than two categories    import pandas as pd from sklearn.preprocessing import LabelEncoder df = pd.DataFrame([ ['house', 3, 2572, 'S', 1372000, 'Y'], ['apartment', 2, 1386, 'N', 699000, 'N'], ['house', 3, 1932, 'L', 800000, 'N'], ['house', 1, 851, 'M', 451000, 'Y'], ['apartment', 1, 600, 'N', 324000, 'N'] ]) df.columns = ['type', 'bedrooms', 'area', 'garden_size', 'price', 'loan_approved'] print(df) # Converting garden_size using mapping mapping = dict({'N': 0, 'S': 5, 'M': 10, 'L': 20}) df['num_garden_size'] = df['garden_size'].map(mapping) # Converting label loan_approved using LabelEncoder loan_enc = LabelEncoder() df['num_loan_approved'] = loan_enc.fit_transform(df['loan_approved']) print(df) Encoding Nominals One-Hot Encoding Explode nominal attributes into many binary attributes, one for each discrete value\nfrom sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder import pandas as pd df = pd.DataFrame({\u0026quot;Fruits\u0026quot;: ['Apple', 'Banana', 'Banana', 'Mango', 'Banana']}) type_labelenc = LabelEncoder() num_type = type_labelenc.fit_transform(df[\u0026quot;Fruits\u0026quot;]) print(num_type) # output: # [0 1 1 2 1] print(num_type.reshape(-1, 1)); # output: # [[0] # [1] # [1] # [2] # [1]] type_enc = OneHotEncoder() type_trans = type_enc.fit_transform(num_type.reshape(-1, 1)).toarray() print(type_trans) # output: # [[1. 0. 0.] # [0. 1. 0.] # [0. 1. 0.] # [0. 0. 1.] # [0. 1. 0.]] Using Pandas\u0026rsquo; function:\nimport pandas as pd df = pd.DataFrame({\u0026quot;Fruits\u0026quot;: ['Apple', 'Banana', 'Banana', 'Mango', 'Banana']}) dummies = pd.get_dummies(df) print(dummies) Encoding with Many Classes\n To avoid drastically increasing dataset size, define a hierarchy structure For example: for a ZIP code column, use regions -\u0026gt; states -\u0026gt; city as the hierarchy and choose specific level to encode the ZIP code column Try to group the levels by similarity to reduce the overall number of groups  Handling Missing Values Most ML Algorithms cannot deal with missing values automatically\nCheck the Missing Values using Pandas\nimport pandas as pd df = pd.DataFrame({ \u0026quot;Fruits\u0026quot;: [\u0026quot;Banana\u0026quot;, \u0026quot;Apple\u0026quot;, \u0026quot;Mango\u0026quot;, \u0026quot;Mango\u0026quot;, \u0026quot;Apple\u0026quot;], \u0026quot;Number\u0026quot;: [5, None, 3, None, 1] }) # Display the total number of missing values for each column print(df.isnull().sum()) # Display the total number of missing values for each row print(df.isnull().sum(axis=1)) Important to Consider:\n What were the mechanisms that caused the missing values? Are these missing values missing at random? Are there rows or columns missing that you are not aware of?  Treating Missing Values\n Dropping rows  May loose to much data, (overfitting) May bias the sample   Dropping columns  May loose information in features (underfitting)   Imputing the values  Dropping The Missing Values\nimport pandas as pd df = pd.DataFrame({ \u0026quot;Fruits\u0026quot;: [\u0026quot;Banana\u0026quot;, \u0026quot;Apple\u0026quot;, \u0026quot;Mango\u0026quot;, \u0026quot;Mango\u0026quot;, \u0026quot;Apple\u0026quot;], \u0026quot;Number\u0026quot;: [5, None, 3, None, 1] }) # Drop the rows with null values print(df.dropna()) # Drop the columns with null values print(df.dropna(axis=1)) Imputing (Replacing) the Missing Values\nImputing using the mean strategy:\nfrom sklearn.impute import SimpleImputer import numpy as np arr = np.array([ [5, 3, 2, 2], [3, None, 1, 9], [5, 2, 7, None] ]) imputer = SimpleImputer(strategy='mean') imp = imputer.fit_transform(arr) print(imp) Feature Engineering  Creating new features to use as inputs to ML model using domain and data knowledge Scikit-learn\u0026rsquo;s library sklearn.feature_extraction can help in this stage Some rules of thumb  Use intuition Try generating features first and then apply dimensionality reduction if needed Consider transformations of attributes, e.g. squaring, multiplication of attributes Try not to overthink Try not to include too much manual logic    Filtering and Scaling Filter Examples:\n Remove color channels from the image if it\u0026rsquo;s not important Remove frequencies from audio if power is less than threshold  Scaling:\n Many algorithms are sensitive to each feature being on different scale  Gradient Descent KNN   Some algorithms are not sensitive to different scales  Decision trees Random forests    Scaling Transformation in Sklearn:\n Mean/variance standardization  Many algorithms behave better with smaller values Keeps outlier information, but reduces the impact sklearn.preprocessing.StandardScaler   MinMax scaling  Robust to small standard deviations sklearn.preprocessing.MinMaxScaler   MaxAbs scaling  Divides by the maximum value of the feature sklearn.preprocessing.MaxAbsScaler   Robust scaling  Uses 75th and 25th quartile metrics to do the scaling Robust to outliers as they have minimal impact sklearn.preprocessing.RobustScaler   Normalizer  Applied to the row, and not to columns as others sklearn.preprocessing.RobustScaler    Standard Scaler\nfrom sklearn.preprocessing import StandardScaler import numpy as np arr = np.array([ [5, 3, 2, 2], [2, 3, 1, 9], [5, 2, 7, 6] ], dtype=float) scale = StandardScaler() print(scale.fit_transform(arr)) print(scale.scale_) MinMaxScaler (produces values between 0 and 1)\nfrom sklearn.preprocessing import MinMaxScaler import numpy as np arr = np.array([ [5, 3, 2, 2], [2, 3, 1, 9], [5, 2, 7, 6] ], dtype=float) scale = MinMaxScaler() print(scale.fit_transform(arr)) print(scale.scale_) Transformation Polynomial Transformation\n Polynomial transformation can be applied to numeric features of the model Can use sklearn.preprocessing.PolynomialFeatures The newly created features can be then fed to the Model Beware of overfitting if the degree is too high Risk of extrapolation beyond the range of the data Consider non-polynomial transformations as well:  Log transforms Sigmoid transforms    from sklearn.preprocessing import PolynomialFeatures import numpy as np import pandas as pd df = pd.DataFrame({'a': np.random.rand(5), 'b': np.random.rand(5)}) cube = PolynomialFeatures(degree=3) cube_features = cube.fit_transform(df) cube_df = pd.DataFrame(cube_features, columns=[ '1', 'a', 'b', 'a^2', 'ab', 'b^2', 'a^3', 'ba^2', 'ab^2', 'b^3' ]) print(cube_df) Radial Basis Function\n Widely used in Support Vector Machines as a kernel and in Radial Basis Neural Networks (RBNN) Gaussian RBF is the most common RBF used  Text-Based Features Bag-of-words Model\n Represent documents as vector of numbers, one for each word (tokenize, count and normalize) Sparse matrix implementation is typically used Count Vectorizer is available in scikit-learn library: sklearn.feature_extraction.text.CountVectorizer  Includes lowercasing and tokenization on white space and punctuation   TfidVectorizer - Inverse Document-Frequency Vectorizer is available in sklearn.feature_extraction.text.TfidVectorizer  Hashing Vectorizer - stateless mapper from text to term index available in sklearn.feature_extraction.text.HashingVectorizser     Model Training, Tuning, and Debugging Supervised Learning: Neural Networks  Perceptron  Perceptron is a simplest form of a neural network It is a single-layer neural network   Neural Network  Contains several layers skelearn.neural_network.MLPClassifier Deep Learning Frameworks  MXNet TensorFlow Caffe PyTorch Keras     Convolutional Neural Network  Very efficient for Image processing Kernel is applied to extract features from the image Pooling layer: reducing dimension to reduce the size of the data   Recurrent Neural Network  Input data involves time-series or sequential features e.g forecasting, translation    Supervised Learning: K-Nearest Neighbors  Define a distance metric  Euclidean distance Manhattan distance Any vector norm   Choose the number of $k$ neighbors Find the $k$ nearest neighbors of the new observation that we want to classify\\ Assign the class label by majority vote Imortant to find the right $k$  Commonly use $k = \\frac{\\sqrt{N}}{2}$, where $N$ = number of samples    Characteristics of K-Nearest Neighbor Algorithm\n Non-parametric, instance-based, lazy Model requires keeping the original data set Space complexiy and prediction-time complexity grow with the size of training data Suffers from curse of dimensionality: points become increasingly isolated with more dimensions, for a fixed-size training dataset scikit-learn: sklearn.neighbors.KNeighborsClassifier  Supervised Learning: Linear and Non-Linear Support Vector Machines  Suited for extreme cases Unoptimized decision boundary can result in greater miscallsifications on new data  Types of SVM\n Linear support vector machines  For linearly separable sets The goal to define a hyperplane that classifies all training vectors into classes The best choice is to find a hyperplane that leaves the maximum margin from both classes   Non-Linear support vector machines  Applying kernels will support tranform a non-linear input space into a linear feature space Kernel is a function that takes as an input vectors in the original space and returns the dot product of the vectors in the feature space  Polynomical Kernel Radial Basis Function RBF Kernel Sigmoid Kernel, etc\u0026hellip;      Supervised Learning: Decision Trees and Random Forests  Algorithm decides which features are used for splitting in each layer Entropy - relative measure of disorder in the data source Simple to understand, interpret and visualize Less need for feature transformation Susceptible to overfitting scikit-learn: sklearn.tree.DecisionTreeClassifier  Building a Decision Tree\n Which features to choose? What are conditions for splitting? Where to stop? Pruning  Types of Decision Trees\n Regression Trees  Used when dependent variable is continuous Uses mean/average   Classification Trees  Used when dependent variable is categorical Uses mode/class    Common Decision Tree Algorith\n GIRI Index Chi-Squared Information Gain Reduction Invariance  Model Training: Validation Set  Model Training  Improve the model by optimizing parameters or data   Model Tuning  Analyze the model for generalization quality and sources of underperformance such as overfitting    Splitting Data: Training, Testing, Validation**  Training Set  Labeled dataset Dataset used for training the model   Validation Set  Usually 80/10/10 split, where 10% is for validation set and 10% for test set Labeled dataset Dataset used for assessing the Model\u0026rsquo;s performance on each trained epoch The split between training and validation dataset is required to avoid model overfitting Overfitting happens when the Model is giving good results on the training data, however is not able to generalize on the data it has not been trained on. Giving good results on both training and validation dataset gives us greater confidence in the model and its ability to generalize Validation dataset should be representative   Testing Set  Measures the final predictive power of the Model Can be used to compare the performance of different models    Model Training: Bias Variance Tradeoff Bias\n Indicates that the model is underfitting Caused by wrong assumptions, lack of complexity in the model  Variance\n Indicates that the model is overfitting Caused by over-complex models Model can be performing great on the testing set, but is not performing well on the validation set  $Total Error(x) = Bias^2 + Variance + Irreducible Error$\n As the model grows in complexity, it tends to move from Low Variance, High Bias to High Variance and Low Bias Best model is the model which keeps the Total Error minimal by making the right Bias-Variance tradeoff Examples of Improving the Model  Adjusting the Model complexity Adjusting the training size Adjusting the Hyperparameters    Learning Curve\n Plots the model performance Training dataset and validation dataset error or accuracy against training set size scikit-learn provides sklearn.learning_curve.learning_curve  Uses stratified k-fold cross validation by default    Model Debugging: Error Analysis  Filter on failed predictions and manually look for patterns Pivot on target, key attributes, and failure type, and build histograms of error counts Common patterns  Data problems (many variants for the same word) Labeling errors (data mislabeled) Under/over-represented subclasses (too many examples of one type) Discriminating information is not captured in features   It often helps to look on what model is predicting correctly  Model Tuning: Regularization  Regularization helps reduce variance / overfitting of the model Regularization penalizes for certain model complexity Higher complexity models are more likely to be unable to generalize well Regularization is achieved by adding a term to the loss function, that penalizes for the large weights: $loss + penalty$ Regularization is another hyperparameter that should be found out based on k-fold cross validation  Regularization Techniques Regularization in Linear Models\n L1 regularization, Lasso  $ penalty = \\sum_{j=1}^{n} \\lvert w_j \\rvert $\n L2 regularization, Ridge  $ penalty = \\sum_{j=1}^{n} \\lvert w_j^2 \\rvert $\nL2 Regularization In Neural Network\n$ penalty = (\\sum_{j=1}^n \\lvert w^{[j]} \\rvert^2 ) \\frac{\\lambda}{2 m} $\n$n$ - the number of layers $w^{[j]}$ - the weight matrix for the $j^th$ layer $m$ - the number of inputs $\\lambda$ - the regularization parameter\nScikit Learn Support\n Ridge regression model: Linear regression with L2  sklearn.linear_model.Ridge   Lasso regression model: Linear regression with L1  sklearn.linear_model.Lasso   Elastic net regression model: Linear regression with both  sklearn.linear_model.ElasticNet   Strength of regularization $c = \\frac{1}{\\alpha} $  Model Tuning: Hyperparameter Tuning Parameter vs Hyperparameter\n Parameter: an internal configuration whose value can be estimated from the data Hyperparameter: an external configuration whose value cannot be estimated from the data  Tuning Hyperparameters\n Grid Search  Will try all combinations for hyperparameters and evaluated the best Computationally intensive   Random Search  Finding the best set of hyperparameters based on random permutations of possible values of hyperparameters    Model Tuning Training Data Tuning Possible Issues and Solutions\n Small training data  More data can be sampled and labeled if possible   Training set biased against missing or more important scenarios  Sample and label more data for those scenarios if possible   Can\u0026rsquo;t easily sample or label more?  Consider creating synthetic data (duplication or techniques like SMOTE)   Training data doesn\u0026rsquo;t need to be exactly representative. Testing dataset needs to be exactly representative.  Feature Set Tuning  Add features that help capture pattern for classes of errors Try different transformations of the same feature Apply dimensionality reduction to reduce impact of weak features  Dimensionality Reduction\n Common cause of overfitting: too many features for the amount of data Dimensaionality Reduction: Reduce the (effective) dimension of the data with minimal loss of information The curse of dimensionality: certain models may not be able to give good results in the current dimension due to sparsity of data  Model Tuning: Feature Extraction  Mapping data into smaller feature space that captures the bulk of the information in the data  aka Data compression   Improves computational efficiency Reduces the curse of dimensionality Techniques  Principal Component Analysis (PCA)  Unsupervised Approach for feature extraction   Linear Discriminant Analysis (LDA)  Supervised Approach for feature extraction   Kernel versions of these for fundamentally non-linear data    Feature Selection vs Feature Extraction\n Both Feature selection and Feature Extraction reduce the dimensionality of the feature space Feature Selection  Uses algorithms to remove some of the features from the model Selected features will enable the model to have better performance There is no change (such as transform, linear combination or non-linear combination) in the selected features   Feature Extraction  Using algorithms to combine original features to generate a new set of features Number of features to be used in the model is generally less than the original number of features    Model Tuning: Bagging/Boosting Bagging\n Good for high variance but low bias Generates group of weak learners that when combined together generate higher accuracy  Create a x dataset of size m by randomly sampling original dataset with replacement (duplicates allowed)   Train weak learners on the new datasets to generate predictions Choose the output by combining the individual predictions (average in regression problem) or voting (classification) Reduces variance Keeps bias the same sklearn  sklearn.ensemble.BaggingClassifier sklearn.ensemble.BaggingRegressor    Boosting\n Good for high bias and accepts weights on individual samples Assign strength to each week learner Iteratively train learners using misclassified examples by the previous weak learners Training a sequence of samples to get a strong model sklearn:  sklearn.ensemble.AdaBoostClassifier sklearn.ensemble.AdaBoostRegressor sklearn.ensemble.GradientBoostingClassifier   XGBoost library   Model Evaluation and Model Productionizing Using ML Models in Production  Integrating an ML solution with existing software Keeping it running successfully over time  Aspects to Consider\n Model hosting Model deployment Pipelines to provide feature vectors Code to provide low-latency and/or high-volume predictions Model and data update and versioning Quality monitoring and alarming Data and model security and encryption Customer privacy, fairness and trust Data provider contractual constraints  Types of Production Environments\n Batch Predictions  All inputs are know upfront Predictions can be served real-time from pre-computed values   Online Predictions  Low latency requirements   Online Training  Data patterns change frequently, needs online training    Model Evaluation Metrics Confusion Matrix  Confusion Matrix values will be obtained when running the test data on the ML Model  Metrics $ Accuracy = \\frac{ TP + TN }{ TP + TN + FP + FN }$\n$ Precision = \\frac{ TP }{ TP + FP }$\n$ Recall = \\frac{ TP }{ TP + FN }$\n$ F1-Score = \\frac{ 2 x Precision x Recall }{ Precision + Recall }$\nCross Validation Cross validation is a model validation technique, for assessing the prediction performance of the model. Based on this, certain chunk of data, referred to as testset, will be excluded from the training cycle and utilized for testing stage.\nK-Fold Cross Validation  K-fold gives you an opportunity to train across all the data Is especially useful for smaller data sets Typically 5-10 folds are used  Steps\n Randomly partition the data into k-segments For each segment / fold, train the model on all other segments exclusing the selectged one  Use the fold excluded from training to evaluate the model   Train on all the data Average metric across K-folds estimates test metric for trained model  Leave-one Out Cross Validation  K = number of data points Used for very small sets K specifies the number of rows to used for training, and then leave one out  Stratified K-fold Cross Validation  Preserve class proportions in the folds Used for imbalanced data There are seasonality or subgroups  Metrics for Linear Regression $ Mean Squared Error $ $$ Mean Squared Error (MSE) = \\frac{1}{N} \\sum_{i=1}_{N} ( \\hat{y_i} - y_i )^2 $$\n$ R^2 $ $$ R^2 = 1 - \\frac{Sum of Squared Error (SSE)}{Var(y)} $$\n $R^2$ Coefficient of Determination Values between 1 and 0 1 indcates that regression perfectly fits the data  $$ Adjusted R^2 = 1 - (1 - R^2) \\frac{no. of data pts. - 1}{no. of data pts. - no. of variables - 1} $$\nUsing ML Models in Production: Storage Considerations\n Read/Write speed, latency requirements Data Storage Format Platform-dependency Ability for schema to evolve Schema/data separability Type richness Scalability  Model and Pipeline Persistence  Predictive Model Markup Language (PMML):  Vendor-independent XML-based language for storing ML models Support varies in different libraries  KNIME (analytics / ML Library): Full support Scikit-learn: extensive support Spark MLlib: Limited support     Custom Methods:  Scikit-learn: uses Python pickle method to serialize/deserialize Python objects Spark MLlib: transformers and estimators implement MLWritable TensorFlow: Allows saving of MetaGraph MxNet: Saves into JSON    Model Deployment  A/B Testing  May help detect production issues at early stage   Shadow Testing  Model is running behind the scenes Allows estimating Model\u0026rsquo;s performance while not serving production systems    Using ML Models in Production: Monitoring and Maintenance Monitoring Considerations:\n Qualiy Metrics and Buinsess Impacts Dashboards Alarms User Feedback Continuous model performance reviews  Expected Changes\n The real-world domain may change over time The software environment may change High profile special cases may fail There may be a change in business goals Performance deterioration may require new tuning Changing goals may require new metrics Changing domain may require changes to validation set Your validation set may be replaced over time to avoid overfitting  Using ML Models in Production: Using AWS  AWS SageMaker  Pre-built notebooks Built-in, high performance algorithms One-click training Hyperparameter optimization One-click deployment Fully managed hosting with auto-scaling   Amazon Rekognition Image Amazon Rekognition Video Amazon Lex  Service for building conversational interfaces into any application using voice and text ASR (Automatic Speech Recognition) NLU (Natural Language Understanding)    Amazon Transcribe  Amazon Polly  Text to speech service   Amazon Comprehend  NLP (Natural Language Processing) service  Discover insights and relationships in text Identify language based on the text Extract key phrases, places, people, brands or events Understand how positive or negative the text is Automatically organizes a collection of text files by topic     Amazon Translate  Automatic speech recognition (ASR) service   AWS DeepLens  Custom-designed deep learning inference engine HD video camera with on-board compute optimized for deep learning Integrates with AmazonSageMaker and AWS Lambda From inboxing to first inference in \u0026lt; 10 minutes Tutorials, examples, demos and pre-built models   AWS Glue  Data integration service for managing ETL jobs (Extract, Transform, Load)   Deep Scalable Sparse Tensor Network Engine (DSSTNE): Neural network engine  Common Mistakes  You solved the wrong problem The data was flawed The solution didn\u0026rsquo;t scale Final result doesn\u0026rsquo;t match with the prototype\u0026rsquo;s results It takes too long to fail The solution was too complicated There weren\u0026rsquo;t enough allocated engineering resources to try out long-term science ideas There was a lack of a true collaboration  "
},
{
	"uri": "https://majdarbash.github.io/python-2.7/1-python-introduction/",
	"title": "Python 2.7: Introduction",
	"tags": [],
	"description": "",
	"content": "Python installation:\nYou can install python by using this link:\nhttps://www.python.org\nDocumentation can be downloaded from:\nhttps://www.python.org/doc\nAs I will be using Mac, i will install python from:\nhttps://www.python.org/download/mac/\nYou can run python using command:\n$ python Following flags can be provided\n-v - verbose output You can use PyCharm - a python IDE provided by JetBrain - has some really cool features. Just google PyCharm and download the community / professional edition.\n"
},
{
	"uri": "https://majdarbash.github.io/ml-neural-networks/neural-networks-overview/",
	"title": "What are Neural Networks? Why do we use them?",
	"tags": ["artificial intelligence", "neural networks", "machine learning"],
	"description": "",
	"content": "Why do we use neural networks? Neural networks help us solve a lot of complicated problems when there\u0026rsquo;s no clear way to solve it. Sometimes problems can be complicated enough where we don\u0026rsquo;t know how to solve it, or solving them in conventional way will lead to very complicated algorithms which cannot adapt to changes.\nNeural networks play essential role in acting like human brain - helping us solving problems which we are good at: speech recognition, object recognition, pattern recognition, classification, etc\u0026hellip;\nNeural networks consists of layers on neurons, starting from input layer - which accepts the signal from receptors - one or multiple hidden layers (quite a big amount of hidden layers is one of main characteristics of deep learning networks) and output layer. Neurons of each layer are interconnected with the neurons next layer through synapses - which act like connectors transmitting signals. These synapses have positive or negative weights which will inhibit (negative weight) or excite (positive weight) neurons. Incoming signals from synapses are combined within the neuron and are passed through outgoing synapses to the next layer neural connections. Adjusting synaptical weights gives the ability to the neural network to respond to experience.\nMNIST - is a database of handwritten digits which is publicly available and can be used to train our networks.\nImageNet task - is a competition of classification of 1.3 million high resolution images from the web into 1000 different object classes. Measurements are done on top-1 and top-5 error rates. Top-1 error rate - error rate of classification of image matching the first answer of classification of machine. Top-5 error rate - error rate of classification of image being one of the top 5 choices selected by machine.\nSpeech Recognition - improved drastically with introduction of deep neural networks. Google achieved 12.3% error rate in android 4.1 after 5,870 hours of training data.\nNeuron Models Linear Neurons This is the simplest model for a neuron.\n\\begin{equation} y = b + \\sum_{i}{x_i w_i} \\end{equation}\n$b$ - bias for the neuron\n$i$ - index of input connection to neuron i\n$x_i$ - $i^{th}$ input\n$w_i$ - weight on $i^{th}$ input\nBinary Threshold Neurons \\begin{equation} z = \\sum_{i}{x_i w_i} \\end{equation}\n\\begin{equation} y = \\begin{cases} 1, if z \\geq \\Theta \\\\ 0, otherwise \\end{cases} \\end{equation}\nz - total input calculation\ny - output of the neuron\nRectified Linear Neurons (Linear Threshold Neurons) \\begin{equation} z = \\sum_{i}{x_i w_i} \\end{equation}\n\\begin{equation} y = \\begin{cases} z, if z \\geq 0 \\\\ 0, otherwise \\end{cases} \\end{equation}\nz - total input calculation\ny - output of the neuron\nSigmoid Neurons \\begin{equation} z = b + \\sum_{i}{x_i w_i} \\end{equation}\n\\begin{equation} y = \\dfrac{1}{1 + e^{-z}} \\end{equation}\nz - total input calculation\ny - output of the neuron\nThe beauty about the neuron is that it will generate an output betwen 0 and 1. with higher $z$ you will get $y$ approaching 1, while the lower, negative $z$ will lead to $y$ approaching 0.\nStochastic Binary Neurons Use the same equations of logistic units. The output of logistic is treated as the probability of producing a spike.\n\\begin{equation} z = b + \\sum_{i}{x_i w_i} \\end{equation}\n\\begin{equation} p(s = 1) = \\dfrac{1}{1 + e^{-z}} \\end{equation}\nz - total input calculation\ns - spike\np - probability of producing 1\nFor example, if p(s=1) = 0.9, neuron will be giving us 1 for 90% of the times. This gives some intrinsically random nature to the neuron.\n"
},
{
	"uri": "https://majdarbash.github.io/blockchain/blockchain-the-technical-side/",
	"title": "2 - Blockchain - The Technical Side",
	"tags": [],
	"description": "",
	"content": "Public an private keys are large integer numbers and they are represented using a separate Wallet Import Format (WIF), consisting of letters and numbers.\n\nPrivate key is used to generate a signature for each blockchain transaction a user sends out. It is used to confirm that the transaction has come from the user and also prevents the transaction from being altered by anyone once it has been issued.\nYou sign the cryptocurrencies you send to others using a private key.\nPrivate key derives public key. With a has function public key is transformed to address visible to everyone. You can receive cryptocurrencies to your address.\nCryptographic Hash Functions\nCollision when two inputs to a hash function produce the same output is extremely unlikely to happen.\nHash functions are deterministic - the same input will always yield the same output.\nBitcoin uses a SHA-256. Ethereum blockchain uses a hashing algorithm called Ethash. Hash created with Ethash looks like this:\n0xb846300e188829d1b819389b31cef3b9cfaf335082ee66f830a875f1c1beb396\nWhen data is written to the block it’s hashed. This principle allows the nodes participating in the blockchain to detect any changes to data.\nHashed data is used to create a link between each specific block. Hash of each previous block is written into the block in the chain and then hashed - producing the current block’s hash. This means that the current block hash will contain the prior hash.\nData written for the blockchain is considered permanent, making blockchain immutable.\nBlockchain relies on public key cryptography to acknowledge the idea of ownership on the blockchain.\nProcess of miners verifying actions on the blockchain is known as a proof of work.\nMessage Encryption\nBob wants to send message to Alice\nBob uses Alice’s public key to encrypt the message\nAlice decrypts the message using her private key\nMessage Signing\nBob uses his private key to create digital signature of the message\nAlice uses Bob’s public key to verify the content of the message and that the message is from Bob\nMerkle Tree\nEvery 2 nodes in this binary tree will be hashed together and merged into a single hash. As the result hash root is obtained.\nVerifying that a data chunk is part of the bunch represented by a merkle root will require less computation.The Bitcoin blockchain uses Merkle proofs in order to store the transactions in every block.\nAnatomy of the Block\n Transactions (limited to 1mb in size, 1500 - 2000 transactions) Block height: unique, auto-incremental number, identifying block’s position Timetstamp Nonce Hash of each previous block is written into the block in theHash of the previous block in the chain  Validity: not all blocks are valid. Finding a valid block = mining. Blockchains have an arbitrary “difficulty” setting which changes how hard it is to find a block. The work required to create a valid block is where value comes from. Miners are rewarded financially for finding a block. This is the work in “proof of work”.\nBased on the set difficulty you have to find a nonce which will result in the block hash value to be below the threshold. As a result of this the block will become valid and you will receive a reward for your work - currently it’s 12.5 bitcoins.\nTampering a single transaction will invalidate all the subsequent transactions and will require someone to recalculate the valid nonces for them.\nBlockchain protocols work on peer-to-peer network model. (e.g. Ethereum network, Bitcoin and Bittorent). There are 3 types of nodes operating in blockchain network: Full Node, Light Node, Miners.\n"
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/2019-11-01-ml-building-blocks-services-and-terminology/",
	"title": "ML Building Blocks: Services and Terminology",
	"tags": [],
	"description": "",
	"content": " Terminology Stages Data Process / ML Workflow  ML Problem Framing Data Collection / Integration Data Preparation  Data Cleaning Shuffling Training Data Test-Validation-Train Split Cross Validation   Data Visualization \u0026amp; Analysis Feature Engineering Model Training  Parameter Turning   Model Evaluation Business Goal Evaluation  Feature and Data Augmentation   Prediction    Terminology Stages  Training  Refers to how machine uses historical data sets to build its prediction algorithms.   Model  Model is what your machine creates after it\u0026rsquo;s been trained and refines over time as it learns.   Prediction  Prediction is machine\u0026rsquo;s best estimate of what the outcome of specific input or set of inputs would be. It\u0026rsquo;s sometimes called the Inference of a Model.    Data In Training Process, Data is split into:\n Training Dataset  Used by machine to create first model. Constitutes the majority of data.   Test Dataset  Is used to test the model for accuracy.    Process / ML Workflow  ML Problem Framing Data Collection / Integration Data Preparation Data Visualization \u0026amp; Analysis Feature Engineering Model Training Model Evaluation Business Goal Evaluation Prediction  Goal of Machine Learning model is to provide solution to a Business Problem. This happens through prediction. Prediction is not accurate and improves over time through provided feedback.\n ML Problem Framing  Forming Machine Learning Problem from the Business Problem What to use and how to use it? Do we have all the data needed? What algorithm do we use to answer the business question?  Supervised Learning  Learning from historical data set with a known answer.   Unsupervised Learning  Outcome is not known, ML algorithm will choose how to quantify the data and then give us the result.   Reinforcement Learning  The algorithm is rewarded based on the choices it makes while learning.      Classification Problems\n Binary Classification  2 classes   Multiclass Classification  3 + classes    Problem Definition\n Defining:  Observations Labels (Variables we are trying to predict) Features (Feature Engineering Process)     Data Collection / Integration  Structured data Semi-structured data Unstructured   Data Preparation Data Cleaning  Handling outliers Handing missing feature values  Introduce new indicator variable to represent missing value Removes rows Imputation  Replacing missing value with a value from dataset - may be a calculated guess. For example, for numerical we can use: mean, median.      Shuffling Training Data Makes data order not important and improves the results in certain algorithms.\nTest-Validation-Train Split  Test: 20% Validation: 10% Train: 70%  Cross Validation  Validation Leave-one-out (LOOCV) K-Fold   Data Visualization \u0026amp; Analysis Helps us understand the data better, refine the data, clean the outliers. This will result in better features leading to better models.\n Statistics Scatter-polts  Could help detect feature correlations   Histograms  Will help us detect outliers and skews in data     Feature Engineering Process of converting raw data into more useful features.\n Numeric Value Binning  Helps introduce non-linearity into linear models, breaking up continuous values Continuous values can be partioned into Bins based on ranges   Quadratic Features  Deriving new non-linear features by combining feature pairs   Non-Linear Feature Transformations Tree Path Features  Uses leaves of decision tree as features   Domain-Specific Transformations  Text: stop words removal / stemming, lowercasing, puctuation, cutting off very high / low percentiles Web-page Features: multiple fields of text, URL, anchor text, relative style and positioning     Model Training Parameters are the knobs used to tune our Machine Learning Algorithm.\nParameter Turning  Loss Function  Predicts how far your predictions are from the ground truth values.  Mean Square Loss Hinge Loss Logistic Loss     Regularization  Prevent overfitting by constraining weights to be small.   Learning Parameters  How fast or slow will your algorithm learn. Learning too fast may mean the algorithm will never reach the optimum value. Learning too slow means algorithm may take too long and never converge to the optimum.     Model Evaluation  Overfitting \u0026amp; Underfitting Bias-Variance Tradeoff Evaluation Metrics (Will be checked on test dataset.)  Regression  Root Mean Square Error (RMSE) MAPE (Mean Absolute Percent Error) $R^2$   Classification  Confusion Matrix ROC Curve Precision-Recall       Business Goal Evaluation  How well the model is performing related to business goals Make the decision to deploy or not  Accuracy Model generalization on unseen/unknown data Business success criteria     Feature and Data Augmentation Increases the complexity of the training data set by deriving features from internal / external data.\nPrediction  Model deployment is continuous process Monitoring distribution of production data vs. traning data is required Model should be re-trained with fresh learning data which reflect the current production distribution Model can be trained periodically  "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/networking/",
	"title": "Networking",
	"tags": [],
	"description": "",
	"content": " Ephermal Ports Reserved IP Addresses in VPC Network to VPC Connectivity  AWS Managed VPN AWS Direct Connect AWS Direct Connect Plus VPN AWS CloudHub VPN Software VPN Transit VPC   VPC to VPC Connectivity  VPC Peering AWS PrivateLink VPC Endpoints   Internet Gateways  Internet Gateway Egress-Only Internet Gateway NAT Instance NAT Gateway NAT Instances vs. Nat Gateways   Routing  Routing Tables Border Gateway Protocol (BGP)   Enhanced Networking  Placement Groups   Route 53  Route 53 Routing Policies   CloudFront Elastic Load Balancer  Read more on VPCs from AWS CSAA\n  AWS does not allow multicast (unicast vs multicast) TCP - stateful, connection based with acknowledging receipt UDP - stateless, no retransmission delays ICMP - used by network devices to exchange info (e.g. traceroute, ping)  Ephermal Ports  Short-lived transport protocol ports used in IP communication Above the well-know ports (\u0026gt; 1024) Refered to as \u0026ldquo;Dynamic Ports\u0026rdquo; Suggested range 49152 to 65535  Linux kernels use 32568 to 61000 Windows platform default from 1025   NACL and Security Group implications  Client defines the Ephermal port to use for response    Reserved IP Addresses in VPC  10.0.0.0 - Network address 10.0.0.1 - Reserved by AWS for VPC Router 10.0.0.2 - Reserved by AWS for Amazon DNS 10.0.0.3 - Reserved by AWS for future use 10.0.0.255 - VPCs don\u0026rsquo;t support broadcast so AWS reserves this address  Network to VPC Connectivity AWS Managed VPN  Managed IPSec VPN connection Quick and simple way to establish secure tunneled connection to a VPC; Redundant link for Direct Connect or other VPC VPN Support static routes or BGP peering and routing Dependent on your internet connection  AWS Direct Connect  Direct connection to AWS backbone May require additional telecom and hosting provider relationships and/or new network circuits Work with your existing Data Networking Provider; Create Virtual Interfaces (VIF) to connect to VPCs or other AWS service like S3 or Glacier (public VIF) Direct Connect connections consist of a single connection between your network and AWS with no inherent redundancy (by default it\u0026rsquo;s not HA) Traffic coming from on-prem via a Direct Connect connect is restricted from internet access  AWS Direct Connect Plus VPN  IPSec VPN connection over private lines Extra layer of security through encrypted tunnel over AWS Direct Connect  AWS CloudHub VPN  Connect locations in a \u0026ldquo;Hub and Spoke\u0026rdquo; manner using AWS\u0026rsquo;s Virtual Private Gateway Link remote offices for backup or primary WAN access to AWS resources and each other  Software VPN  Provide your own VPN endpoint and software (e.g. OpenVPN) Useful when you want to manage both ends of the VPN connection for compliance reasons or you want to use VPN option not supported by AWS You must build and design the VPN architecture for redundancy and availability  Transit VPC  Common strategy for geographically disperse VPCs in order to create a global network transit center Locations and VPC-deployed assets across multiple regions that need to communicate with one another Cisco, Juniper Networks and Riverbed have offerings which work with their equipment and AWS VPC  VPC to VPC Connectivity VPC Peering  AWS-provided network connectivity between two VPCs Uses AWS Backbone without touching the Internet Transitive peering is not supported  A connected to B and B connected to C, doesn\u0026rsquo;t mean that A is connected to C via B   VPC peering request is made from Originator to Accepter  AWS PrivateLink  AWS-provided network connectivity between VPCs and AWS Services Use AWS backbone to reach other services, without going through the Internet  VPC Endpoints  Interface Endpoint  Elastic Network Interface with Private IP Uses DNS entries to redirect traffic API Gateway, CloudFormation, CloudWatch, etc\u0026hellip; Secured by Security Groups   Gateway Endpoint  The gateway which is a target for a specific route Uses prefix lists in the route table to redirect traffic Amazon S3, DynamoDB VPC Endpoint Policies    Internet Gateways Internet Gateway  Horizontally scaled, redundant and highly available component No AZ risk or bandwidth constraints Provides route table target for Internet-bound traffic Perform NAT for instances with public IP addresses  Egress-Only Internet Gateway The purpose of an \u0026ldquo;Egress-Only Internet Gateway\u0026rdquo; is to:\n  Allow IPv6 based traffic within a VPC to access the Internet\n  Denying any Internet based resources the possibility of initiating a connection back into the VPC\n  IPv6 addresses are globally unique and are therefore public by default\n  Provides outbound intenret access for IPv6 addressed instances\n  Prevents inbound access to those IPv6 instances\n  Stateful - forwards traffic from instance to internet and then sends back the response\n  Must create a custom route for ::/0 to the Egress-Only Internet Gateway\n  Use Egress-Only Internet Gateway instead of NAT for IPv6\n  NAT Instance  EC2 instance from a special AWS-provided AMI Translates traffic from many private IP instance to a single public IP and back Doesn\u0026rsquo;t allow public internet initiated connections into private instances Not supported for IPv6 (use Egress-Only Gateway) NAT instance must live on a public subnet with route to Internet Gateway Private instances in private subnet must have route to the NAT instance, usually the default route destination of 0.0.0.0/0  NAT Gateway  Fully-managed NAT service that replaces need for NAT instances on EC2 Must be created in a Public subnet Uses an Elastic IP for public IP for the life of hte gateway Private instances must have route to the NAT gateway, usually to destination 0.0.0.0/0 Created in a specified AZ with redundancy in that zone For multi-AZ deployment, create NAT Gateways in each AZ with routes for private subnets to use the local Gateway Up to 5Gbps bandwidth that can scale up to 45 Gbps  NAT Instances vs. Nat Gateways  NAT Instances allow you to detach and attach EIPs while NAT Gateways do not allow you to detach NAT Instances can use security groups as they are just EC2 instances NAT Instances also can be configured to support port forwarding while NAT Gateways do not support this  Routing Routing Tables  VPCs have an impicity router and main routing table You can modify the main routing table or create new tables Each route table contains a local route for the CIDR block Most specific route for an address wins Taffic to VPC Endpoints is routed through the routing tables  Border Gateway Protocol (BGP)  Popular protocol for the Internet BGP is the only protocol supported by AWS Propgates information about the network to allow dynamic routing Required for Direct Connect and optional for VPN Alternative of not using VGP with AWS VPC is static routes AWS Supports BGP community tagging as a way to control traffic scope and route preference Required TCP port 179 + ephermal ports Autonomous System Number (ASN) = Unique point identifier Weighting is local to the router and higher weight is preferred path for outbound traffic  Enhanced Networking  Generally used for High Performance Computing use-cases Uses single root I/O vritualization (SR-IOV) to deliver higher performance than traditional virtualized network interfaces Might have to install driver if other than Amazon Linux HVM AMI Interface Options  Intel 82599 VF Interface (10 Gbps) Elastic Network Adapter (25 Gbps)    Placement Groups  Clustered  Instances are placed into a low-latency group within a single AZ Required for low network latency and/or high network throughput Pros: Get the most out of Enhanced Networking Instances Cons: Finite capacity: recommend launching all you might need up front   Spread  Instances spread across underlying hardware Required to reduce risk of simultaenous failure if underlying hardware fails Pros: Can span multiple AZ\u0026rsquo;s Cons: Max of 7 instances running per group per AZ   Partition  Instances are grouped into partitions and spread across racks Required to reduce risk of correlated hardware failure for multi-instance workloads Pros: Better for large distributed or replicated workloads than Spread Cons: Not supposed for Dedicated Hosts    Route 53  Register domain names Check the health of your domain resources Route internet traffic for your domain Route 53 currently supports 13 different DNS record types including; AAAA, CNAME and SPF Route 53 does not currently support any of DNSSEC related records, such as DNSKEY  Route 53 Routing Policies  Simple Failover  Health check of the primary route will lead to failover to secondary route   Geolocation Geoproximity  Closer proximity to the region is specified. Bias value ranging between -99 and +99 is used to control the proximity radius   Latency  Traffic will be routed to the region that provides the best latency   Mutlivalue Answer  respond to DNS queries with up to eight healthy records selected at random not good for distributing the traffic - DNS client may cache the IP for some time which may lead to unexpected results   Weighted  Traffic is distributed respecting the weights of the routes.    CloudFront SSL and TLS\n 1995: SSLv2 first public release 1996: SSLv3 released to fix security design flaws 1999: TLSv1.0 with SSL backwards 2005: TLSv1.2 Present: TLSv1.3  CF Settings\n CF Security Policy  Defines the protocols and ciphers that CloudFront uses   Server Name Indication (SNI)  Allows CloudFront distribution instances to server content over HTTPS to clients that support SNI. Older browsers will not have access to content over HTTPS.    Elastic Load Balancer  Application Load Balancer (Layer 7)  Supports VPC HTTPS / HTTP Path, Method, Query string or Host-based Routing Web Sockets SNI Sticky Sessions Static IP, Elastic IP (through AWS Global Accelerator) User Authentication   Network Load Balancer (Layer 4)  Supports VPC TCP / UDP / TLS Web Sockets SNI, as of 2019 Static IP, Elastic IP   Classic Load Balancer (Layer 4 /7)  Supports EC2-Classic or VPC Sticky Sessions    "
},
{
	"uri": "https://majdarbash.github.io/ml-neural-networks/neural-networks-types-of-learning/",
	"title": "Neural Networks: Supervised, Unsupervised &amp; Reinforcement Learning Types of Learning",
	"tags": ["artificial intelligence", "neural networks", "machine learning", "types of learning", "supervised", "unsupervised", "reinforced"],
	"description": "",
	"content": "There are 3 types of learning\n Supervised learning Reinforcement learning Unsupervised learning  Supervised Learning In supervised learning we try to predict an output when given an input vector. Input and target are clear in supervised learning.\nRegression - target ouptut is a real numbers or a vector of real numbers\nClassification - target output is a class label\nModel-class is a function:\n\\begin{equation} y = f(x; W) \\end{equation}\n$y$ - output of the model-class\n$x$ - input vector\n$W$ - other parameters provided to the function $f$\nLearning is the process of adjusting the parameters $W$, which will reduce the discrepancy between the target output $t$ and the actual output $y$.\nFor regression, discrepancy can be measured as:\n\\begin{equation} \\dfrac{1}{2} (y - t) ^2 \\end{equation}\nExamples of unsupervised learning can be:\n linear regression for regression problems random forest for classification and regression problems support vector machines for classification problems  Unsupervised Learning Here you have only input data (X) and no output variables. The goal is to model the structure in the data order to learn more about the data. After your data is normalized and certain metrics are correlated between different data groups, you may find out some interesting facts about the data you are dealing with.\nThere are mainly 2 groups of unsupervised learning problems:\nClustering - discovering how data can be grouped (i.e. grouping customers by a certain behavior)\nAssociation - association learning problem, discovering rules that describe large portions of data\nExamples can include:\n k-means for clustering problems Apriori algorithm for association rule learning problems  Reinforcement Learning The objective of reinforcement learning is to select each action to maximize the rewards. Usually it is not easy to conclude which action was right or wrong as the reward is happening in the future and the time lag will add some uncertainity on the learning / adjustment process.\nBasically the learner decides What to do in order to maximize the reward. The exact output is not clear like in supervised learning, and we know that we are expecting an output (in contrast to unsupervised learning).\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/2-python-basics-variables-strings-date-and-time/",
	"title": "Python 2.7: Basics: Variables, Strings, Date and Time",
	"tags": [],
	"description": "",
	"content": "Variables x = 10 This statement will create a variable called x which will store a number 10. By assigned value python understands that this variable will store numeric data. We can also store other data types, by assigning different types of values:\nx_int = 1 x_float = 1.23 x_bool = True Note that booleans should have an uppercase in the beginning. (True and not true, False and not false)\nTo print anything on the screen you can use the print function:\nprint x_float Whitespaces In python whitespaces have a significant meaning - they are used to structure code beyond the scope of indentation and formatting. Badly formatted code will result in IndentiationError.\nComments Single-line comments\n# my comment Multi-line comments\n\u0026quot;\u0026quot;\u0026quot; my comment - mutline line comment \u0026quot;\u0026quot;\u0026quot; Math Python will go beyond the standard math operators provided by the programming languages. In addition to operations like multiplication, addition, subtraction and division, python will provide you with some more interesting operators.\nLet\u0026rsquo;s assume you want to raise 2 to a power of 3. This is how we can do it in python:\nresult = 2 ** 3 Exponentiation is performed in python by using ** operator.\nStrings Similar to any other language:\nx_string = \u0026quot;John\u0026quot; # espace character x_string = 'This is John\\'s birthday' String is actually an array of characters. You can directly access characters by using indexes:\nx_string = \u0026quot;Python\u0026quot; # will display the character t print x_string[2] String operators: len() - returns the number of characters in a string\n# will print number 5 as a number of characters in the word hello print len(\u0026quot;hello\u0026quot;) string.lower() - this method is used to convert all characters to lowercase\n# output: hello print \u0026quot;Hello\u0026quot;.lower() string.upper() - converts all characters to uppercase\n# output: HELLO print \u0026quot;Hello\u0026quot;.upper() str() - converts non-strings to strings\n# creating a float variable pi pi = 3.14 # converting pi to string print str(pi) String Concatenation Strings can be concatenated using the summation operator +:\n# output: Times of our life print \u0026quot;Times \u0026quot; + \u0026quot; of \u0026quot; + \u0026quot; our \u0026quot; + \u0026quot; life \u0026quot; # output: My score is 50 x_int = 50 print \u0026quot;My score is \u0026quot; + str(x_int) Another way to print strings by doing string replacement is to format them with %\n# output: Jane and Mike were best friends since a long time string_1 = \u0026quot;Jane\u0026quot; string_2 = \u0026quot;Mike\u0026quot; print \u0026quot;%s and %s were best friends since a long time\u0026quot; % (string_1, string_2) This can be a more interesting exercise:\n# input: for first name: [Tom], for last name: [Hanks] # output: Dear Tom Hanks, please proceed to the registration counter first_name = raw_input(\u0026quot;What's your first name?\u0026quot;) last_name = raw_input(\u0026quot;What's your last name?\u0026quot;) print \u0026quot;Dear %s %s, please proceed to the registration counter\u0026quot; (first_name, last_name) Date and Time The code below displays the current time\n# output - will print current time in format of : 2016-11-10 18:18:31.379762 from datetime import datetime now = datetime.now() print now # output: current year, e.g. 2016 print now.year # output: current month, e.g. 11 print now.month # output: current day, e.g. 10 print now.day just like above, you can also use now.hour, now.minute and now.second\n"
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/2019-11-6-crisp-dm-on-aws/",
	"title": "CRISP-DM on AWS",
	"tags": [],
	"description": "",
	"content": "CRISP-DM  Stands for Cross Industry Standard Process - Data Mining Is a framework to Data Science Project  Phases of CRISP-DM  Business Understanding Data Understanding Data Preparation Modeling Evaluation Deployment   Business Understanding  Understanding business requirements   Questions from the business perspective which need answering Highlight project\u0026rsquo;s critical features People and resources required  Analyzing supporting information   List required resources and assumptions Analyze associated risks Plan for contengencies Compare costs and benefits  Converting to a Data Mining or Machine Learning problem   Review machine learning question Create technical data mining objective Define the criteria for successful outcome of the project  Preparing a preliminar plan   Number and duration of stages Dependencies Risks Goals Evaluation methods Tools and techniques   Data Understanding  Data collection  Detail Various sources and steps to extract data Analyze data for additional requirements Consider other data sources   Data properties  Describe the data, amount of data used, and metadata properties Fidn key features and relationshps in the data Use tools and techniques to explore data properties   Quality  Verifying attributes Identifying missing data Reveal inconsitencies Report solution    AWS tools for Data Understanding\n Amazon Athena  Run interactive SQL queries on Amazon S3 data Schema-on-read Serverless   Amazon QuickSight  Fast, cloud-powered business intelligence and data visualization service   AWS Glue  Managed Extract-Transform-Load (ETL) service     Data Preparation Tasks \u0026amp; Modeling Data Prepation Tasks 1. Final Dataset Selection  Total size Included and Excluded columns Record selection Data type  2. Data Preparation  Cleaning  Missing data  Dropping rows Add default value or mean value Use statistical methods to calculate the value   Address noise values   Transformed  Derive additional attributes from the original Normalization Attribute transformation   Merging  Merging data into the final data set Null values introduced may require a cleaning iteration   Formatting  Rearrange attributes Randomly shuffle data Remove constraints of the modeling tool (Unicode characters \u0026hellip;)    Data Modeling 1. Model selection and creation  Identify modeling technique (Regression for numeric values, Recurrent NN for sequence prediction\u0026hellip;) Constraints of mdoeling technique and tool  2. Model testing plan  Test/train dataset split (30% test, 70% train) Model evaluation criterion  3. Parameter tuning/testing  Build multiple models with different parameter settings Describe the trained models and report on the findings  AWS Tools for Data Preparation and Modeling\n AWS EMR + Spark  IPython notebooks, Zeppelin notebooks, R studio Scala, Python, R, Java, SQL Cost savings: Leverage Spot instances for task nodes   AWS EC2 + Deep Learning AMI  GPU CUDA support for training Preinstalled deep learning frameworks  MXNet, TensorFlow, Caffe2, Torch, Keras, Theano\u0026hellip;   Includes Python Anaconda Data Science Platform with popular libraries like Numpy, Sikit-learn You can install R Studio on EC2 Deep Learning AMI     Evaluation 1. Evaluate how the model performed related to the business goals  Accuracy of the model Model generalization on unseen/unknown data Evaluation of the model using existing business criteria  Reviewing the project\n Assess the steps taken in each phase Perform quality assurance checks  2. Make the final decision to deploy or not Based on complete evaluation and business goals acceptance criteria we will take a decision wether a model will be deployed or not. This requires careful analysis of the false positives and true negatives.\nRunning Jupyter Notebook on EC2 Instance  Create instance using Deep Learning AMI Connect to the instance using SSH Run: screen (read more \u0026hellip;) Run Jupyter Notebook: jupyter notebook --no-browser OR jupyter notebook --no-browser --ip=0.0.0.0 --port=[choose your port] Copy-paste the URL containing the token to the browser and access the example   Deployment 1. Planning deployment  Runtime  AWS EC2 Instances AWS EC2 Container Service AWS Lambda  Trained model could be saved to S3 and then loaded in Lambda function     Application Deployment  AWS CodeDeploy AWS OpsWorks AWS Elastic Beanstalk   Infrastructure Deployment  AWS CloudFormation AWS OpsWorks AWS Elastic Beanstalk   Code Management  AWS CodeCommit AWS CodePipeline AWS Elastic Beanstalk    2. Maintenace and monitoring Monitoring\n AWS CloudWatch AWS CloudTrail AWS Elastic Beanstalk  3. Final report  Highlight processes used in the project Analyze if all the goals for the project were met Detail the findings of the project Identify and explain the model used and reason behind using the model Identify the customer groups to target using this model  4. Project review  Assess the outcomes of the project Summarize the results and write thorough documentation Common pitfalls Choosing the right ML solution Generalize the whole process to make it useful for the next iteration  "
},
{
	"uri": "https://majdarbash.github.io/python-2.7/3-python-built-in-functions-examples/",
	"title": "Python 2.7: Built-in Functions, Examples",
	"tags": [],
	"description": "",
	"content": "Python provides you with a great variety of built-in functions and modules. We wil display a bit of interesting examples here, using some built-in functions.\ndef biggest_number(*args): return max(*args) As you noticed, we can pass multiple inputs to our function using *args syntax.\n# output: 6 print max(4,2,6) # output: 3 print min(3,6,7) # output: 98 print abs(-98) Determining the type of variabel and using it: # output: displays the types of variables: int, str, float x_int = 52 x_string = \u0026quot;hello\u0026quot; x_float = 2.436 # output: print type(x_string) # output: print type(x_float) # output: print type(x_int) # verify the input and increment it if numeric def increment(number): if type(number) == int or type(number) == float: return number + 1 else: return False def increment_input(input): result = increment(input) if result: print \u0026quot;The result is : %s\u0026quot; % result else: print \u0026quot;Sorry, the input you provided is not a number\u0026quot; # output: Sorry, the input you provided is not a number input1 = raw_input(\u0026quot;Enter a string: \u0026quot;) increment_input(input1) # output: the number provided + 1 input2 = raw_input(\u0026quot;Enter a number: \u0026quot;) input2 = int(input2) increment_input(input2) Creating average function # calculating average with the provided list of numbers # using pre-built function sum def average(numbers): total = float(sum(numbers)) total = total / len(numbers) return total print average([2,4,9]) Generating random numbers from random import randint random_number = randint(0, 12) # output: a random number between 0 and 12 print rand_number ### Sum of digits in a number def digit_sum(n): total = 0 while n != 0: number = n % 10 n = int(n / 10) total = total + number return total # output: 10 = 1+2+3+4 print digit_sum(1234) Factorial function def factorial(x): total = 1 for number in range(1, x + 1): total = total * number return total # output: 24 = 4! = 4 * 3 * 2 * 1 print factorial(4) Is Prime number function def is_prime(x): x = abs(x) for number in range(0, x): # skipping the numbers 0 and 1 if abs(number) == 0 or abs(number) == 1: continue # if number can be divided by any other number then it's not a prime number if x % number == 0: return False return True # output: True print is_prime(13) # output: False print is_prime(12) # output: True print is_prime(-11) String reverse function def reverse(word): reversed_word = \u0026quot;\u0026quot; for character in word: reversed_word = character + reversed_word return reversed_word # output: emesrever print reverse('reverseme') Anti-vowel function This function will remove the vowel in a string and return a function without vowels.\ndef anti_vowel(word): word_without_vowels = \u0026quot;\u0026quot; for character in word: if character.lower() in ['a', 'e', 'i', 'o', 'u']: continue word_without_vowels = word_without_vowels + character return word_without_vowels # output: \u0026quot;Hr y g, dr!\u0026quot; print anti_vowel(\u0026quot;Here you go, dear!\u0026quot;) ### Counting number of item occurences in the list def count(sequence, item): total = 0 for search_item in sequence: if item == search_item: total = total + 1 return total # output: 3 (number 2 occurs 3 times in the list) print count([1,2,3,2,3,2], 2) ### Removing odd numbers from the list def purify(list): new_list = [] for item in list: if item % 2 == 0: new_list.append(item) return new_list # output: [2] - odd numbers will be removed print purify([1,2,3]) ### Multiplying numbers in the list def product(numbers): total = 1 for number in numbers: total = total * number return total # output: 6 = 1 * 2 * 3 print product([1,2,3]) ### Removing duplicates from the list def remove_duplicates(list): clean_list = [] for item in list: if item in clean_list: continue clean_list.append(item) return clean_list # output: [1,3,4] print remove_duplicates([1,3,3,4]) ### Calculating median in the list of numbers # even number of elements in the list # taking an average of the middle elements # otherwise, taking the middle element def median(numbers): numbers = sorted(numbers) if len(numbers) == 0: return 0 if len(numbers) % 2 == 0: middle = len(numbers) / 2 - 1; return float(numbers[middle] + numbers[middle + 1]) / 2 else: middle = (len(numbers) - 1) / 2; return numbers[middle] # output: 0 - median of empty list print median([]) # output: 5.5 print median([1,2,5,6,7,8]) # output: 3 print median([1,2,3,4,5]) Grades Example: calculating average, variance and standard deviation: grades = [100, 100, 90, 40, 80, 100, 85, 70, 90, 65, 90, 85, 50.5] def print_grades(grades): for grade in grades: print grade def grades_sum(grades): total = 0 for grade in grades: total = total + grade return total def grades_average(grades): return grades_sum(grades) / float(len(grades)) def grades_variance(grades): average = grades_average(grades) variance = 0 for grade in grades: variance = variance + (average - grade) ** 2 variance = variance / len(grades) return variance def grades_std_deviation(variance): return variance ** 0.5 # output: the grades list, each grade on a single line print_grades(grades) # output: 80.4230769231 print 'Average Grade: %s' % grades_average(grades) # output: 334.071005917 variance = grades_variance(grades) print 'Variance: %s' % variance # output: 18.2776094147 print 'Standard Deviation: %s' % grades_std_deviation(variance) "
},
{
	"uri": "https://majdarbash.github.io/ml-neural-networks/machine-learning-python/",
	"title": "Python 2.7: Setting up Neural Network with PyBrain",
	"tags": ["artificial intelligence", "python", "pybrain", "neural networks"],
	"description": "",
	"content": "Today, I\u0026rsquo;m experimenting machine learning concepts in python. For this purchase I\u0026rsquo;m using PyBrain. If you would like to have a better idea about Python, I suggest having a quick glance at posts 1-10 in Python category.\nPyBrain is a Machine Learning library for Python. PyBrain stands for Python-Based Reinforcemnet Learning, Artificial Intelligence and Neural Network Library.\nFor complete guide on installation you can get complete details from:\nhttp://pybrain.org/docs/\nIn the example below, based on pybrain.org tutorial I\u0026rsquo;m creating a network, dataset and training my network on the dataset.\nInstalling PyBrain:\n$ git clone git://github.com/pybrain/pybrain.git $ python setup.py install\nBuilding a network Let\u0026rsquo;s build a simple network. Assume our network accepts 2 inputs, and is expected to generate 2 outputs. Let\u0026rsquo;s experiment with the following network structure:\n one input layer (2 neurons) one hidden layer (3 neurons) one output layer (1 neuron)  from pybrain.tools.shortcuts import buildNetwork net = buildNetwork(2, 3, 1); Activating Network Network gets populated with random values. We can test the output of the network by activating it.\nLet\u0026rsquo;s pass the inputs 2 and 3 to our network:\nprint net.activate(\\[2, 3\\]) Customizing your network structure You can check out the network structure, using the following print commands:\nfrom pybrain.tools.shortcuts import buildNetwork\n# building network net = buildNetwork(2, 3, 1); # activating network on input 2, 3 print net.activate(\\[2, 3\\]); # will display the network structure print net \u0026quot;\u0026quot;\u0026quot; output: FeedForwardNetwork-8 Modules: \\[, , , \\] Connections: \\[ 'out'\u0026gt;, 'hidden0'\u0026gt;, 'out'\u0026gt;, 'hidden0'\u0026gt;\\] \u0026quot;\u0026quot;\u0026quot; # output: print net\\['in'\\] # output: print net\\['out'\\] # output: print net\\['hidden0'\\] When using buildNetwork the hidden layer is constructed with a sigmoid squashing function. Let\u0026rsquo;s assume you would like to change the hidden layer to a different type of layer, i.e. Hyperbolic Tangent function. You can do so, by supplying the hidden layer class as an argument to buildNetwork constructor:\nfrom pybrain.tools.shortcuts import buildNetwork from pybrain.structure import TanhLayer from pybrain.structure import SoftmaxLayer # the hidden layer of network 1 is constructed # with Hyperbolic Tangent activation function net1 = buildNetwork(2, 3, 1, hiddenclass = TanhLayer) # the hidden layer of network 2 is constructed # with Softmax activation function net2 = buildNetwork(2, 3, 1, hiddenclass = TanhLayer, outclass = SoftmaxLayer) # network is using bias net3 = buildNetwork(2, 3, 1, bias = True) ### Building a DataSet SupervisedDataSet - this class is used for standard supervised learning. Supports input and target values whose size is defined. from pybrain.datasets import SupervisedDataSet # dataset supports 2-d input and 1-d target ds = SupervisedDataSet(2, 1) # data set for the XOR function ds.addSample((0, 0), (0)) ds.addSample((0, 1), (1)) ds.addSample((1, 0), (1)) ds.addSample((1, 1), (0)) # will print dataset length # output: 4 print len(ds) # iterating through dataset like a dictionary for input, target in ds: print input, target # will print inputs of data set print ds\\['input'\\] # will print targets of dataset print ds\\['target'\\] if you want to clear the dataset, you can use: \\# clear the data set ds.clear(); Training the Neural Network on the Dataset Now that we\u0026rsquo;ve got the network ready it\u0026rsquo;s time to train our network. We will do so by using back propagation algorithm - BackpropTrainer class.\nAll we have to do is to provide our network instance and dataset instance to the trainer - instantiated from BackpropTrainer class, and then run the train method.\nfrom pybrain.supervised.trainers import BackpropTrainer from pybrain.tools.shortcuts import buildNetwork from pybrain.datasets import SupervisedDataSet from pybrain.structure import TanhLayer # building network to be trained on XOR output net = buildNetwork(2, 3, 1, bias = True, hiddenclass = TanhLayer) # dataset supports 2-d input and 1-d target ds = SupervisedDataSet(2, 1) # data set for the XOR function ds.addSample((0, 0), (0)) ds.addSample((0, 1), (1)) ds.addSample((1, 0), (1)) ds.addSample((1, 1), (0)) trainer = BackpropTrainer(net, ds) # this will train the network for full epoch and return # double which is proportional to the error. print trainer.train() # will continue training until results converge # returns a list of tuples containing the errors for every training epoch print trainer.trainUntilConvergence() The example above is used just to show different functions and usages of these functions and won\u0026rsquo;t lead to effective results. The problem with the above example is that trainUntilConvergence by default requires validationProportion. The default value is 0.25, meaning that 25% of dataset will be used for the validation dataset. These two datasets are split and don\u0026rsquo;t intersect. The problem is that omitting 25% of dataset samples will lead to a badly trained network.\nYou can solve XOR problem using different approach:\n\\# learn XOR with a nerual network with saving of the learned paramaters import pybrain from pybrain.datasets import \\* from pybrain.tools.shortcuts import buildNetwork from pybrain.supervised.trainers import BackpropTrainer ds = SupervisedDataSet(2, 1) ds.addSample((0, 0), (0,)) ds.addSample((0, 1), (1,)) ds.addSample((1, 0), (1,)) ds.addSample((1, 1), (0,)) net = buildNetwork(2, 4, 1, bias=True) trainer = BackpropTrainer(net, learningrate=0.01, momentum=0.99) trainer.trainOnDataset(ds, 1000) trainer.testOnData() print net.activate((1, 1)) As you notice we are trainig our network explicitly for 1000 epochs and not until the results converge with expected output. You can validate the results of the network by rounding the output: \\# output: 0.0 # output: 1.0 # output: 1.0 # output: 0.0 print round(net.activate((0, 0))) print round(net.activate((0, 1))) print round(net.activate((1, 0))) print round(net.activate((1, 1))) Saving the neural network to file and loading it After searching the internet for XOR pybrain example I found some reliable code, which actually doesn\u0026rsquo;t ommit the solution space as mentioned in the example above. As there\u0026rsquo;s no validation set, the results cannot be tested for convergence, and we have to train for certain number of epochs. Eventually the results did converge, based on evidence.\nhttps://github.com/thedanschmidt/PyBrain-Examples/blob/master/xor.py\n\\# learn XOR with a nerual network with saving of the learned paramaters import pybrain from pybrain.datasets import \\* from pybrain.tools.shortcuts import buildNetwork from pybrain.supervised.trainers import BackpropTrainer import pickle if \\_\\_name\\_\\_ == \u0026quot;\\_\\_main\\_\\_\u0026quot;: ds = SupervisedDataSet(2, 1) ds.addSample((0, 0), (0,)) ds.addSample((0, 1), (1,)) ds.addSample((1, 0), (1,)) ds.addSample((1, 1), (0,)) net = buildNetwork(2, 4, 1, bias=True) try: f = open('\\_learned', 'r') net = pickle.load(f) f.close() except: trainer = BackpropTrainer(net, learningrate=0.01, momentum=0.99) trainer.trainOnDataset(ds, 1000) trainer.testOnData() f = open('\\_learned', 'w') pickle.dump(net, f) f.close() print net.activate((1, 1)) The beauty of the code above is that the first time you run it, the file _learned does not exists and the code will jump to except clause, which will train the network and popualate the _learned file. Afterwards we will activate the network with certain input in the main scope.\nIn any subsequent execution of the code, _learned file will be opened successfully and the training part will be skipped. We will be able to reuse our network directry from the file!\nNote the usage of pickle library which supports dumping / loading objects from / to the file.\n"
},
{
	"uri": "https://majdarbash.github.io/aws-csap/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": " Security Concepts  Shared Responsibility Model Principle of Least Privillege** Security Facets Typical Components SAML 2.0, OAuth 2.0, OpenID Connect AWS Artifact   Multi-Account Management Network Controls and Security Groups AWS Directory Services Credentials and Access Management Encryption  Key Management Service (KMS) CloudHSM (Hardware Security Module) AWS Certificate Manager   Distributed Denial of Services Attacks IDS and IPS AWS Service Catalog  AWS Service Catalog Constraints     Security Concepts Shared Responsibility Model  AWS - Reponsbility for Security of the Cloud Customer - Responsibility for Security in the Cloud  Principle of Least Privillege**  Give users (or services) nothing more than those privileges required to perform their intended function. (and only when they need them)  Security Facets  Identity  Who are you?   Authentication  Prove that you\u0026rsquo;re who you say   Authorization  Are you allowed to do this?   Trust  Do other entities that I trust say they trust you    Typical Components  Identities  Users who want to grant access to certains services   Identity Providers  e.g. Facebook, Google, etc..   Identity Broker  Facilitates communication with identity to grant access to certain services   Identity Store  Contains information about identities privileges   Services  SAML 2.0, OAuth 2.0, OpenID Connect SAML 2.0\n Can handle both authorization and authentication XML-based protocol Can contain user, group, membership and other useful information Assertions in the XML for authentication, attributes or authorization Best suited for Single Sign-On for enterprise users  OAuth 2.0\n Allow sharing of protected assets without having to send login credentials Handles authorization only, not authentication Issues token to client Application validates token with Authorization Server Delegate access, allowing the client applications to access information on behalf of user Best suited for API authorization between apps  OpenID Connect\n Identity layer built on top of OAuth 2.0, adding authentication Uses REST/JSON message flows Supports web clients, mobile clients, Javascript clients Extensible Best suited for Single Sign-on for consumer  AWS Artifact  AWS resource for compliance-related information Provides on-demand access to AWS\u0026rsquo; security and compliance reports and select online agreements  Multi-Account Management  AWS Organizations Service Control Policies Tagging Resource Groups Consolidated Billing  Identity Account Structure\n Manage all user accounts in one location Users trust relationship from IAM roles in sub-accounts to identify Account to grant temporary access Variations include by Business Unit, Deployment, Environment, Geography  Logging Account Structure\n Centralized logging repository Can be secured so as to be immutable Can use Service Control Policies (SCP) to prevent sub-accounts from changing logging settings  Publishing Account Structure\n Common repository for AMI\u0026rsquo;s, Containers, Code Permits sub-accounts to use pre-approved standardized services or assets  Central IT Account Structure\n IT can manage IAM users and groups while assigning to sub-account roles IT can provide shared services and standardized assets (AMI\u0026rsquo;s, databases, EBS, etc.) that adhere to corporate policy  Network Controls and Security Groups Security Groups\n Virtual firewalls for individual assets (EC2, RDS, AWS Workspaces, etc..) Controls inbound and outbound traffic Port or port ranges Inbound rules are by SourceIP, Subnet or other Security Groups Outbound rules are by Destination IP, Subnet, or other SG  Network Access Control Lists (NACLs)\n Additional layer of security for VPC that acts as a firewall Apply to entire subnets rather than individual assets Default NACL allows all inbound and outbound traffic NACLs are stateless - outbound traffic simply obeys outbound rules - no connection is maintained Can duplicate or futher restrict access with SG Remmeber ephermal ports for OUtbound if you need them  Why NACLs?\n NACLs provide a backup method of security if you accidentally change yoiur SG to be too permissive Covers the entire subnet so users to create new instances and fail to assign a proper SG are still protected Part of a multi-layer Least Privilege concept to explicitly allow and deny  AWS Directory Services  AWS Cloud Directory  Cloud-native directory to share and control access to hierarchical data between applications Cloud appliocations that need hierarchical data with complex relationships   Amazon Cognito  Sign-up and sign-in functionality that scales to millions of users and federated to public social media services Develop consumer apps or SaaS   AWS Directory Service for Microsoft Active Directory  AWS-managed full Microsoft AD (standard or enterprise) running on Windows Server 2012 R2 Enterprises that want hosted Microsoft AD or you need LDAP for Linux Apps   AD Connector  Allow on-premises users to log into AWS services with their existing AD credentials. Also allows EC2 instances to join AD domain. Single sign-on for on-prem employees and for adding EC2 instances to the domain   Simple AD  Low scale, low cost AD implementation based on Samba Simple user directory, or you need LDAP compatibility    Credentials and Access Management AWS Security Token Service (STS)\nToken Vending Machine Concept\n Common way to issue temporary credentials for mobile app development Anonymous TVM (Token Vending Machine) - Used as a way to provide access to AWS services only, does not store user identity Identity TVM - used for registration and login, and authorizations AWS now recommends that mobile developers use Cognito and related SDK  AWS Secret Manager\n Store passwords, encryption keys, API keys, SSH keys, PGP keys, etc\u0026hellip; Alternative to storing passwords or keys in a \u0026ldquo;vault\u0026rdquo; Can access secrets via API with fine-grained access control provided by IAM Automatically rotate RDS database credentials for MySQL, PostgreSQL and Aurora Better than hard-coding credentials in scripts or application  Encryption  Encryption at Rest  Data is encrypted such as on EBS, on S3, in an RDS database or in an SQS queue waiting to be processed   Encryption in Transit  Data is encrypted as it flows through a network or process, such as SSL/TLS for HTTPS, or with IPSec for VPN connections    Key Management Service (KMS)  Key storage, management and auditing Tightly integrated into many AWS services, like Lambda, S3, EBS, EFS, DynamoDB, SQS, etc. You can import your own keys or have KMS generate them Control who manages and accesses keys via IAM users and roles Audit use of keys via CloudTrail Differs from Secret Manager as its purpose-build for encryption key management Validated by many cmpliance schemes  CloudHSM (Hardware Security Module)  Dedicated hardware device, Single Tenanted Must be within a VPC and can access via VPC Peering Does not natively integrated with many AWS services like KMS, but rather requires custom application scripting Offload SSL from web servers, act as an issuing CA, enable TDE for Oracle databases  Cloud HSM vs KMS\n CloudHSM  Single-Tenant HSM Customer-managed durability and available Customer managed root of trust Broad 3rd Party Support   AWS KSM  Multi-Tenant AWS Service Highly available and durable key storage and management AWS managed root of trust Broad AWS Service Support    AWS Certificate Manager  Managed service that lets you provision, manage and deploy public or private SSL/TLS certificates Directly integrated into many AWS services like CloudFront, ELB and API Gateway Free public certificates to use with AWS services; no need to register via a 3rd party certificate authority You can import 3rd party certificates for use on AWS Supports wildcard domains Managed certificate renewal Can create a managed Private Certificate Authority as well for internal and proprietary apps, services or devices  Distributed Denial of Services Attacks  Phishing is one of the common way to compomise the systems Amplification/Reflection Attacks  Send small request to NTP server by manipulating data packets (NTP Monlist) NTP server replies to target device with a big response (MONLIST command output) Target device is flooded   Application Attacks (Layer 7)  HTTP GET Flood    Mitigating DDoS\n Minimize attack surface  NACLs, SGs, VPC Design   Scale to absorb attack  Auto-Scaling Groups, AWS CloudFront, Static Web Content via S3   Safeguard exposed resources  Route 53, AWS WAF, AWS Shield   Learn normal behavior  AWS GuardDuty, CloudWatch    IDS and IPS  Intruder Detection System  watches network to identify someone trying to compromise the system   Intruder Prevention System  Prevents exploits, sits behind firewall and scans and analyzes suspicious content   Systems usually consist of a Collection/Monitoring system and monitoring agents on each system Logs collected in CloudWatch, S3 or other tools are sometimes called Security Information and Event Management (SIEM) system AWS marketplace has different third-party choices of IDS and IPS appliances  CloudWatch\n Log events across AWS services Higher-level comprehensive monitoring and eventing Log from multiple accounts Logs stored indefinetely Alarms history for 14 days  CloudTrail\n Log API activity across AWS services More low-level granular Log from multiple accounts Logs stored to S3 or CloudWatch indefinetely No native alarming; Can use CloudWatch alarms  AWS Service Catalog  Framework allowing administrators to create pre-defined products and landscapes for their users Granular control over which users have access to which offerings Make use of adopter IAM roles so users don\u0026rsquo;t need underlying service access Allows end users to be self-sufficient while uploading enterprise standards and deployments Based on CloudFormation templates Administrators can version and remove products. Existing running product versions will not be affected.  AWS Service Catalog Constraints  Launch Constraint  IAM role that Service Catalog assumes when an end-user launches a product.   Notificaiton Constraint  SNS topic to receive notifications about stack events.   Template Constraint  One or more rules that narrow allowable values an end-user can select    "
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/2019-11-21-isolation-forest-algorithm/",
	"title": "Anomaly Detection: Isolation Forest Algorithm",
	"tags": [],
	"description": "",
	"content": " Defining the Anomaly Detection Problem  Methods to Resolve Anomaly Detection Problem Well Defined Anomaly Distribution Assumption   Isolation Forest Algorithm  Algorithm Steps Example     Defining the Anomaly Detection Problem  Data is set of vectors in d-dimensional space  $x_1$, \u0026hellip; $x_n$ each $x_i$ $\\epsilon$ $R^d$   Mixture of nominal points and anomaly points Anomaly points are generated by different generative process than the normal points  Solutions\n Supervised  Training data labeled with \u0026ldquo;nominal\u0026rdquo; or \u0026ldquo;anomaly\u0026rdquo;   Clean  Training data are all \u0026ldquo;nominal\u0026rdquo;, test data contaminated with \u0026ldquo;anomaly\u0026rdquo; points   Unsupervised  Training data consists of mixture of \u0026ldquo;nominal\u0026rdquo; and \u0026ldquo;anomaly\u0026rdquo; points    Methods to Resolve Anomaly Detection Problem  Density Based  DBSCAN LOF   Distance Based  KNN K-MEANS Regression hyperplane distance   Parametric  GMM Single Class SVMs Extreme value theory    Well Defined Anomaly Distribution Assumption   WDAD: the anomalies are drawn from a well-defined probability distribution\n  The WDAD assumption is often risky\n adversarial situations (fraud, insider threats, cyber security) diverse set of potential causes user\u0026rsquo;s notion of anomaly changes with time     Isolation Forest Algorithm  Builds an ensemble of random trees for a given data set Anomalies are points with the shortest average path length  Assumes that outliers takes less steps to isolate compared to normal point in any data set   Anomaly score is calculated for each point based on the formula: $2^{-E(h(x))/c(n)}$  $h(x)$ is the number of edges in a tree for a point $x$ $c(n)$ is a normalization constant for a data size of size $n$   Isolation score can be used in both supervised and unsupervised setting  Algorithm Steps  Sampling for Training  Choose a sampling proportion from the original data set   Generate a Binary Decision Tree  Split based on 2 random elements  Randomly choose an attribute Randomly choose a value of an attribute in its range of values   Perform a split to branch the tree   Repeat the process iteratively for the sub-data set After generating a Tree, repeat  Create a Forest, collection of Trees Stop when maximum number of Trees is reached   Feed data set an calculate anomaly score for each data point  Calculate anomaly score for a data point across Tree, using the equation:  $2^{-E(h(x))/c(n)}$   Average out the calculated anomaly scores   Score Interpretation  Anomalies will get a score closer to 1 Scores much smaller than 0.5 indicates normal observations If all scores are close to 0.5 then the entire sample doesn\u0026rsquo;t seem to have clearly distinct anomalies    Example In the following example we are using python\u0026rsquo;s sklearn library to experiment with the isolation forest algorithm. In the example below we are generating random data sets:\n Training Data Set  Required to fit an estimator   Test Data Set  Testing Accuracy of the Isolation Forest Estimator   Outlier Data Set  Testing Accuracy in detecting outliers    Generated Data:\n# importing libaries ---- import numpy as np import pandas as pd import matplotlib.pyplot as plt from pylab import savefig from sklearn.ensemble import IsolationForest # Instantiating container for Mersenne Twister pseudo-random number generator with a seed of int 42 rng = np.random.RandomState(42) # Generating 2 clusters of data using \u0026quot;Standard Normal\u0026quot; distribution # In Standard Normal distribution 95% of data lies within +-2 # Multiplying by 0.2 leaves 95% within +-0.4 X_train = 0.2 * rng.randn(1000, 2) print((abs(X_train[:, 0]) \u0026lt;= 0.4).sum() / len(X_train[:, 0]) * 100, \u0026quot; percent of data lies within 2 devitations from the mean\u0026quot;) # Generating second cluster of data (5,5) points away from the center of the first cluster in both axes X_train = np.r_[X_train + 5, X_train] # Generating the Dest Data using the same distribution of the training data X_test = 0.2 * rng.randn(100, 2) # Second cluster of the Test Data as well X_test = np.r_[X_test + 5, X_test] # Generating outliers spread throughout the plot using uniform distribution X_outlier = rng.uniform(low=-1, high=6, size=(50, 2)) # Visualizing the generated data sets: Training - blue, Test - green, Outliers - red plt.title(\u0026quot;Generated Data Sets\u0026quot;) plt.scatter(X_train[:, 0], X_train[:, 1], c='blue') plt.scatter(X_test[:, 0], X_test[:, 1], c='green') plt.scatter(X_outlier[:, 0], X_outlier[:, 1], c='red') plt.show() # Fitting the Isolation Forest Estimator with Training Data # Contamination factors indicates the percentage of data we believe to be outliers clf = IsolationForest(behaviour='new', max_samples=100, random_state=rng, contamination=0.1) clf.fit(X_train) # Running predictions on the Test and Outliers Data Sets using the estimator pred_test = clf.predict(X_test) pred_outlier = clf.predict(X_outlier) print(\u0026quot;Accuracy with Test Data: \u0026quot;, (pred_test == 1).sum() / len(pred_test)) print(\u0026quot;Accuracy In Outlier Detection: \u0026quot;, (pred_outlier == -1).sum() / len(pred_outlier)) "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/migrations/",
	"title": "Migrations",
	"tags": [],
	"description": "",
	"content": " Migration Strategies Cloud Adoption Framework  TOGAF Cloud Adoption Phases   Hybrid Architectures Migration Tools Network Migrations and Cutovers Amazon Snow Family  Migration Strategies  Re-Host Simply move assets without change, e.g. on-prem MySQL to EC2. (Lift and Shift) Re-Platform Move assets but change underlying platform, e.g. Migrate MySQL DB to RDS MySQL. (Lift and Reshape) Re-Purchase Abandon existing and purchase new. (Drop and Shop) Rearchitect Redesign application in a cloud-native manner, e.g. create Serverless version of legacy application. Retire Get rid of applications which are not needed. Retain Do nothing - decide to reevaluate at a future date.  Cloud Adoption Framework TOGAF The Open Group Architectural Framework\n Approach for designing, planning, implementing and governing enterprise IT architectures Started development in 1995 De-facto standard in Enterprise Architecture practice Favored EA framework for most Fortune 500 companies  Cloud Adoption Phases  Project Running projects to get familiar and experience the benefits from the cloud. Foundation Build foundation to scale the cloud adoption. Creating landing zone (pre-configured, secure, multi-account AWS environment), Cloud Center of Excellence (CCoE), operations model, as well as assuring security and compliance readiness. Migration Migrate existing applications including mission-critical applications or entire data centers to the cloud as you scale your adoption across a growing portion of your IT portfolio. Reinvention Focus on reinvention by taking advantage of the flexibility and capabilities of AWS to transform your business by speeding time to market and increasing the attention on innovation.  Holistic Approach to Cloud Adoption Framework:\n Business  Creation of a strong business case for Cloud Adoption Business goals are in harmony with cloud objectives Ability to measure benefits (ROI - Return on Investment, TCO - Total cost of Ownership)   People  Evaluate organizational roles and structures, new skills and process needs and identify gaps Incentives and Career Management aligned with evolving roles Training options appropriate for learning styles   Governance  Portfolio Management geared for determining cloud eligibility and priority Program and Project management more agile projects Align KPI\u0026rsquo;s with newly enabled business capabilities   Platform  Resource provisioning can happen with standardization Architecture patterns adjusted to leverage cloud-native New application development skills and processes enable more agility   Security  Identiy and Access Management modes change Logging and Audit capabilities evolve Shared Responsibility Model removes and adds some facets   Operations  Service monitoring has potential to be highly automated Performance management can scale as needed Business continuity and disaster recovery takes on new methods in the cloud    Hybrid Architectures  Using cloud resources along with on-prem resources First step as a pilot for cloud migrations VMWare - infrastructure can be extended to the cloud Integrations should be loosely coupled  Examples:\n Storage Gateway  Creates a bridge between on-prem and AWS Seamless to end users Common first step due to low risk and appealing economics   Middleware  Great way to leverage cloud services Loosely coupled, canonical-based Example: Can publish messages from Corporate Landscape to SQS to be consumed on the other side by a worker   VMWare vCenter Plugin  Allows transparent migration of VMs to and from AWS VMWare Cloud furthers this concept with more public-native features    Migration Tools  Storage Migration  AWS Storage Gateway AWS Snowball   Server Migration Service  Automates migration of on-prem VMWare vSphere or Microsoft Hyper-V/SCVMM virtual machines to AWS Replaces VMs to AWS, sync volumes and create periodic AMIs Minimizes cutover downtime by syncing VMs incrementally Supports Windows and Linux VMs only (like AWS) The Server Migration Connector is downloaded as a virtual appliance into your on-prem vSphere or Hyper-V setup   Database Migration Service (DMS)  Along with Schema Conversion Tool (SCT) helps customers migrate databases to AWS RDS or EC2-based databases SCT can copy database for homogenous migrations and covert schemas for heterogenous migrations DMS is used for smaller, simpler conversions and also supports MongoDB and DynamoDB SCT used for larger, more complex datasets like data warehouses DMS has replication function for on-prem to AWS or to Snowball or S3   Application Discovery Service  Gathers information about on-prem data centers to help in cloud migration planning Collects configs, usage and behavior data from your servers to help in estimating TCO of running on AWS Can run agent-less (VMWare Environment) or agent-based (non-VMWare Environment) Only supports those OSes that AWS Supports   AWS Migration Hub  Network Migrations and Cutovers  Ensure there\u0026rsquo;s no overlap between VPC and on-prem Most migrations start with VPN to AWS With higher usage, you may choose AWS Direct Connect and keep the VPN as backup Transition from VPN to Direct Connect can be done seamlessly using BGP (BGP weighing or static routes)  Amazon Snow Family  Evolution of AWS Import/Export process Move massive amount of data to and from AWS Data is encrypted at rest Data is encrypted at transit  Solutions:\n AWS Import/Export Ship an external hard drive to AWS. AWS plugs it in and copies the data to S3. AWS Snowball NAS in a box AWS ships to you. You can copy over 80TB of data and ship it back. Data will be available on S3. AWS Snowball Edge Similar to Snowball with some computing power, like Lambda and Clustering. AWS Snowmobile Shipping container full of storage (up to 100PB) and a truck to transport it.  "
},
{
	"uri": "https://majdarbash.github.io/python-2.7/4-python-conditional-control-flows/",
	"title": "Python 2.7: Conditional &amp; Control Flows",
	"tags": [],
	"description": "",
	"content": "Python supports:\n Standard comparison operators: \u0026lt;, \u0026gt;, ==, !=, \u0026lt;=, \u0026gt;=. Logical operators: and, or, not (not is evaluated first, then and and then or)  Conditional Clause if [condition]: [statements ...] else: [statements ...] More sofisticated example, would be:\nif [condition 1]: [statements ...] elif [condition 2]: [statements ...] else: [statements ...] Beware of the indentation as it will indicate how many statements to be executed within the condition success block. Example:\nx_int = 12 if not x_int \u0026gt; 20: print \u0026quot;The value of our variable is %s\u0026quot; % x_int print \u0026quot;We used an if condition here\u0026quot; Function Definition Function can be defined using the following syntax:\ndef [function name](): [statements ... ] Example of function definition:\n# function defintion - notice the indentation defining the scope def get_user_input(x_int): if x_int % 2 == 0: print \u0026quot;You've got an even number\u0026quot; else: print \u0026quot;You've got an odd number\u0026quot; # obtaining raw input represented as string x_string = raw_input(\u0026quot;Type your number: \u0026quot;) # converting the string to integer x_int = int(x_string) # calling the above defined function get_user_input(x_int) "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/architecting-to-scale/",
	"title": "Architecting to Scale",
	"tags": [],
	"description": "",
	"content": " Architectural Patterns  Loosely Coupled Architecture Horizontal Scaling vs. Vertical Scaling   Auto-Scaling  Type of Auto-Scaling Amazon EC2 Auto-Scaling Options Auto-Scaling Policy Scaling Cooldown Period   AWS Kinesis DynamoDB Scaling  DDB Terminology DDB Partitions and Scaling DynamoDB Accelerator - DAX   CloudFront SNS (Simple Notification Service) SQS  Queue Types   Amazon MQ AWS Lambda, Serverless Application Manager and EventBridge  AWS Serverless Application Model (AWS SAM)   Amazon EventBridge Simple Workflow Service (SWF) AWS Step Functions AWS Batch Elastic MapReduce  Components of Elastic MapReduce    Architectural Patterns Loosely Coupled Architecture Components can stand independently and require little or no knowledge of the inner workings of the other components.\nBenefits:\n Layers of Abstraction Permits more flexibility Interchangable components More atomic functional units  Horizontal Scaling vs. Vertical Scaling  Vertical scaling requires downtime Horizontal scaling is theoretically unlimited In horizontal scaling instances can be added on demand which may be a more cost effective solution Horizontal scaling can be automated while vertical scaling would require scripting Operations  Scale Out (horizontal) Scale In (horizontal) Scale Up (vertical) Scale Down (vertical)    Auto-Scaling Type of Auto-Scaling  Amazon EC2 Auto-Scaling Application Auto-Scaling  API used to to control scaling for resources other than EC2, like DynamoDB, ECS, EMR Provides a common way to interact with the scalability of resources   AWS Auto Scaling  Provides centralized way to manage scalability for whole stacks; Predictive scaling feature Console that can manage both of the above from a unified standpoint    Amazon EC2 Auto-Scaling Options  Maintain - Specific minimum number of instances running Manual - Use maximum, minimum or specific number of instances Schedule - Scale in/out based on schedule Dynamic - based on real-time metrics of the system  Auto-Scaling Policy  Target Tracking Policy Simple Scaling Policy Step Scaling Policy (More Sophisticated Logic)  Scaling Cooldown Period  Gives resources time to stabilize before automatically triggering another scaling event Different from health check period 300 seconds by default Automatically applies to dynamic scaling and optionally to manual scaling but not supported for schedule scaling  AWS Kinesis   Collection of services for processing streams of various data\n  Data is processed in \u0026ldquo;shards\u0026rdquo; - each shard can ingest 1000 records per second\n  Default limit of 500 shards\n  Record consists of Partition Key (128 bit MD5 hash), Sequence Number and Data Blob (up to 1MB)\n  Sequence numbers can be duplicated across Shards\n  Transient Data Store - default retention period of 24 hours, can be configured to up to 7 days\n  Kinesis Data Streams - Ingest and stores data streams for processing\n  Kinesis Firehose - Prepares and loads the data continously to the destinations you choose\n  Kinesis Data Analytics - Run standard SQL queries against data streams\n  DynamoDB Scaling  Throughput: Read/Write capacity units Max item size is 400KB There\u0026rsquo;s no limit on number of items  DDB Terminology  Parition - physical space where DDB data is stored Partition Key - Unique identifier for each record, also called Hash Key Sort Key - Optional second part of a composite key that defines storage order - sometimes called a Range Key  DDB Partitions and Scaling  Partitions have limitation of Capacity Units and Storage Number of Partitions required are determined by both factors  Capacity - RCU / 3000 + WCU / 1000 Storage - Total Size / 10GB Total Partitions = Round Up Max(Capacity, Storage)   RCU and WCU will be equality allocated across partitions Partition Key should be designed to have high avariability across paritions to distribute the WCUs and RCUs load across the partitions DynamoDB allows Auto-Scaling based on Target Utilization and Limits  Supports Global Secondary Indexes Uses Target Tracking method Doesn\u0026rsquo;t scale down if consumptions drops to zero Workaround1: send requests to table at minimal level Workaround2: manually reduce max capacity to be the same as minimum   DynamoDB supports On-Demand scaling  Costs more than traditional provisioning and auto-scaling    DynamoDB Accelerator - DAX  Sits in from of DDB and provides in-memory caching Micro-second level reads Good for read-intensive applications  CloudFront  Supports static / dynamic content at edge locations Supports Adobe Flash Media Server\u0026rsquo;s RTMP protocol Web Distributions support streaming through HTTP / HTTPS Origins can be S3, EC2, ELB and another Web Server Cache invalidation requests can delete the file from the edge location or you have to wait for TTL to expire Support Zone Apex (domain without subdomain infront of it) Supports Geo-Restriction  SNS (Simple Notification Service)  Enables Publish/Subscribe design pattern Topics - Channels for publishing notifications Subscriptions - configuring an endpoint to receive messages published to a topic  Endpoint options: HTTP/HTTPS, Email, SMS, SQS, Amazon Device Messaging (push notifications), Lambda   Supports Fan-out Architecture - helps achieve a loosely coupled architecture  SQS  Highly scalable hostead messaging queue Available integration with KMS for encrypting messages Transient Storage - default 4 days, max 14 days Supports first-in / first-out queueing Maximum message size of 256KB - Java SDK allows up to 2GB by utilizing S3 Allows Loosely Coupled Architecture  Queue Types  Standard Queue - no guarantee about the order of the messages FIFO Queue- maintains receiving order - holds all messages until a message is processed  Amazon MQ  Managed, HA Implementation of Apache ActiveMQ Similar to SQS, but a different implementation Supports different protocols Designed as a drop-in replacement for on-premise message brokers (Lift and Shift to the Cloud) Recommended to use SQS if you are building a new application from scratch  AWS Lambda, Serverless Application Manager and EventBridge  Run code on-demand without the need for infrastructure Supports Node.js, Python, Java, Go and C# Code is stateless - executed on an event basis (SNS, SQS, S3, DynamoDB Streams, etc.) Very useful for event driven architectures No limits to scaling a function since AWS dynamically allocates capacity in relation to events  AWS Serverless Application Model (AWS SAM)  Open source framework for building serverless apps on AWS Uses YAML as configuration language Includes CLI functionality to create, deploy and update serverless apps using AWS services such as Lambda, DynamoDB and API Gateway Enables local testing and debugging of apps using a Lambda-like emulator via Docker Extension of CloudFormation so you can use everything CloudFormation can provide by way of resources and functions AWS Serverless Application Repository - contains sample apps Serverless Framework is different from AWS SAM - supports other provides besides AWS  Amazon EventBridge  Ingest events from your own apps, SaaS and AWS Services Setup rules to filter and send events to targets  Simple Workflow Service (SWF)  Create distributed asynchronous systems as workflows Supports both sequential and parallel processing Best suited for human-enabled workflows, e.g. order fulfillment or procedural requests AWS recommends Step Functions over SWF for new applications Main Components: Activity Worker, Decider (Activity Workers are doing long-polling) AWS Simple Workflow is used when we need to support external processes processes or specialized execution logic (maybe beyond the scope of AWS)  AWS Step Functions  Managed Workflow and Orchestration platform Scalable and Highly Available Defined your app as a state machine Create tasks, sequential steps, parallel steps, branching paths or timers Amazon State Language declarative JSON Apps can interact and update the stream via Step Function API Visual Interface describes flow and realtime status Detailed logs for all the steps Out-of-the box coordination of AWS components (e.g. Order processing flow) Recommended by AWS over Simple Workflow Service for new applications  AWS Batch  Management tool for creating, managing and executing batch-oriented tasks using EC2 Instances   Create a Computer Environment: Managed/Unmanaged, Spot, On-Demand, vCPUs Create a Job Queue with priority and assigned to a Comput Environment Create Job Definition: Script/JSON, ENV vars, mount points, IAM role, container image, etc. Schedule the Job  Elastic MapReduce  Managed Hadoop framework for processing huge amounts of data Also supports Apache Spark, HBase, Presto and Flink Most common used for log analysis, financial analysis, or ETL (extract, transform and load) activities A Step is a programming task for performing some process on the data (i.e. count words) A Cluster is a collection of EC2 instances provisioned by EMR to run your steps Master Node, Core Node (HDFS), Task Node  Components of Elastic MapReduce  Hadoop HDFS - Distributed File System Hadoop MapReduce - Distributed Processing Flume - Log Collection ZooKeeper - Resource Coordination Sqoop - Data Transfer Oozie - Workflow Apache Pig - Scripting Hive SQL Mahout - Machine Learning HBase - Columnar Datastore Ambari - Management and Monitoring  "
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/2019-11-11-developing-ml-apps/",
	"title": "Developing Machine Learning Applications",
	"tags": [],
	"description": "",
	"content": " AWS SageMaker  Notebook Instance   AWS SageMaker Neo Machine Learning Algorithms  Linear Supervised Algorithms Non-Linear Supervised Algorithms Unsupervised Learning Algorithms Deep Learning  Convolutional Neural Networks (CNNs) Recurrent Neural Networks (RNNs)     Automatic Model Tuning  Hyperparameters Tuning Hyperparameters  Manual Brute Force Meta model SageMaker\u0026rsquo;s method     Advanced Analytics with Amazon SageMaker  Building and Training Machine Learning Models with Amazon SageMaker and Apache Spark  Building Machine Learning Pipelines using Spark and SageMaker      Anomaly Detection on AWS  Random Cut Forest Algorithm Architecture   Building Recommendation Systems with MXNet and GluOn  Collaborative Filtering  Matrix Factorization How to Choose a Model      AWS SageMaker AWS SageMaker is a fully managed service that allows the users to build, train and deploy machine learning models.\nUnderlying steps are fully managed by AWS through each of the stages:\n Build  Easily and quickly setup Jupyter Notebook instance   Train  Distributed training environment Spinning up a cluster of instances Loading Docker container which has an algorithm Bring in data from S3 Train the algorithms Output data back to S3 Tear-Down the cluster Allows you to use custom-build algorithm and bring in your own Docker container   Tune Deploy  Notebook Instance  Jupyter Notebooks  Open-source web application that allows users to author and execute code interactively Widely used by the data scientists community Can use any of AWS built-in models Docker image can be provided for endpoint deployment     AWS SageMaker Neo Challenges Faced\n Framework  Choose a framework best-suited for the task at hand   Models  Build the models using the chosen framework   Train  Train a mdoel using sample data to make accurate predictions on bigger data sets   Integrate  Integrate the model with the application   Deploy  Deploy the application, the model and the framework on a platform    AWS SageMaker Neo\n Helps developers take models trained in any framework and port them to any platform Converts model from Framework-specific format to portable code Optimizes the framework to run up to 2 times faster and 100x memory footprint reduction Supports populat deep learning and decision tree models Apache MXNet, TensorFlow, PyTorch and XGBoost Various EC2 instances and edge devices  AWS SageMaker Neo Components\n Neo Compiler Container  Takes the framework-specific model as an input Converts to framework-agnostic representation Optimzes the model performance Reduces the model\u0026rsquo;s runtime footprint   Shared Object Library   Machine Learning Algorithms Machine Intelligence\n Knowledge Acquisition  Inductive Reasoning Coming up with rules which would explain the observations   Inference  Ability to use acquired knowledge to derive truths Deductive Reasoning Predictions    Predicting Numbers\n Regression Problem  Linear Supervised Algorithms  Linear decision boundary  Hyperplane that best separates samples belonging to different classes   Linearly separable classes  If there exists a linear surface that separates 2 classses Error margin is expected in real-life situation   Support Vector Machine (SVM) Perceptron AWS SageMaker Linear Learner  Linear + Logstic Regression    Non-Linear Supervised Algorithms  Decision Tree RandomForest, XGBoost are based on these approaches Factorization Machines  Good for high dimensional sparse datasets Click prediction \u0026amp; item recommendation   Polynomial Neural Networks AWS SageMaker supports XGBoost - Gradient Boosted Trees  Unsupervised Learning Algorithms  Clustering  Given a collection of data points trying to divide the samples into clusters Assume that points belonging to the same cluster are somehow similar Number of clusters should be specified Different Algorithms and Hyperparameters will lead to different clusters What do clusters represent? (will need to be defined after running the algorithm)   Anomaly Detection  Random Cut Forest   Topic Modelling  Given collection of documents and number of topics to discover Algorithm produces top words appearing to define a topic   AWS  SageMaker Supports K-Means Clustering Principal Component Analysis (PCA)  Reduces dimensionality within a dataset Precursor to Supervised Learning   Latent Dirichlet Allocation (LDA)  Used for Topic Modelling by AWS Comprehend   Anomaly Detection  Kinesis Data Analytics Amazon SageMaker   Hot Spot Detection  Helps identify relatively dense regions in your data Supported Kinesis Data Analytics      Deep Learning  Neural Networks Neuron Back-propagation Algorithm  Factors in Advent of Deep Learning\n Algorithms Data Programming Models GPUs \u0026amp; Acceleration  Deep Neural Networks\n Image understanding Natural Language Processing Speech Recognition Autonomy  Networks with over thousand layers have been experimented with\n Training can be distributed across several instances AWS provides GPU powered instances on-demand  AWS SageMaker\n DeepAR Forecasting  Time Series Prediction    Convolutional Neural Networks (CNNs)  Breakthrough in deep learning Especially useful for image processing Able to corelated nearby pixels in an image instead of treating as completely independent input Convolution operation is applied to subsection of the image Use Cases  Object recognition, Image classification Semantic segmenetation Artistic style transfer Meow generator   AWS SageMaker  Image Classification (ResNet CNN)    Recurrent Neural Networks (RNNs)  Output of a Neuron is feeded to the Neuron itself Long Short-Term Memory (LSTM Network) AWS SageMaker  Sequence to Sequence (seq2seq)  RNN for text-summarization, translation, TTS       Automatic Model Tuning  Wraps up tuning jobs Works with custom algorithms or pre-built learning algorithms Helps find the best hyperparameters Improves performance of the machine learning model  Hyperparameters  Help tuning Machine Learning Model to get the best performance Have large influence on performance of ML Model Grows exponentially Non-linear / interact Expensive Evaluations  Neural Networks\n Learning Rate Layers Regularization Drop-out  Trees\n Number Depth Boosting step size  Clustering\n Number Initialization Pre-processing  Tuning Hyperparameters Manual  Defaults, guess and check Experience, intuition, and heuristics  Brute Force  Grid  We try out each possibility of hyperparameter values and compare based on this   Random  Randomly pick values for each of the hyperparameters   Sobol  Meta model  Builds another ML model on top of your ML model Objective to predict which hyperparameters which yield the best potential accuracy  SageMaker\u0026rsquo;s method  Gaussian process regression models objective metric as a function of hyperparameters  Assumes smoothness Low data Confidence estimates   Bayesian optimization decides where to search next  Explore and exploit Gradient free    SageMaker Integration\n Accepts SageMaker algorithms Frameworks Your own algorithm in a docker container  Flat Hyperparameters\n Continuous Integer Categorical  Advanced Analytics with Amazon SageMaker Building and Training Machine Learning Models with Amazon SageMaker and Apache Spark Apache Spark\n Powerful data processing tool Rich ecosystem Distributed processing  Spark and SageMaker Integration\n Spark runs locally on SageMaker notebooks The SageMaker-Spark SDK  Scala and Python SDK Amazon SageMaker algorithms are compatible with Spark MLLib There are Spark and Amazon SageMaker hybrid pipelines   Connect a SageMaker notebook to a Spark Cluster (e.g. Amazon EMR)  Spark / SageMaker Integration Components\n Spark DataFrame  Is a distributed data combined from different data sources   Estimator  Algorithm to use Parameters associated with the algorithm Types / Number of instances to host your model   Model  Training data is used to create a Model Model created    Building Machine Learning Pipelines using Spark and SageMaker Problem: Recognizing handwritten numbers 0-9 using MNIST data set.\nIn this example Apache Spark will pre-process data to do feature reduction using PCA (Principal Component Analysis). We instantiate PCA object proving the input: set of Features, and a target number of features k. PCA algorithm will choose the most significant features and return \u0026ldquo;Projected Features\u0026rdquo;. Those features will act as an input to the second stage in the pipeline.\nDefining the pipeline in Jupyter Notebook using both Spark SDK and SageMaker SDK will allow us to automate the process of pre-processing, training and deploying the model.\nAs a result we expect to have 2 Endpoints running on AWS SageMaker Infrastructure:\n Endpoint (PCA) Endpoint (K-Means)  This allows to fully de-couple the pre-processing task from prediction. On calling transform() function the Pipeline will first contact PCA Endpoint to reduce the features of the provided input and then will call K-Means Endpoint to get the prediction based on the Projected Features as an input.\nOur pipeline will consist of the following steps:\n Performing feature analysis/reduction on the input data set using PCA algorithm running on Apache Spark cluster  SageMaker Job will be created for running PCA feature reduction   Training on reduced feature data using K-Means algorith on AWS SageMaker  SageMaker Job will be created and will run automatically on completion of Step 1   Running Test-Data using the created AWS SageMaker Endpoint   Anomaly Detection on AWS Random Cut Forest Algorithm  Algorithm developed by AWS for Anomaly detection Improvement of an existing algorithm [\u0026ldquo;Isolation Forest\u0026rdquo;](/aws certified mls/2014/09/10/isolation-forest-algorithm.html) Published in ICML 2016 Incorporated into AWS Kinesis Data Analytics and AWS SageMaker  How it works\n Choose a bounding box randomly Choose the bigger dimension Perform the random cut Keep doing until each point is isolated Building a tree of random cuts Repeat the steps above to result in several trees, i.e. the Forest  Dealing with Stream of Data\n Reservoir Sampling  Maintain a random sample of 5 points in a stream Put first 5 observed data points in our sample buffer of size 5   For each new point, decide whether to keep or discard it  Flip a coin for each data point For the first data point, the probability of retaining the point is 50% For each new data point, it becomes less and less likely that the point will be retained   Produce a Random Cut Forest, with each tree looking into its own subsample of the stream For each new point we have to calculate the insertion point in the tree  Based on the insertion point in the tree we will have to perform displacements to accommodate the new point Average Displacement in Forest will represent Anomaly Score    Shingle\n Results can be improved using Shingling technique A single data point is replaced by a window of datas Shingle size will define the window size (e.g. 48 hours in the time axis)  Architecture Kinesis Streams\n Data from the source can be streamed to Kinesis Streams Kinesis Analytics can process the data from Kinesis Streams and label the Anomaly Score Using Amazon Kinesis Firehose the data can be redirected to S3 for further processing  SageMakers\n Data from the source can be passed to SageMaker through S3 After Training, Model will be deployed to the Endpoint Any new data can be checked for Anomaly using AWS Lambda function   Building Recommendation Systems with MXNet and GluOn Collaborative Filtering  User based recommenders  Identifies a short list of other users who are \u0026ldquo;similar\u0026rdquo; to you and rated an item. Assumes the average of their rating as your rating.   Item based recommenders  Identifies a short list of other items who are \u0026ldquo;similar\u0026rdquo; to the item in question and take a weighted average of the ratings for those top few items which you provided and predict that number as your likely rating for that item.    Matrix Factorization Factorizes a matrix to separate matrices, that when multiplied approximate to the completed matrix.\n Using GluOn Library MXNet with Linear Model MXNet with Non-Linear Model using Neural Network SageMaker  Points to Consider\n Matrix Factorization is ideal for small amounts of data Memory becomes a challenge with large data sets Factorization Machines and Distribution of the workload can help address the limitation and distribute the workload  Cold-Start Problem\n New users have no previous rating history New items have never been rated before Detecting similarity in items or users will help solve the Cold-Start Problem Content-based models  Lot of information can be extracted from item/user Search criterias, click behavior, etc.. provide more data supply whenever the rating history is not available    Hybrid Models\n Combine item/user-based models with content based  Semantic Models\n Finding similarities based on Semantic of the data (movie titles, description, untapped data like images) Using Deep Structure Semantic Models (DSSM)  How to Choose a Model "
},
{
	"uri": "https://majdarbash.github.io/python-2.7/5-python-classes/",
	"title": "Python 2.7: Classes",
	"tags": [],
	"description": "",
	"content": "In the example below of class syntax, the class in parantheses indicate the clsaas from which the new class inherits - in the current case it\u0026rsquo;s object.\nclass Person(object): \u0026quot;\u0026quot;\u0026quot;Person class\u0026quot;\u0026quot;\u0026quot; def __init__(self, name, phone, gender): self.name = name self.phone = phone self.gender = gender def description(self): print \u0026quot;Name: %s, phone: %s, gender %s\u0026quot; % (self.name, self.phone, self.gender) def is_male(self): if self.gender == 'male': return True else: return False def is_female(self): if self.gender == 'female': return True else: return False person = Person(\u0026quot;John\u0026quot;, \u0026quot;+011111xxxx\u0026quot;, \u0026quot;male\u0026quot;); # output: Name: John, phone: +011111xxxx, gender: male person.description(); # output: True print person.is_male(); ### Class scope example Notice how variable is_alive will be available to all the members of the Animal class: class Animal(object): is_alive = True def __init__(self, name): self.name = name zebra = Animal(\u0026quot;Jeffrey\u0026quot;) giraffe = Animal(\u0026quot;Bruce\u0026quot;) panda = Animal(\u0026quot;Tach\u0026quot;) print zebra.name, zebra.is_alive print giraffe.name, giraffe.is_alive print panda.name, panda.is_alive # output: Jeffrey True # output: Bruce True # output: Tach True Inheritance In the example below we have defined a class called Person which inherits from the object, and a class called Student, which inherits the Person.\nclass Person(object): def __init__(self, name, age): self.name = name self.age = age def description(self): print self.name, self.age class Student(Person): def __init__(self, name, age, studentId): self.studentId = studentId return Person.__init__(self, name, age) def info(self): print \u0026quot;Student id number is: %s\u0026quot; % (self.studentId) student = Student(\u0026quot;Tom Jones\u0026quot;, 26, 8928372) # output: Tom Jones 26 student.description() # output: Student id number is: 8928372 student.info() Accessing parent class using super() At any point in time you can access the parent / superclass of the current class by using super function:\nclass A(object): def test(self): print \u0026quot;This is the parent function\u0026quot; class B(A): def test(self): print \u0026quot;This is the overridden function in the child\u0026quot; def parent_test(self): super(B, self).test() # instantiating from the child class B instance = B() # accessing the test function # output: This is the overridden function in the child instance.test(); # accessing the parent function using super() # output: This is the parent function super(B, instance).test() # accessing the parent function using parent_test, which is using super() # output: This is the parent function instance.parent_test() Another example: using __repr__ and print methods: class Point3D(object): def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): return \u0026quot;(%d, %d, %d)\u0026quot; % (self.x, self.y, self.z) my_point = Point3D(1, 2, 3) # will use __repr__ function of the class # output: (1, 2, 3) print my_point. "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/business-continuity/",
	"title": "Business Continuity",
	"tags": [],
	"description": "",
	"content": " Concepts Disaster Recovery Architectures Storage Options Compute Options HA Approaches for Databases Network Options Failure mode and Effects Analysis (FMEA)  Concepts  Business Continuity (BC)  Seeks to minimize business activity disruption when something unexpected happens   Disaster Recovery (DR)  Act of responding to these events that threaten business continuity   High Availability  Designing in redundancies to reduce the chance of impacting service levels   Fault Tolerance  Design in the ability to tolerate faults   Service Level Argreement (SLA)  An agreed goal or target for a given service on its performance or availability   Recovery Time Objective (RTO)  Time taken after disruption to restore business processes to their levels   Recovery Point Objective (RPO)  Acceptable amount of data loss measured in time.    Business Continuity Plan specify RTO and RPO. RTO and RPO metrics then define amouint of investment on High Availability to be made. They also define what is the process for Disaster Recovery.\nTypes of Disaster:\n Hardware Failure Deployment Failure Load Induced (e.g. DDOS attacks) Data Induced Credential Expiration Dependency Infrastructure Identifier Exhaustion  Disaster Recovery Architectures  Backup and Restore   Requires Minimum entry point into AWS Minimal effort to configure Least flexible, off-site backup storage  Pilot Light   Minimal environment on standby on AWS for failover Switching to AWS will require a manual intervention It may take several minutes or hours to spin an environment AMIs should be up-to-date with on-prem counterparts  Warm Standby   Services are already up and running Could be considered as a shadown environment or production staging Resources could scale up to meet the incoming demand Process can be automated  Multi-Site   Ready at all time to take full production load Fails over in seconds or less No or little intervention required to fail over Most expensive DR option: can be considered as wasteful option Can be automatically configured through Route53 health checks  Storage Options Amazon EBS\n Annual Failure rate less than 0.2% Availability target of 99.999% (replicated within a single AZ) Vulnerable to AZ failure Easy to snapshot which is stored on S3 and multi-AZ durable You can copy snapshots to other regions Supports RAID configurations  Due to the facts that EBS operates over network, it\u0026rsquo;s not recommended to operate higher than RAID1    EBS RAID Configurations\n RAID0 (Striping)  No Redundancy Highest Speed Reads and Writes Highest Capacity   RAID1 (Mirroring)  1 drive can safely fail Slight decrease in reads and writes Capacity reduced by 50% due to mirroring   RAID5  Redundancy: 1 drive can safely fail 2 drives will store the data 1 drive stores the parity bit to be able to recreate the data good reads (similar to RAID0), but low writes capacity of (n-1)/n   RAID6  2 drives can safely fail minimum of 4 drives needed (2 parity) Same reads as RAID0, but worst writes capacity of (n-2)/n    S3 Storage\n Standard: 99.99% availability = 52 minutes/year Standard Infrequent Access: 99.9% = 9 hours/year One-zone Infrequent Access: 99.5% availability = 2 days/year Eleven 9s of durability: 99.99999999999% Standard \u0026amp; Standard-IA have multi-AZ durability. One-zone only as single AZ durability S3 is a backing service for many AWS services  Amazon EFS\n Implementation of the NFS file system True file system as opposed to block EBS or object storage (S3) File locking, strong consistency, concurrently accessible Each file object and metadata is stored across multiple AZs Can be accessed from all AZs concurrently Mount targets are highly available  Other Options\n Amazon Storage Gateway Snowball Glacier  Compute Options  Up-to-Date AMIs are critical for rapid fail-over AMIs can be copied to other regions for safety or DR staging Horizontally scalable architectures are preferred Reserved instances is the only way to guarantee that resources will be available when needed Auto Scaling and Elastic Load Balancing work together to provide automatic recovery by maintaining minimum instances Route 53 Health Checks also provide \u0026ldquo;self-healing\u0026rdquo; redirection of traffic  HA Approaches for Databases  If possible, choose DDB over RDS because of inherent faul tolerance Choose Aurora because of redundancy and automatic recovery features If aurora can\u0026rsquo;t be used choose multi-AZ RDS Frequent RDS snapshots can protect against data corruption or failure - and they wont\u0026rsquo; impact performance of multi-AZ deployment Regional replication is also an option, but will not be strongly consistent If hosting database on EC2, you have to develop your own HA plan  Redshift\n Currently Redshift doesn\u0026rsquo;t support multi-AZ deployments Best HA option is to use multi-node cluster which supports replication and node recovery Single node Redshift cluster does not support data replication - in case of failure will have to restore an S3 snapshot  Memcached does not support replication\n Use multiple nodes in each shard to minimize data loss on node failure Launch multiple nodes across available AZs to minimize data loss on AZ failure  Redis\n Use multiple nodes in each shard and distribute the nodes across multiple AZs Enable multi-AZ on the replication group to permit automatic failover if the primary node fails Schedule regular backups of your Redis cluster  Network Options  Subnets should be created in different AZs, resources should be allocated in multiple AZs Create at least 2 VPN tunnels to your Virtual Private Gateway Direct connect is not HA by default, you need to establish a secondary connection via another Direct Connect (ideally use another provider) or use a VPN Route 53\u0026rsquo;s Health Checks provide basic level of redirecting DNS resolutions Elastic IPs allow you flexibility to change backing assets without impacting name resolution For mutli-AZ redundancy of NAT Gateway, create gateways in each AZ with routes for private subnets to use the local Gateway  Failure mode and Effects Analysis (FMEA) FMEA is a systematic process to examine:\n What could go wrong What impact it might have What is the likelyhood of it occurring What is our ability to detect and react  Severity * Proability * Detection = Risk Priorty Number (RPN)\nSteps\n Round up Possible Failures Assign scores for each failure mode: customer impact, likelihood, detect and react -\u0026gt; calculate RPN Prioritize on Risk - implement mitigation plan, additional redundancy  "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/deployment-and-operations/",
	"title": "Deployment and Operations Management",
	"tags": [],
	"description": "",
	"content": " Software Deployments CI, CD, CD Elastic Beanstalk CloudFormation Elastic Container Service API Gateway Management Tools Enterprise Applications AWS Machine Learning Landscape  AI Services ML Services ML Frameworks \u0026amp; Infrastructure    Software Deployments Types of Deployment:\n Big Bang Phased Rollout Parallel Adoption  Deployment Strategies:\n Rolling Deployment  Changing launch configuration to specific version will rollout the changes   A/B Testing  Using Route53 we can specific the ALB which gets the traffic   Canary Release  Deploy new version on production - if no errors are detected, deploy the rest   Blue-Green Deployment  Create new ALB and ASG with new version Switch to the new version using Route53 Switch back to the old version using Route53 The goal of blue/green deployment is to achieve immutable infrastructure - you don\u0026rsquo;t make changes to your application after it\u0026rsquo;s deployed but redeploy altogether How to achieve Blue-Green Deployment on AWS:  Update DNS with Route53 to point to new ELB or instance Swap ASG of new instances behind the ELB Change ASG LC to use new AMI and terminate old instances Swap environment URL on Elastic Beanstalk Clone stack using AWS Opswork and update DNS      Blue-Green Deployments are not recommended:\n If the code is very tightly coupled with database schema  Schema should be forward and backward compatible in best case   The upgrade requires special upgrade routines to be run during deployment Off-the-shelf products might not be blue-green friendly  CI, CD, CD  Continuous Integration  Merge code changes to main branch as frequently as possible with automated testing as you go   Continous Delivery  You have automated your release process to the point you can deploy at the click of a button   Continuous Deployment  Each code change that passes all stages of the release process is released to production with no human intervention required    Continuous Integration Pipeline:\n Get latest from Repo Make changes Unit Testing Commit to Repo Integration Testing Acceptance Testing Deploy to Production Smoke Testing  CI/CD Consideration:\n Objective is to create smaller, incremental compartmentalized improvements and features Lowers deployment risk and tries to limit negative impact Test Automation must be strong Feature toggle patterns useful for dealing with in-progress features not read for release (versus more traditional branching strategies) Microservice architectures lend themselves well to CI/CD practices  AWS Development Lifecycle Tools\n AWS CodeCommit AWS CodeBuild AWS CodeDeploy AWS CodePipeline AWS X-Ray AWS CodeStar  Elastic Beanstalk  Orchestration service to make it easy to deploy scalable web packages Wide range of supported platforms - Docker, PHP, Java, NodeJS Multiple Environments within Application (DEV, QA, PRD\u0026hellip;) Great for ease of deployment, but not great if you need a lot of control and flexibility  Deployment Options\n All At Once  minimal deployment time, downtime, manual rollback process   Rolling  one by one - terminates old version instances and replaces the new instances no downtime expected, manual rollback process   Rolling with Additional Batch  launch new version instances, and then take old instances out of service no downtime, manual rollback process   Immutable  launch a full set of instances in a separate ASG and cut over when health checks are passed no downtime, terminate new instances to rollback   Blue/Green  CNAME DNS entry is changes when new version is up, old version is preserved no downtime, rollback is achieved through URL SWAP    CloudFormation  Infrastructure as Code JSON/YAML to model and provision entire landscapes Repeatable, automatic deployments and rollbacks Nest components for reusability Supports over 300 Resource Types Supports custom resources via SNS / Lambda  Concepts\n Templates Stacks Change Sets  Stack Policies\n Protect certain resources from being unintentionally deleted or updated Once created, stack policy cannot be deleted, but can be modified via the CLI  CloudFormation Best Practices\n AWS Provides Python \u0026ldquo;helper scripts\u0026rdquo; which can help you install software and start services on EC2 CloudFormation should be used to make changes - don\u0026rsquo;t apply changes directly to the resources Make use of Change Sets to identity potential trouble spots Use Stack Policies to protect against accidential changes for sensitive resources Use version control systems like CodeCommit or Github to track changes to templates  Elastic Container Service There are 2 main services:\n ECS: Elastic Container Service  Managed, highly available and highly scalable platform AWS-specific platform that supports Docker containers Leverages AWS services: Route53, ALB, CloudWatch, etc. Collection of containers are called tasks, tasks provide a service Limited extensibility   EKS: Elastic Kubernetes Service  Managed, highly available and highly scalable platform Comptaible with upstream K8s so its easy to lift and shift from other K8s A hosted K8s platform that handles many things internally Collection of containers is called \u0026ldquo;Pods\u0026rdquo;. They can share resources and access to each other Extensible via a wide variety of third-party community ad-ons    Launch Types:\n Amazon EC2 Launch Type  You provision EC2 instances You are responsible for upgarding, patching and taking care of EC2 pool You must handle cluster optimization Allows more granular control over infrastructure   Amazon Fargate Launch Type  Fargate automatically provisions underlying resources Fargate provisions compute as needed Fargate handles cluster optimization Limited control, infra is automated    API Gateway  Managed, high availability service to front-end REST APIs Backed with custom code via Lambda, as a proxy for another AWS Service or any other HTTP API on AWS or elsewhere Regionally based, private or edge optimized (deployed via CloudFront) Supports API Keys, Usage Plans for user identitifcation, throttling and quota management Can be published as product and monetized via AWS marketplace API Gateway can cache responses  Management Tools AWS Config\n Allows to assess, audit and evaluate configuration of your AWS resources Very useful for Configuration Management as part of ITIL Program Creates baseline for various configuration settings and files - tracks variations against tha baseline AWS Config rules can check resources for certain desired conditions and if violations are found, the resources is flagged as \u0026ldquo;noncompliant\u0026rdquo;  AWS OpsWorks\n Managed instance of Chef or Puppet Provide configuration management to deploy code, automate tasks, configure instances, perform upgrades, etc. Has three offerings  OpsWorks for Chef Automate OpsWorks for Puppet OpsWorks Stacks  AWS creation compatible with Chef recipe Uses Chef solo client installed on EC2 instances to run Chef recipes Supports on-prem servers with agent installed      AWS OpsWorks Stacks\n Stacks  Collection of resourced needed to support a service or application   Layers  Different components of application delivery hierarchy Stacks can be cloned - but only within the same region When you create stack you must specify a region, this stack can control only resources in that region    AWS System Manager\n Centralized console for a wide variety of system management tasks Designed for managing a large fleet of systems - tens or hundreds SSM Agent enables System Manager features and support all OSs supported by OS as well as back to Windows Server 2003 SSM Agent is installed by default on recent AWS-provided base AMIs for Linux and Windows Manages AWS-based and on-prem based systems via the agent  Services:\n Inventory - collects information from managed instances about applications, versions, meta data State Manager - create states that represent a certain configuration is applied to instances Logging - CloudWatch Log agent and stream logs directly to CloudWatch from instances Parameter Store - storing configuration data, connecton strings, passwords Insights Dashboard - account-level view of Cloudtrail, Config, Trust Advisor Resource Groups - group resource through tagging for organization Mantenance Window - defined schedules for instances to patch, update apps, run scripts and more Automation - automating routine maintenance tasks and scripts Run Command - Run commands without SSH or RDP Patch Manager - automates the application of certain patches  uses baselines to defined which patches are auto-approved for update (e.g. pre-defined baseline for Windows Server 2008 - 2016) supports creation of custom basedline to define applying critical, optional, and important updates   AWS System Manager Documents  defines the actions that Systems Manager performs on your managed instances    AWS System Manager Documents\n Command Document  Run Command and State Manager Used to execute commands. State Manager uses command documents to apply a configuration. These actions can be run on one or more targets at any point during the lifecycle of an instance.   Policy Document  Used wiht State Manager Enforce a policy on your targets   Automation Document  Used with Automation Used when performing common maintenance and deployment tasks    Enterprise Applications  Amazon App-Stream  Enables user to run apps in the browser   AWS Client VPN Amazon Chime  Online meeting and video conferencing service   Alexa for Business  Deploy Alexa functionality and skills internally Management functionality more appropriate for an enterprise organization than buying and provisioning individual Alexa device   Amazon WorkLink  Provides secure access to internal web applications for mobile devices When mobile user requests an app it\u0026rsquo;s rendered on a secure machine then image is sent to the mobile client   Amazon WorkSpaces  Workspace as a service   AWS Connect  Fully managed contact center with configurable call handling, inbound / outbound telephony, interactive voice response, chatbot and analytics   Amazon WorkDocs  Document storage and collaboration platform: supports version management, sharing and collaborative edits   Amazon WorkMail  Fully managed email and calendar as a service Comptaible with Microsoft Exchange (Outlook), IMAP, Android and iOS clients    AWS Machine Learning Landscape AI Services App Developers, no ML experience required\n Amazon Comprehend Amazon Lex Amazon Polly Amazon Rekognition Amazon Translate Amazon Transribe Amazon Personalize  ML Services ML Developers and Data Scientists\n Amazon Sage Maker  Ground Truth Notebooks Training Hosting Algorithms Marketplace    ML Frameworks \u0026amp; Infrastructure ML Researchers and Academics\n Frameworks  MX Net TensorFlow   Interfaces  Gluon Keras   Amazon GreenGrass Amazon EC2 AWS Deep Learning AMIs  "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/cost-management/",
	"title": "Cost Management",
	"tags": [],
	"description": "",
	"content": " Concepts Cost Optimization Strategies  Appropriate Provisioning Right-Sizing Purchase Options Managed Services Optimized Data Transfer   Tagging and Resource Groups  Resource Groups Resource Groups   Spot Instances and Reserved Instances  Reserved Instances Spot Instances   Cost Management Tools  Concepts  CapEx (Capital Expenses) OpEx (Operational Expenses) Total Cost of Ownership (TCO)  A comprehensive look at the entire cost model of a given decision or option often including both hard costs and soft costs.   Return on Investment (ROI)  The amount an entity can expect to receive back within a certain amount of time given an investment.    Cost Optimization Strategies Appropriate Provisioning  Provision the resources you need and nothing more Consolidate where possible for greater density and lower complexity (multi-database RDS, containers) CloudWatch can help by monitoring utilization  Right-Sizing  Using lowest-cost resource that still meets the technical specifications. Architecting for the most consistent use of resources is best versus spikes and valleys. Loosely coupled architectures using SNS, SQS, Lambda and DynamoDB can smooth demand and create more predictability and conistency.  Purchase Options  For permanent applications or needs, Reserved Instances provide the best cost advantage. Spot instances are best for temporary horizontal scaling. EC2 fleet lets you define target mix of On-Demand, Reserved and Spot instances to meet your needs. AWS Pricing can vary from region to region Consider potential savings by location resources in a remote region if local access is not required. Route53 and CloudFront can be used to reduce latency of a remote region.  Managed Services  Leverage managed services such as MySQL RDS over self-managed options such as MySQL on EC2 Cost savings gained through lower complexity and manual intervention (saving on soft costs)  Optimized Data Transfer  Data going out and between AWS regions can become a significant cost component. Direct Connect can be a more cost-effective option given data volume and speed.  Tagging and Resource Groups Resource Groups  Tags are arbitrary name/value pairs that you can assign to virtually all AWS assets to serve as metadata. Tagging strategies can be used for Cost Allocation, Security, Automation, and many other uses.  Example: tag used in IAM policy to implement access controls to certain resources.   Enforcing standardized tagging can be done via AWS Config Rules or custom scripts Most resources can have up to 50 tags.  Resource Groups  Resource Groups are grouping of AWS assets defined by tags. Create custom consoles to consolidated metrics, alarms and config details around given tags.  Common Resource Groupings:\n Environments - DEV,QA,PRD Project Resources Collection of resources supporting key business processes Resources allocated to various departments or cost centers  Spot Instances and Reserved Instances Reserved Instances  Purchase usage of EC2 instance in advance for a significant discount over On-Demand pricing Provides capacity reservation when used in a specific AZ AWS Billing automatically applies discounted rates when you launcgh an instance that matches your purchased RI. EC2 has three RI types: Standard, Convertible and Scheduled. RIs can be shared across multiple accounts within Consolidated Billing. If you find you don\u0026rsquo;t need your RI\u0026rsquo;s, you can try to sell them on the Reserved Instance Marketplace.  Standard vs. Convertiable Reserved Instances\n Terms: 1 / 3 year(s) Average Discount: Standard - 40%-60%, Convertible - 31%-54% Change AZ, Instance Size, Networking Type: yes Change instance family, OS, Tenancy, Payment Options: Standard - no, Convertible - yes Benefit from Price Reductions: Standard - no, Convertible - yes Sellable on RI Marketplace: Standard - yes, Convertible - coming soon  RI Attributes:\n Instance type Platform Tenancy - Defaul / shared Availability Zone (optional)  if zone is selected, discount applies to AZ - Zonal RI if no zone is specified, no reservation is created but the discount is applied to any instance in the family in any AZ in the region - Regional RI Zonal RI can be changed to Regional RI via console or AWS API Instance size flexibility is only available for Linux/Unix Regional RIs with default tenancy. Not available for Windows, RHEL, or SLES.    Spot Instances  Excess EC2 capacity that AWS tries to sell on a market exchange basis Customer creates a Spot Request and specifies AMI, desired instance types, and other key information Customer defines highest price willing to pay for instance. If capacity is constrained and others are willing to pay more, your instance might get terminated or stopped. Spot request can be a \u0026ldquo;fill and kill\u0026rdquo;, \u0026ldquo;maintain\u0026rdquo; or \u0026ldquo;duration-based\u0026rdquo; For \u0026ldquo;One-Time Request\u0026rdquo;, instance is terminated and ephemeral data is lost. For \u0026ldquo;Request and Maintain\u0026rdquo;, instance can be configured to Terminate, Stop or Hivernate until price point can be met again. Price and demand fluctuate between AZs  Dedicated Instances and Hosts Dedicated Instances:\n Dedicated instance is virtualized instances on hardware just for your AWS account May share hardware with other non-dedicated instances in the same account Available as On-Demand, Reserved Instances and Spot Instances Cost additional 2# per hour per region  Dedicated Host:\n Physical serveres dedicated to just your use You then have control over which instances anre deployed on that host. Available as On-Demand or with Dedicated Host Reservation Useful if you have server-bound software licenses that use metrics like per-core, per-socket or per-VM Each dedicated host can only run one EC2 instances size and type  Cost Management Tools  AWS Budgets  Set predefined limits and notifications Can be based on Cost, Usage, Reserved Instance Utilization or RI Coverage Useful as a method to distribute cost and usage awareness and responsibility to platform users   Consolidated Billing  Enable a single Payer account that\u0026rsquo;s locked down to only those who need access. Economies of scale by bringing together resource consumption across accounts.   Trusted Advisor  Run a series of checks on your resources and proposes suggested improvements Can help recommend cost optimization adjustments like RIs or scaling adjustments Core checks are available to all customers Full Trusted Advisor benefits require a Business or Enterprise support plan    "
},
{
	"uri": "https://majdarbash.github.io/aws/",
	"title": "AWS WhitePapers",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/",
	"title": "AWS Certified MLS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csap/",
	"title": "AWS Certified SAP",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/",
	"title": "AWS Certified SAA",
	"tags": [],
	"description": "",
	"content": "Please note that the content below is based from my notes.\n"
},
{
	"uri": "https://majdarbash.github.io/genetic-algorithm/",
	"title": "Genetic Algorithm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/ml-neural-networks/",
	"title": "Neural Networks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/blockchain/",
	"title": "Blockchain",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/python-2.7/",
	"title": "Python 2.7",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/symfony/",
	"title": "Symfony 3",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/software-architecture-and-design/",
	"title": "Software Architecture &amp; Design",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/",
	"title": "Random",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/aws-applications/",
	"title": "AWS Applications",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/categories/aws-certified-saa/",
	"title": "AWS Certified SAA",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/",
	"title": "Docs",
	"tags": [],
	"description": "",
	"content": "Docs "
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/ha-arhictecture-high-availability/",
	"title": "HA Arhictecture: High Availability",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/route53/",
	"title": "Route53",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/iam/",
	"title": "IAM",
	"tags": [],
	"description": "",
	"content": "IAM (Identity Access Management)"
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/databases/",
	"title": "Databases",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/serverless/",
	"title": "Serverless",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/aws-certification-solution-architect-iam-s3/",
	"title": "S3",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2017/09/30/docker-building-nginx-cookbook-in-dockerized-container/",
	"title": "How to build Docker Image for NGINX using Chef (chef-solo)?",
	"tags": [],
	"description": "",
	"content": "package [\u0026lsquo;nginx\u0026rsquo;] do action :install end\npackage [\u0026lsquo;php7.0-fpm\u0026rsquo;] do action :install end\npackage [\u0026lsquo;php-imagick\u0026rsquo;, \u0026lsquo;php-curl\u0026rsquo;, \u0026lsquo;php-gd\u0026rsquo;, \u0026lsquo;php-mcrypt\u0026rsquo;, \u0026lsquo;php-xml\u0026rsquo;, \u0026lsquo;php-mbstring\u0026rsquo;, \u0026lsquo;php-soap\u0026rsquo;, \u0026lsquo;php-mysql\u0026rsquo;, \u0026lsquo;php-pear\u0026rsquo;] do action :install end\npackage [\u0026lsquo;ntp\u0026rsquo;, \u0026lsquo;htop\u0026rsquo;, \u0026lsquo;vim\u0026rsquo;] do action :install end\ntemplate \u0026lsquo;/etc/nginx/sites-enabled/default\u0026rsquo; do source \u0026lsquo;default.conf\u0026rsquo; end Default server configuration  server { listen 80 default_server; listen [::]:80 default_server;\n# SSL configuration # # listen 443 ssl default_server; # listen [::]:443 ssl default_server; # # Note: You should disable gzip for SSL traffic. # See: https://bugs.debian.org/773332 # # Read up on ssl_ciphers to ensure a secure configuration. # See: https://bugs.debian.org/765782 # # Self signed certs generated by the ssl-cert package # Don't use them in a production server! # # include snippets/snakeoil.conf; root /app; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; server_name _; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ =404; } # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # location ~ \\.php$ { include snippets/fastcgi-php.conf; # With php7.0-cgi alone: # fastcgi_pass 127.0.0.1:9000; # With php7.0-fpm: fastcgi_pass unix:/run/php/php7.0-fpm.sock; } # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #}  }\nVirtual Host configuration for example.com  You can move that to a different file under sites-available/ and symlink that to sites-enabled/ to enable it.  #server {\nlisten 80; listen [::]:80;  server_name example.com;  root /var/www/example.com; index index.html;  location / { try_files $uri $uri/ =404; } #} RUN apt-get -y update RUN apt-get -y install python-software-properties RUN apt-get -y update\nADD ./Berksfile /Berksfile ADD ./solo.rb /var/chef/solo.rb ADD ./solo.json /var/chef/solo.json ADD ./cookbooks /var/chef/cookbooks\nRUN cd / \u0026amp;\u0026amp; /opt/chef/embedded/bin/berks vendor /var/chef/cookbooks RUN chef-solo -c /var/chef/solo.rb -j /var/chef/solo.json\nCMD service php7.0-fpm start \u0026amp;\u0026amp; nginx -g \u0026ldquo;daemon off;\u0026rdquo; docker run -d -p 8080:80 majdarbash/nginx bash (overriding CMD entry point, to access bash, e.g. troubleshopt) running with mounted volume in www (note that nginx will serve from /app based on the chef template above) docker run -v ~/Developer/docker-chef-solo-skeleton/www:/app -p 8080:80 -d majdarbash/nginx\npushing the image to the repository docker push majdarbash/nginx docker run -v [local_directory]:/app -p 8080:80 -d majdarbash/nginx "
},
{
	"uri": "https://majdarbash.github.io/categories/random/",
	"title": "Random",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2017/09/30/docker-provisioning-docker-container-with-chef-solo/",
	"title": "How to provision Docker Images using Chef-Solo?",
	"tags": [],
	"description": "",
	"content": "\u0026mdash;\u0026mdash;\u0026ndash; added part \u0026mdash;\u0026mdash;\u0026mdash;- RUN apt-get -y update RUN apt-get -y install python-software-properties RUN apt-get -y update\nADD ./Berksfile /Berksfile ADD ./solo.rb /var/chef/solo.rb ADD ./solo.json /var/chef/solo.json\nRUN cd / \u0026amp;\u0026amp; /opt/chef/embedded/bin/berks install \u0026ndash;path /var/chef/cookbooks RUN chef-solo -c /var/chef/solo.rb -j /var/chef/solo.json RUN echo \u0026ldquo;daemon off;\u0026rdquo; \u0026raquo; /etc/nginx/nginx.conf\nCMD [\u0026ldquo;nginx\u0026rdquo;] "
},
{
	"uri": "https://majdarbash.github.io/categories/cheat-sheets/",
	"title": "Cheat Sheets",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2017/09/30/markdown-cheat-sheet/",
	"title": "CS: Markdown",
	"tags": [],
	"description": "",
	"content": "Alternatively, for H1 and H2, an underline-ish style:\nAlt-H1 Alt-H2 {% endraw %}Strong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this. {% endraw %}Some text that should be aligned with the above item.\n Unordered list can use asterisks   Or minuses   Or pluses {% endraw %}  I\u0026rsquo;m a reference-style link\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\n{% endraw %}Inline-style: Reference-style: {% endraw %}s = \u0026#34;Python syntax highlighting\u0026#34; print s No language indicated, so no syntax highlighting. But let\u0026amp;#039;s throw in a \u0026lt;b\u0026gt;tag\u0026lt;/b\u0026gt;. {% endraw %}Quote break.\n This is a very long line that will still be quoted properly when it wraps. Oh boy let\u0026rsquo;s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. {% endraw %} Quote break.\n This is a very long line that will still be quoted properly when it wraps. Oh boy let\u0026rsquo;s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. {% endraw %}  Hyphens\n Asterisks\n Underscores\n{% endraw %}This line is separated from the one above by two newlines, so it will be a separate paragraph.\nThis line is also a separate paragraph, but\u0026hellip; This line is only separated by a single newline, so it\u0026rsquo;s a separate line in the same paragraph.\n{% endraw %}"
},
{
	"uri": "https://majdarbash.github.io/2017/09/29/docker-cheat-sheet/",
	"title": "CS: Docker Commands",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2017/08/19/best-safest-way-upgrade-wordpress-site/",
	"title": "How to upgrade Wordpress? Best and safest way to upgrade",
	"tags": [],
	"description": "",
	"content": "tar czf backup.tgz /var/www/[your site directory] mysqldump -uroot -p [database name] \u0026gt; database.sqlcd /var/www/[your site directory] cp -avr /tmp/wordpress/* . rm -rf /tmp/wordpress /tmp/latest.zip// the debug log will be saved to wp-content/debug.log define( \u0026lsquo;WP_DEBUG_LOG\u0026rsquo;, false);"
},
{
	"uri": "https://majdarbash.github.io/2017/06/27/angularjs-seo-implications/",
	"title": "Overcoming SEO challenges with AngularJS 1-based Applications",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2017/03/15/php-coding-style-fixer/",
	"title": "CI Pipeline: Unifying and automating code using Coding Style Fixer",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2017/02/08/testing-installed-ssl-certificate/",
	"title": "How to test your SSL installation?",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2017/01/23/learning-algorithm-neuron/",
	"title": "Closer look into Learning Algorithm of a Neuron",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/categories/ml-neural-networks/",
	"title": "ML Neural Networks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2017/01/15/perceptrons/",
	"title": "First Generation of Neural Networks: Perceptrons",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2016/12/05/downloading-m3u8-videos/",
	"title": "How to download the streaming m3u8 videos formats?",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2016/10/20/react-native-the-shoe-app/",
	"title": "React Native: Building the &#34;Shoes&#34; App",
	"tags": [],
	"description": "",
	"content": "run your app on Android react-native run-androiddirectory listing ls -Fimport React, { Component } from \u0026lsquo;react\u0026rsquo;; import { AppRegistry, StyleSheet, Text, View } from \u0026lsquo;react-native\u0026rsquo;;\nexport default class Shoes extends Component { render() { return ( Welcome to React Native! To get started, edit index.ios.js Press Cmd+R to reload,{'\\n\u0026rsquo;} Cmd+D or shake for dev menu ); } }\nconst styles = StyleSheet.create({ container: { flex: 1, justifyContent: \u0026lsquo;center\u0026rsquo;, alignItems: \u0026lsquo;center\u0026rsquo;, backgroundColor: \u0026lsquo;#F5FCFF\u0026rsquo;, }, welcome: { fontSize: 20, textAlign: \u0026lsquo;center\u0026rsquo;, margin: 10, }, instructions: { textAlign: \u0026lsquo;center\u0026rsquo;, color: \u0026lsquo;#333333\u0026rsquo;, marginBottom: 5, }, });\nAppRegistry.registerComponent(\u0026lsquo;Shoes\u0026rsquo;, () =\u0026gt; Shoes);"
},
{
	"uri": "https://majdarbash.github.io/categories/events/",
	"title": "Events",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2016/09/29/facebook-developers-29th-september-2016-dubai/",
	"title": "Facebook For Developers - 29th September 2016, Dubai",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/.svn/",
	"title": ".svn",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/.svn-files/",
	"title": ".svn files",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/genetic-algorithm/01-algorithm-overview/",
	"title": "1.1 - Genetics Algorithm - Algorithm Overview",
	"tags": ["artificial intelligence", "genetics algorithm", "school timetabling problem"],
	"description": "",
	"content": "1.1.1 – Introduction \nGenetic Algorithm is a directed search algorithms based on the mechanics of biological evolution developed by John Holland, University of Michigan (1970’s) to understand the adaptive processes of natural systems and design artificial systems software that retains the robustness of natural systems this algorithms provide efficient, effective techniques for optimization and machine learning applications so it is widely used today in business, scientific and engineering circles.\nSome GA Application Types:\nDomain\nApplication Types\n1.1.2 - Description 1.1.2.1 - Charles Darwin Theory Charles Darwin Theory: “Living organisms are fighting the forces of nature to survive. Those who are the fittest (strongest, fastest, biggest) are most likely to survive, Those who survive mate and reproduce (selection) and children are similar (inheritance), but not exactly like parents because of cross-fertilization and mutation, thus children can be more or less fitness than parents also, Children repeat the path of their parents, after several generations the organisms become much fitter.”\n1.1.2.2 - Genetic algorithm Suppose that there are many possible solutions for the problem: x1, x2, x3, x4… The main idea is to view each solution xi of the problem as an individual living organism. The number of possible solution can be incredibly large n ◊ ∞, so we consider m \u0026lt; n and chose a Population: P(t) = {x1t, x2t, … , xmt} ,with time the organisms and the whole population will be evolving.\n1.1.2.3 - Crossover Crossover is replacing some genes in the parent by the corresponding genes of the other.\nFor example:\nP1 = 101 | 0010 ⇒ O1 = 101 | 1001\nP2 = 011 | 1001 ⇒ O2 = 011 | 0010\n1.1.2.4 - Mutation Mutation is randomly choosing a gene and replacing it with other gene; mutation helps to add diversity to the population and help avoiding local maximum\nFor example:\nO1 = 1011001 ⇒ O1 = 1001101\n1.1.2.5 - Fitness Function Fitness Function is a function that tells you how good the individual is.\n\n1.1.2.6 - Selection Methods Select some of the population for reproduction there are several method for selection\n Roulette Wheel selection: Probability selection from all population with probability proportional of their fitness Ranked selection: few fittest individual  1.1.2.7 - Summary Genetic Algorithm Summary:\n Choose the initial population Select parent chromosomes Perform crossover Perform mutation Evaluate fitness of the new population Repeat 2 until satisfied  "
},
{
	"uri": "https://majdarbash.github.io/genetic-algorithm/02-problem-specification/",
	"title": "1.2 - Genetics Algorithm - Problem Specification",
	"tags": ["artificial intelligence", "genetics algorithm", "school timetabling problem"],
	"description": "",
	"content": "1.2.1 - Introduction We have a set of teachers who needs a schedule for a set of subjects over a set of periods, this schedule must take into account characteristics of periods of week, the limited availability of teacher and rooms, and some constraints which should be fulfilled.\nThe problem of assigning teachers to subjects and groups for timetabling problem has been studied by many authors. Tillet (1975) was the first to address it, using an integer linear programming model.\nThen other authors like Shih and Sullivan (1977), Schniederjans and Kim (1987) and also Badri et al. (1998) developed linear programming models a different approach is followed by Dyer and Mulvey (1976), Bloomfield and McSharry (1979), Mulvey (1982) and Dinkel et al. (1989) who developed capacitated flow models for the problem.\n1.2.2 - Problem Description Our timetabling problem has the following elements:\n A set of groups: A set of subjects: A set of teachers: we have to types of them:   Full time teachers: are all week periods free teachers, they can attend at any time and have no forbidden periods at any time of the week. Part time teachers: they have certain periods to attend in, we must respect those periods in assigning subject.   A set of periods of the school week: the week is divided into five days each day has Five periods the total number of periods for each group is 5*5=25 period, it means a maximum of 30 lessons per week to each group.  1.2.3 - Problem Elements (Objectives and Constraints of the Problem) In a course timetabling problem, generally, constraints are considered in two types. One of them is called hard constraints. Every acceptable timetable must satisfy these Constraints, they are:\n (H1) Every lesson has to be assigned to a period or a number of periods, depending on the lesson\u0026rsquo;s length. (H2) No teacher can give two simultaneous lessons. (H3) No room can be assigned to two simultaneous lessons. (H4) No teacher can give a subject which is out of his specialist, or the set of subjects he can give. (H5) Forbidden periods of the teacher must be respected (no part time teacher can give out of his available times. (H6) The teacher of a specified subject of a certain group must be unique.  The other type of constraints is the soft type, without those constraints an acceptable solution will be generated but the more we stick to those constraints the higher quality of the solution will be fulfilled.\nThose soft constraints are:\n (S1) All lessons of the same section cannot be assigned to the same day must be respected. (S2) Class timetables should be as compact as possible, eliminating idle times for students. (S3) Class timetables should be as compact as possible, eliminating idle times for students. (S4) In class timetables, if possible students should not have a day with a single lesson.  1.2.4 - Genetic Algorithm Application 1.2.4.1 - Flow Chart  Choose the initial population. Select parent chromosomes. Perform Crossover. Perform Mutation. Evaluate fitness of the new population. Repeat 2 until satisfied.  1.2.4.2 - Elements of Our Solution  Nod x: which is a set of variables x (R, S, T, P)R: represents room\nS: represents subject\nT: represents teacher\nP: represents period\nthe number of those variables is equal to the number of all lessons of all groups of all classes. The initial solution (chromosomes):\nWe start working from the initial solution, which is the totally randomly generated population. When starting working we can reach the newer and newer population with better fitness function results, until totally satisfying the hard constraints. When all the hard constraints are satisfied we start working on the soft constraints until they are satisfied too. It’s essential during the work of this algorithm that all the hard constraints should be satisfied, while it’s not such essential that the soft constraints are satisfied. The chromosome is deemed to be a solution, if it satisfies all the hard constraints, and the optimal solution is the one that satisfies all the soft constraints too, producing a fitness function value of 0.   Parent Chromosomes:\nParent chromosomes represent a parent with 2 children – male and female chromosomes, which are the actual data of the parent chromosome. These chromosomes are named as parent chromosomes, because they will reproduce using the crossover and mutation modifications to produce newer and more adaptive to the environment chromosomes, which will be decided by the fitness function, and choosing the most fitted parents for the next population.   Population:\nPopulation represents the current state in the process of the evolution. Population is a set of chromosomes, which is limited to specific size. By continuous reproduction, and choosing the best fit for the next population, the present population is expected to develop to produce better and better results. Crossover Point:\nCrossover is one of the methods used for reproduction. Using this method reproduction is being made. This reproduction method is based on taking parent chromosomes and choosing a point – crossover point, where each one of the parent chromosomes will be divided into two parts. Then, the parent chromosomes reproduce by exchanging their parts, to produce two new chromosomes.One of the parameters needed to be identified how the crossover points are chosen. The basic method for choosing the crossover point is to choose a random cell within the chromosome. The second parameter that needs to be identified is the number of crossovers that has to been. Just as we are doing crossover in one point, we can do it in more than one point that might give a more variety to the reproduced chromosomes. Mutation:\nMutation is another method of reproduction. Mutation is singular method of reproduction, as it is performed on a single chromosome, and not on the parent chromosomes (male and female). Using this method, we choose one of the cell parameters that we want to differentiate from others – to mutate. Then we mutate it to produce some new cell that introduces new data to the chromosome, and thus can affect its fitness.The two parameters that need to be identified here are: what should be mutated – some property of a cell, or all properties of a cell, and the second parameter – what is the best number of mutations that need to be done to introduce some variety, and not to get further from the optimal solution.   Objective function f(x):\nObjective function is the function which takes chromosome as an argument, evaluates it and gives the number, which represents the fitness, or the adaptation of the chromosome to the current environment.\nIn our scenario, the fitness function gives optimal value, when it equals 0. The fitness function is supposed to be evaluated checking the chromosome to match the hard and soft constraints. In our program the genetic algorithm goes through 2 stages of counting fitness function:\n1 – hard stage – here we estimate the hard constraints within the chromosome\n2 – soft stage – here we estimate the soft constraints within the chromosome, is used when the chromosome hard stage is accomplished. Stopping criteria:\nThe basic stopping criteria which can be considered is reaching zero as a fitness function of soft stage. As our program is multi-threaded, user can view the progress, pause and resume the algorithm while it’s running. These options help a user a lot, as the program does not have to make decision when to stop. Depending on the results, and user decision, the stop criteria will be decided, unless the fitness function of the second, soft stage reaches zero.  1.2.4.3 - Programming Elements Used Here you can find the description of the programming elements used in the implementation of the Genetic Algorithm on the problem of Time-Tabling. To support the implementation we have used the OOP approach in our implementation, so the classes where organized into a specific hierarchy to make it easier and more organized, thus producing cleaner and faster implementation.\nHarshly saying, the classes are organized into the following hierarchy:\n\nEach of the above stated classes are described below in detail:\n\nBuffer is a static class that is used in order to communicate with the database and buffer some data. This will speed up the performance and make communication with the database easier.FetchData() – method fetches the data from the database by executing the “SELECT ALL” query, in order to speed up performance of the systemgetRandomCell() – fetches random cell, which should be validgetRandomGCCell() – fetches the random cell, according to the provided groupid argumentWriteChromosome() – saves the provided chromosome to the specified text fileReadChromosome() – reads a saved into text file chromosome and returns it as a result of the method.\n\nCell class represents the smallest building block of the chromosome. If we want to define the chromosome as a schedule, than we can define a cell as a one-slot entry, which specifies the slot number, teacher, room and subject and group to be taught.We provided the cell object with a copy construct, to return a new instance copy of the same cell. Also as we can notice from the above description, GenerateValidCell() will perform a call to the getRandomCell() / getRandomGCCell() of the Buffer static class in order to make current cell as a generated random valid cell, with or without the pre-specified groupid value.MutatedBy() method will accept a value, indicating what is the mutation performed by, and perform a mutation on the cell, executing the corresponding query. The available mutation types are: mutate by all (new generated random cell), mutate by group, mutate by period, mutate by room and mutate by teacher. Notice that the result of the mutation will be a valid cell._cumul property is very important when counting the fitness function. As the fitness function iterates through the current cell, the _cumul property will record the cumulative impact of the current cell on the fitness function total value._tillnow property will be also important as it also record the cumulative impact, but not totally cumulative, but just on the period of last genAlgoSettings.CrossOverCellNumbers number of cells. Then it can be used in future to define which interval of chromosome had the greatest impact on the total fitness function value.f – the flag which will indicate if the current cell has any contribution on worsening the fitness function value. Can be helpful in drawing the chromosome and coloring the bad cells.\n\nAs we said before, a Cell is the smallest building block of the chromosome, so the chromosome is the schedule itself, defined by a set of Cells. Chromosome is the item within the population, an item which will be evaluated by the fitness function, and finally, the output of the program.elements property – is an array of cells, which define the chromosome itselfff – defines the latest fitness function value, which is saved for faster access to avoid evaluating chromosome each timefv – fitness values, which assess the compliance on the chromosome to each of the 4 hard constraintsnumber – number of elements within the chromosomecopy() – the same copy constructor to copy the chromosome as a whole producing a new instance, and new instances of the contained cells\nFF() – evaluate the chromosome using the fitness function – this will update the ff and fv properties of the current chromosome\nGenerateChromosome() - will generate a totally random chromosome of number number of cells and is used to introduce the initial random population\nMarkUsed() – will mark the current chromosome as used, by activating the used property / flag. This will be helpful to avoid the chromosomes that were already chosen as parents in the iteration through the present population. This way we can ensure that in the next iteration the chromosomes used will be not identical to the same parents already chosen.\nclearFlags() – clears all the flags f of a cell\nclearTillNow() - clears all the _tillnow and _cumul properties of the cells to be reused when transferring from the current population to the new population.\nUpdateGroups() – retrieves and updates the groupid’s values of each cell in the chromosome, according to the _subject and _groupnumber properties. This will ensure the consistency of data, when the chromosome of some old data was saved, and need to be retrieved, but the values of the groupid’s has been changed.\nMutation() – mutates the chromosome, using the procedure defined in the Reproduction.Mutation.Run() static method.\n\nFitness function is a very important static class, which defines the procedures and heuristics for counting the fitness function value. Actually the main methods for counting the fitness function values are the count() and countSoft() method. These methods accept a chromosome as an input parameter, and return the total fitness function value as a result of execution.The interesting thing about the fitness function class that it is designed to operate in 2 modes: soft mode and hard mode. Depending on the current stage value, defined by the HeuristicSwitcher.Stage property the fitness function will decide whether to count its value based on the soft or hard constraints. the chromosome function FF() (Chromosome.FF()) will check for the value of HeuristicSwitcher.Stage to match Stages.Hard or Stages.Soft, and will choose to correspondingly return FitnessFunction.count() or FitnessFunction.countSoft() for the current chromosome.The constraints and their weights are multiplied correspondingly to show the impact of the constraint of the fitness function. They are organized as follows:Hard Constraints / Weights:\nHC1() / hc1w, HC2() / hc2w, HC3() / hc3w, HC4() / hc4w\nSoft Constraints / Weights:\nSC1() / sc1w, SC2() / sc2w, SC3() / sc3w, SC4() / sc4w\n\nThis is considered as the main class of our implementation. The GeneticAlgorithm class is responsible for the main code flow management. The class contains several essential properties, which contribute to the work of the algorithm:\n PresentPopulation – the current population, which is the main source of reproduction to create the new population NewPopulation – new population, which is produced step by step by taking parents from the present population, performing and crossovering to reproduce into the new population. ResultChromosome – here the result chromosome of the best fitness function should be saved.  The algorithm consists of the three major parts: init() – initialization, connection to the database, and preprocessing of the data, input() – generating first random PresentPopulation, which will be the initial seed of the program, and run() – implementation of the algorithm itself to obtain the ResultChromosome using the PresentPopulation seed.\n\nHeuristic Controller – this class is responsible for monitoring the performance of the currently used heuristic of reproduction and changing it to obtain better results. This way we ensure that the program will not enter the infinite loop and will try to reproduce in different types of defined techniques to reach better results. In order to monitor the performance of currently active heuristic, several properties are needed to be recorded and constantly reviewed:This is done by two methods of the static class: checkPerformance() – which will check the performance of the current heuristic based on some specific metrics, and takeAction() – takes some specific appropriate heuristic modification action, in case the performance using the active heuristic is unsatisfactory. Check performance works in the following way:\n Gradient – is the difference between the old and new values for the fitness functions (the change in fitness function values) AccGradientAvg – Gradient Average till the current moment in time (iteration) Stepping = Gradient / AccGradientAvg – shows the share of the current gradient from the gradient average. If the Stepping produced is less than the SteppingThreshold, then it’s considered a bad result  The algorithm checks for the number of consequent bad results. If it is more than GenAlgoSettings.SteppingNumberThreshold, than the whole heuristic after several number of chances is considered as bad, and a next heuristic is executed within the takeAction() method.\n\nHeuristic Switcher checks for the currentHeuristic number and redirect the program to the next heuristic using the NextHeuristic() method. A heuristic within our implementation is defined as a set of values for optional attributes, such as mutateby, CrossOverNumber, MutationNumber, SteppingNumberThreshold, SmartCrossOver and CrossOverCellNumbers. Using these parameters we will be able to define specific heuristic, way of reproduction that can be used by the parent chromosomes to reproduce in different ways. The Stage parameter saves the value, defining if the program is current running in the hard or soft stage.\n\nA male and a female chromosome is considered as parent, which will in future reproduce to obtain a better population. This parent object has a copy() constructor, which will call the copy constructors of both the chromosomes to produce a completely new copy of the parent instance.CrossOver() is a method which will recall the Reproduction.CrossOver.Run() in order to following the specific procedure for the current parent chromosomes. MarkUsed() will mark both the parent chromosomes, male and female as used.\n\nPopulation is a set of chromosomes, which should be used to reproduce and obtain better population through evolution. The population contains the property number, which is the pointer to the last filled chromosome in the population. The count property returns the total number of chromosomes within the current population.This class contains several assisting methods to assist the work of the population in an easier way:\n addChromosome() – adding a new chromosome to the population addParents() – add parent to the current population, i.e. adding male and female chromosomes to the current population ChooseParents() – choose the best 2 chromosomes as parents, with the smallest fitness function values, and mark them as used to avoid re-choosing them on the next iteration. clearUsed() – clear used flag for all the chromosomes in the population GeneratePopulation() – generate a population of random valid chromosomes.  getAllFitness() – perform fitness function execution FF() for all the chromosomes in the population.\n\nReproduction class performs the actual reproduction, corresponding to the two methods: CrossOver and Mutation. The two methods of reproduction are defined as classes within the static reproduction class.\n\nStages Enumerator – indicates the HARD and SOFT stage of the running the algorithm. This will determine the fitness function count method chosen and the heuristics and their order.\n"
},
{
	"uri": "https://majdarbash.github.io/genetic-algorithm/03-user-manual/",
	"title": "1.3 - Genetics Algorithm - User Manual",
	"tags": ["artificial intelligence", "genetics algorithm", "school timetabling problem"],
	"description": "",
	"content": "1.3.1 - Operation Specification The program should be executed as follows:\nData entries for the program should be supplied within the tables in the “genalgo.accdb” database file. These tables contain the teacher definitions, teacher available times, subjects, teacher subjects mapping, rooms and corresponding room types. From all these tables in the preprocessing stage, the groups and then the cells tables are created, which will be used then as an input to our program.\nWe can execute our program from the “SPGenAlgo.exe”. This file will load the database file into memory, read input data from it, and run our Genetic Algorithm implementation in easy-to-use, interactive and multi-threaded way, for best performance and more user-friendly environment. These is the screen displayed when running the program:\n\nThere are several useful labels and form components, which give us very useful feedback about the progress of the program:\n\n\nAfter a while, after satisfying all the hard constraints in the program, the program will give the feedback, through the current stage flag.\n1.3.2 - Performance Metrics According to our test and performance estimations, we have found out that it takes about 450 seconds to obtain the fitness function value below 10 within the hard stage. As about the soft stage, the value is not well defined, and cannot be estimated or tested, cause it’s totally dependent on the case we are working with. Sometimes, the working schedule, may not satisfy any of the soft constraints at all.\n\nAfter working about 740 seconds on satisfying the hard constraints we have obtained the following results:\n\nAccording to our estimations, it would take about 700 – 1000 seconds to obtain the hard constraints satisfied, i.e. a maximum of about 15 minutes.\n"
},
{
	"uri": "https://majdarbash.github.io/genetic-algorithm/04-future-vision-for-optimality/",
	"title": "1.4 - Genetics Algorithm - Future Vision for Optimality",
	"tags": ["artificial intelligence", "genetics algorithm", "school timetabling problem"],
	"description": "",
	"content": "We are proud to see amazing results for the timetabling problem. As stated in the section before, that the maximum time for satisfying essential hard constraints was 15 minutes. This time is good enough, but in worse cases, the performance will be worse of course. This is why we introduce a new area for improvement in future – we would consider that it may be really efficient if we define and implement new heuristics within the HeuristicSwitcher, which might help us a lot in finding solution faster.\nThe second notice is that the values of the all the parameters might be not crisp values, but some fuzzy values. This idea directly introduces the concept of integrating this system with some fuzzy system to describe the Heuristic used, and may also be used to provide feedback about teachers, courses, rooms and times.\nA big field of improvement may be to consider some swapping techniques, or procedures that can be done to improve the schedule. We don’t mean here just to set some parameters in order to create a Heuristic for working with Ordinary CrossOver and Mutation or Smart CrossOver, but we mean here that some set of steps might be applied on each algorithm in order to become more efficient, without the need to apply complex, and relatively random CrossOver and Mutation techniques.\nAnd finally, the Genetic Algorithm Implementation was a great part of project. We have discovered new ways and understood it, and modified it to accommodate our needs, introducing Heuristics, SmartCrossOvers, Mutation By and some other components, to make the Algorithm smarter, in order to obtain the results we have just seen in a good time like this.\n"
},
{
	"uri": "https://majdarbash.github.io/random/accessing-docker-with-non-root-user-privileges/",
	"title": "Accessing Docker with non-root user privileges",
	"tags": [],
	"description": "",
	"content": "Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.39/containers/json: dial unix /var/run/docker.sock: connect: permission denied\nDoes this error look familiar?\nWhat is happening is that docker client is communicating with the engine using socket file. As docker service is launched using root user and socket file is accessible to every user in \u0026ldquo;docker\u0026rdquo; group, you will not be able to connect to this file from docker client executed launched with your current user access.\n2 possible solutions:\n Use docker with root privileges  sudo docker ps Grant your user access to the socket file by adding to the \u0026ldquo;docker\u0026rdquo; group  sudo usermod -a -G docker $USER  Publish Date: 2019-06-13\n "
},
{
	"uri": "https://majdarbash.github.io/tags/account/",
	"title": "account",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/angular/",
	"title": "angular",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/angular-js-core-concepts/",
	"title": "AngularJS 1: Core concepts",
	"tags": ["angular", "js"],
	"description": "",
	"content": "Core Concepts  Directives - HTML annotations that trigger Javascript behaviors Modules - Where our application components live Controllers - Where we add application behaviour Expressions - How values get displayed within the page  Module / Controller Initialization var app = angular.module(‘store’, \\[\\]); ng-app=\u0026quot;store\u0026quot;\u0026gt; … this way the html inside the body tag will be treated as a part of angular application\n{{ expression }} - this will be evaluated using the javascript\n(function(){ var app = angular.module(\u0026#34;store\u0026#34;, []); app.controller(\u0026#34;StoreController\u0026#34;, function(){ this.product = gem; }); var gem = { name: \u0026#34;Product name\u0026#34;, price: 2.95, description: \u0026#34;description\u0026#34;, canPurchase: false, soldOut: true }; })();  \u0026lt;html ng-app=\u0026#34;store\u0026#34;\u0026gt; \u0026lt;body ng-controller=\u0026#34;StoreController as store\u0026#34;\u0026gt; \u0026lt;div ng-hide=\u0026#34;store.product.soldOut\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;{{ store.product.name }}\u0026lt;/h1\u0026gt; \u0026lt;div\u0026gt;{{ store.product.price }}\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;{{ store.product.description }}\u0026lt;/div\u0026gt; \u0026lt;button ng-show=\u0026#34;store.product.canPurchase\u0026#34;\u0026gt;Add to Cart\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Loops with ng-repeat \u0026lt;div ng-repeat=\u0026#34;product in store.products\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;{{ product.name }}\u0026lt;/h1\u0026gt; \u0026lt;div\u0026gt;{{ product.price }}\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;{{ product.description }}\u0026lt;/div\u0026gt; \u0026lt;button ng-show=\u0026#34;product.canPurchase\u0026#34;\u0026gt;Add to Cart\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; Publish Date: 2015-04-08\n "
},
{
	"uri": "https://majdarbash.github.io/tags/apache/",
	"title": "apache",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/apache2/",
	"title": "apache2",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/artificial-intelligence/",
	"title": "artificial intelligence",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/assets/",
	"title": "assets",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/asynchronous-programming/",
	"title": "asynchronous programming",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/aws/",
	"title": "aws",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/aws-applications/",
	"title": "AWS Applications",
	"tags": [],
	"description": "",
	"content": "SQS (Simple Queue Service)  the first AWS service distributed queue system temporary repository for messages that are waiting for processing one component in the application queues messages to be consumed by another component in the application can be used to decouple the components of an application messages can contain up to 256 kb of text\nyou can store up to 2GB per message, however, it will be stored and retrieved from S3 in this case messages can be retrieved using AWS SQS API auto-scaling events can be configured based on queue sizes as well pull-based, not pushed based\nmessages should be consumed from the queues, SQS will not push the messages messages can be kept in the queue from 1 minute to 14 days the default retention period is 4 days visibility timeout - the amount of time that the message is invisible in the SQS queue after a reader picks upthat message. If the job is not processed within visibility timeout, the message will become visible again and can be processed by another reader. maximum visibility timeout is 12 hours polling types:  short polling - returns immediately even if the message queue being polled is empty long polling - doesn\u0026rsquo;t return a response until a message arrived in the message queue or the long poll times out   queue types:  Standard Queue  nearly unlimited number of transactions per second a message is delivered at least once\noccasionally more than one copy of a message might be delivered out of order allows high throughput provides best-effortordering\nensures that messages are generally delivered in the same order as they are sent   FIFO Queue  first-in-first-out ordered queues the order is strictly preserved and guaranteed the message remains available until a consumer processes and deletes it; duplicates are not introduced into the queue support message groups that allow multiple ordered message groups within a single queue limited to 300 transactions per second (TPS)      SWF (Simple Workflow Service)  makes it easy to coordinate work across distributed application components workflow executions can last up to 1 year presents a task-orientedAPI ensures that the task is assigned only once keeps track of all the tasks and events in an application SWF actors:  Workflow Starters\nAn application initiating a workflow.\n(e.g. an e-commerce website following the placement of order) Deciders\nControl the flow of activity tasks in a workflow execution. If something has finished (or failed) in a workflow, a Decider decides what to do next. Activity Workers\nCarry out the activity tasks    SNS (Simple Notification Service)  Service for sending notifications from the cloud Can deliver notifications to devices (via Push notifications), SMS, Email, SQS queues or to any other HTTP endpoint Push notifications to Apple, Google, FireOS, Windows devices and Android devices in China with Baidu Cloud Push Recipients are grouped using Topics One topic can support deliveries to multiple endpoint types The message published to a topic will be delivered to each subscriber Messages published to SNS are stored across different AZs Push-based delivery (no polling) Simple APIs and easy integration with applications Inexpensive, pay-as-you-go model with no up-front costs  SNS vs SQS  Both Messaging Services in AWS SNS - Push SQS - Polls (Pulls)  Elastic Transcoder  Media Transcoder in the cloud Convert media files from their original source format into different formats that will play on smartphones, tablets, PCs, etc. Provides transcoding presets for popular output formats Pay based on the minutes that you transcode and the resolution at which you transcode  API Gateway  Fully managed service that makes it easy for developers to publish, maintain, monitor and secure APIs at any scale. Features  Expose HTTPS endpoints to define a RESTful API Serverless-ly connect to services like Lambda \u0026amp; DynamoDB Send each API endpoint to a different target Run efficiently with low cost Scales automatically Track and control usage by API key Throttle requests to prevent attacks Connect to CloudWatch to log all requests for monitoring Maintain multiple versions of your API   Configuration  Define an API Define Resources and nested Resources (URL Paths) For each resource:  Select supported HTTP methods (verbs) Set security Choose target (such as EC2, Lambda, DynamoDB, etc.) Set request and response transformations     Deployment  Uses API Gateway domain, by default Can use a custom domain Now supports AWS Certificate Manager: free SSL/TLS certs   API Caching (TTL specified)\nWill help you reduce the number of requests made to your endpoint, improving the latency of the requests to your API. Same Origin Policy\nWeb browser permits scripts contained in a first web page to access data in a second web page, but only if both web pages have the same origin. This prevents XSS attacks. CORS can be enabled on the API Gateway  CORS (Cross-Origin Resource Sharing)\nallows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served.  The browser makes HTTP OPTIONS call The server returns a response listing other domains that are approved to GET this URL      Kinesis  Service working with the streaming data Makes it easy to load and analyze streaming data Streaming Data is data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes). Types  Kinesis Streams Kinesis Firehose Kinesis Analytics   Kinesis Streams  Receive data from producers and retain it until consumed Producers - Produce streaming data and stream it to Kinesis streams Consumers - receive data from Kinesis Streams and act on it Retention: 24 hours - 7 days Consists of Shards  5 transactions per second for reads a maximum total data read rate of 2MB per second up to 1000 records per second for writes maximum total write rate of 1MB per second   Data capacity of the stream is determined by the number of shards and their capacities   Kinesis Firehose  Data received could optionally execute a lambda function and will be output to it\u0026rsquo;s the destination which could be S3, ElasticSearch There\u0026rsquo;s no data persistence Data will be forwarded immediately to the target destination   Kinesis Analytics  analyzes the data inside both of Kinesis Firehose and Kinesis Streams, with on-the-fly analysis capability    FAQs  SQS Billing is calculated per request, plus data transfer charges for data transferred out of Amazon SQS 1 million requests per month - fall under the free tier Batch operations cost the same as other SQS requests. Grouping messages into batches, you can reduce the SQS costs. All the messages in SQS have a globally unique ID that SQS returns when the message is delivered to the message queue. This ID is useful for tracking the receipt of a particular message in the message queue. Dead letter queues receive messages from other source queues after a maximum number of processing attempts cannot be completed. SQS message can contain up to 10 metadata attributes - applications can determine how to process the message based on the metadata instead of inspecting the entire message. (attributes: name-type-value triples) SentTimestamp attribute - contains information when the message was sent by a producer and queued by SQS. SenderId attribute - contains either the AWS account ID or the IP address for the sender. Amazon SQS APIs provide deduplication functionality for FIFO queues that prevents your message producer from sending duplicates. Any duplicates introduced by the message producer are removed within a 5-minute deduplication interval. If using standard queues you may experience duplicates - the application must be designed to be idempotent - not affected when processing the same message more than once). Queue type can be chosen only on creation. If you want to convert from one type to another you will have to recreate the queue. In FIFO queues, messages are ordered based on the message group ID. It\u0026rsquo;s a required field when sending a message to the queue. After you\u0026rsquo;re done processing the message, you are responsible for deleting it. Server-side encryption (SSE) - SSE encrypts messages as soon as Amazon SQS receives them using keys in AWS KSM (AWS Key Management Service). The messages are stored in encrypted form and are decrypted when sending a message to an authorized consumer. SSE encrypts the body of the message. The queue metadata, message metadata, and per-queue metrics are not encrypted. SSE uses AES-GCM 256 algorithm. Amazon SQS is PCI DSS Level 1 certified and HIPAA Eligible Service. SQS messages can contain text data, including XML, JSON and unformatted text. The number of inflight messages is limited to 120k for standard queue and 20k for FIFO queue. Inflight messages are those messages that have been received by a consuming component but have not yet been deleted. Queue names are limited to 80 characters.  "
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/aws-cloud-best-practices/",
	"title": "AWS Cloud Best Practices",
	"tags": [],
	"description": "",
	"content": "Difference between Traditional and Cloud Computing Environments  IT Assets as Provisioned Resources  Quick deployment time No hardware commitment   Global, Available, and Scalable Capacity Higher-Level Managed Services\nIn addition to EC2 instances, you can benefit from a lot of great services which will be provisioned and maintained with minimum effort. Built-In Security Architecting for Cost  AWS provides fine-grained billing, which enables you to track the costs associated with all aspects of your solutions   Operations on AWS  Infrastructure as Code Higher levels of automation of the operational processes as the supporting services, e.g. AWS Auto Scaling and self-healing architectures Full automation through DevOps processes for delivery pipeline and management   Disposable Resources Instead of Fixed Servers  Design Principles  Scalability Disposable resources instead of fixed servers Automation Loose coupling Services, not servers Databases Managing increasing volumes of data Removing single points of failure Optimize for cost Caching Security  Scalability   Scaling Vertically\nIncrease in the specifications of an individual resource, such as upgrading a server with a larger hard drive or a faster CPU\n  Scaling Horizontally\nIncrease in the number of resources, such as adding more hard drives to a storage array or adding more servers to support an application\n  Stateless Applications\n An application that does not need knowledge of previous interactions and does not store session information Given the same input provides the same response to any end-user Can scale horizontally because any available compute resources can service any requests    Distributing the load\n Push model  ELB, ALB, Network Load Balancer, Route53 load balancing   Pull model  for asynchronous, event-driven workloads SQS, Amazon Kinesis compute resources pull and consume messages, processing them in a distributed fashion      Stateful Components\n Can be scaled with session affinity Session Affinity  Bind all the transactions of a session to a specific compute resource Existing sessions do not directly benefit from the introduction of newly launched compute nodes      Instantiating Compute Resources  Bootstrapping  You can set up new EC2 instances with user data scripts and cloud-init directives You can use simple scripts and configuration management tools such as Chef or Puppet   Golden Images  Can be used to launch EC2 instances, Amazon RDS DB instances, and Amazon Elastic Block Store (Amazon EBS) volumes Results in faster start times and removes dependencies to configuration services or third-party repositories Important in auto-scaled environments to quickly and reliably launch additional resources as a response to demand changes.   Containers  Docker—an open-source technology that allows you to build and deploy distributed applications inside software containers. Launching from Docker image Amazon Elastic Container Service (Amazon ECS) and AWS Fargate Alternative container environment: Kubernetes and Amazon Elastic Container Service for Kubernetes (Amazon EKS)   Hybrid  Some parts are in a golden image, while others are configured dynamically through a bootstrapping action.    Infrastructure as Code  AWS CloudFormation templates give you an easy way to create and manage a collection of related AWS resources provision and update them in an orderly and predictable fashion CloudFormation templates can live with your application in your version control repository  Automation, Infrastructure Management, and Deployment  Serverless  AWS CodeBuild, and AWS CodeDeploy support the automation of the deployment of these processes   AWS Elastic Beanstalk:  You can use this service to deploy and scale web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. 17 Developers can simply upload their application code, and the service automatically handles all the details, such as resource provisioning, load balancing, auto scaling, and monitoring.   Amazon EC2 Recovery  Creating CloudWatch alarm that monitors EC2 instance and recover if impaired   AWS Systems Manager  You can automatically collect software inventory, apply OS patches, create a system image to configure Windows and Linux operating systems, and execute arbitrary commands.   Auto Scaling  You can maintain application availability and scale your Amazon EC2, Amazon DynamoDB, Amazon ECS, Amazon Elastic Container Service for Kubernetes (Amazon EKS) capacity up or down automatically according to the conditions you define    Alarms and Events  Amazon CloudWatch alarms Amazon CloudWatch Events AWS Lambda scheduled events AWS WAF security automation  Services, Not Servers  Loose Coupling Well-Defined Interfaces  Various components to interact with each other only through specific, technology-agnostic interfaces, such as RESTful APIs Can modify the underlying implementation without affecting other components   Amazon API Gateway  Fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale   Service Discovery  Because each service can be running across multiple compute resources, there needs to be a way for each service to be addressed EC2-hosted service, a simple way to achieve service discovery is through Elastic Load Balancing (ELB). Because each load balancer gets its own hostname, you can consume a service through a stable endpoint.   Asynchronous Integration  Another form of loose coupling between services One component generates events and another that consumes them SQS, Amazon Kinesis, cascading Lambda events, AWS Step Functions, or Amazon Simple Workflow Service Decouples components and introduces additional resiliency   Managed Services  Provide building blocks that developers can consume to power their applications   Serverless Architectures  Can reduce the operational complexity of running applications. It is possible to build both event-driven and synchronous services for mobile, web, analytics, CDN business logic, and IoT without managing any server infrastructure. These architectures can reduce costs because you don’t have to manage or pay for underutilized servers, or provision redundant infrastructure to implement high availability.    Databases  Anti-Patterns  If your application primarily indexes and queries data with no need for joins or complex transactions—especially if you expect a write throughput beyond the constraints of a single instance—consider a NoSQL database instead If your schema cannot be denormalized and the application requires joins or complex transactions, RDBS should be considered Large binary files should be stored in Amazon S3 with metadata in the database.   Databases  managed database services that offer enterprise performance at an open-source cost AWS offers different database technologies based on your workload Scalability  RDBS (Relational Databases) can scale up by upgrading to a larger instance and can scale horizontally by adding more read replicas Write capacity can be scaled horizontally by data partitioning or sharding. Data is split across multiple database schemas each running its own autonomous primary DB instance. RDS removes the operational overhead of running those instances, however, sharding introduces complexity in your application     Data Warehouse  combines transactional data from disparate sources to make them available for analysis and decision making Amazon Redshift is a managed data warehouse service providing a scalable, highly available and cost-effective solution.   Search  Searching enables datasets to be queried that are not precisely structured. AWS supports search services:  Amazon ElasticSearch (ES) Amazon CloudSearch     Graph Databases  Uses graph structures for queries The graph is defined as a consisting of edges (relationships), which directly relate to nodes (data entities) in the store. Relationships allow faster retrieval of complex hierarchical structures in relational systems.   Managing Increasing Volumes of Data  Data lake architecture    Removing Single Points of Failure  Introducing redundancy  Standby Redundancy\nWhen a resource fails, functionality is recovered on a secondary resource with the failover process. During the failover time, the resource remains unavailable. Active Redundancy\nRequests are distributed to multiple redundant compute resources. When one of them fails, the rest can simply absorb a larger share of the workload.   Detect Failure  You should aim at automatic failure detection and reacting to failure. ELB, Route53 with health checks, ASGs and other methods will help you automatically recover from the failure. Design Good Health Checks\nConfiguring the right health checks for your application helps determine your ability to respond correctly and promptly to a variety of failure scenarios. The health checks should reliably assess the health of the back-end nodes. Simple TCP check won\u0026rsquo;t detect the health state of a web server.   Durable Data Storage  Synchronous Replication\nThe transaction is acknowledged only after being durably stored in both the primary location and its replicas. This will protect the integrity of data in the event of failure. In this case, the primary node is coupled with the replicas. Asynchronous Replication\nDecouples the primary node from the replica, however, introduces replication lag - used to horizontally scale the system\u0026rsquo;s read capacity for queries that can tolerate the replication lag. Quorum-based replication\nCombines synchronous and asynchronous replication to overcome the challenges of large-scale distributed database systems. Replication to multiple nodes can be managed by defining the minimum number of nodes that must participate in a successful write operation. Examples:  Redis in AWS ElasticCache provide asynchronous communication - recent transactions can be lost in the event of a failover RDS with Multi-AZ provides synchronous replication to keep data on the standby node up-to-date with the primary.     Automated Multi-Data Center Resilience  Disaster Recovery Plan\n(Could consider failover to a distant second data center in the event of major disruption)  Low probability but huge impact risk   AZs provide a solution for short disruption, during which you    Fault Isolation and Traditional Horizontal Scaling  The measures above are insufficient if there is something harmful about the requests themselves. Same scenarios which caused the failure of the primary instances could be replayed to fail the failover instances Shuffle Sharding  Fault-isolating improvement Instances are grouped into shards Each customer will be distributed to a specific shard The impact is reduced in direct proportion with the number of shards The client could try every endpoint in a set of sharded resources, until one succeeds, making the client fault tolerant    Optimize for Cost  Right Sizing  Benchmarking may help in understanding the instance type and number of instances you require   Cost Optimization is an iterative process  Application and its usage will evolve over time   Elasticity  Autoscaling can help optimize the cost Automate turning off production workloads when not in use Replacing ec2 workloads with services   Take advantage of the variety of purchasing options  Reserved Instances Spot Instances    Caching  Application Data Caching  Amazon ElastiCache Amazon DynamoDB Accelerator (DAX)\nFully managed, highly available, in-memory cache for DynamoDB that delivers performance improvements from milliseconds to microseconds, for high throughput   Edge Caching  Static content cached at Amazon CloudFront edge location    Security  AWS WAF (Web Application Firewall) IAM  Granular set of policies for access control of users IAM roles can be assigned to instances to grant access to the resources   Data Encryption (in transit / at rest) AWS is responsible for the security of underlying cloud infrastructure You are responsible for securing the workloads you deploy to AWS Amazon Cognito  Allows client devices to access AWS resources through temporary tokens with fine-grained permissions   Security as Code  AWS CloudFormation   Real-Time Auditing  AWS Config AWS Inspector AWS Trusted Advisor AWS CloudTrail AWS CloudWatch Logs    "
},
{
	"uri": "https://majdarbash.github.io/random/aws-organizations/",
	"title": "AWS Organizations - Setting Up and Configuring User Accounts",
	"tags": ["aws", "devops", "solution architect"],
	"description": "",
	"content": " Setting Up Organizations on AWS Account Granting User Permissions to Access Resources Accessing Cross-Account Resources using AWS CLI Example  1. Creating AWS Accounts Hierarchy under Organization 2. Create Users and Groups in the Root Account 3. Managing Root Account User Permissions in the Child Account 4. Accessing Child Account resources using AWSCLI    Setting Up Organizations on AWS Account AWS Orgnizations provides you with an efficient way to better manage and organize your AWS accounts. You can create sub-accounts under your root account and invite existing AWS accounts to join your organization. Joining an organization account enables Consolidated Billing and better Access Management through Switch Role functionality.\nHere\u0026rsquo;s what you can do with AWS Organizations:\n Create Organization You can organize Organizations into Organization Units You can invite an existing account Enable root users to access child accounts using one set of credentials  For this, you have to:\n Use generic names and emails for root accounts Create the role with which the administrative user in root account can access the child account From now on, create users in root account and let them switch role   Granting User Permissions to Access Resources Though not compulsory, it\u0026rsquo;s recommended to create users only under the Root Account. By granting AssumeRole permissions to the users, you can allow them to assume certain roles in the child accounts.\n Create all Users in the root account Create role in each account with Principal of the parent account Give permissions to the users to assume the relevant role in the child account Create a role in the child accout which will allow Principal of the root account to access it Define the permissions allowed in the created Role within the Child Account   Accessing Cross-Account Resources using AWS CLI #/.aws/config: [profile developer.development] role_arn = arn:aws:iam::[Child-Account-ID]:role/[roleName] source_profile = [account-profile]  Example In this example we would like to create accounts with the following structure and grant users the correct permissions to each account.\nAWS Accounts:\n1. Creating AWS Accounts Hierarchy under Organization  Root (root@example.com)  Production (production@example.com) Development (development@example.com) Test (test@example.com)    While creating child accounts, AWS will by default create role named as OrganizationAccountAccessRole. This role created in the Child Account, will allow trusted entity being Root Account an AdministratorAccess to the Child Account. Meanwhile from the Root Account we can restrict who of our users is allowed to AssumeRole in the Child Account.\nTrusted Entities of OrganizationAccountAccessRole:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::[AWS-Root-Account-ID]:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } Following the same Analogy, we can create several roles in the Child Account with different permissions to the Resources in the child account. Then we can control which users from the Root Accountcan AssumeRole for which Role in the Child Account, defining User\u0026rsquo;s Cross-Account Resource permissions.\n2. Create Users and Groups in the Root Account Groups:\n Developers DevOps Testers Marketing  Allow Developers to assume \u0026ldquo;Developer\u0026rdquo; role in any of the Children Accounts. Add the following policy to the \u0026ldquo;Developers\u0026rdquo; group:\nDeveloper Group Policy\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::*:role/Developer\u0026quot; } } [profile developer.it-test-aws] role_arn = arn:aws:iam::257040594755:role/Developer source_profile = sac\nDo the same for DevOps, Testers or any other groups.\nDevOps Group Policy\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::*:role/DevOps\u0026quot; } } Tester Group Policy\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::*:role/Tester\u0026quot; } } Marketing Group Policy\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::*:role/Marketing\u0026quot; } } 3. Managing Root Account User Permissions in the Child Account At this stage, user of the Root Account can assume a role in the Child Account, using the \u0026ldquo;AssumeRole\u0026rdquo; permission granted in the policy above.\nNow it\u0026rsquo;s time to create these roles and grant permissions by assigning Policies to them. Each Child account in the current example will have the following roles:\n Developer DevOps Tester Marketing  Each of these roles will have the following Trusted Entities, based on what Root Account users can AssumeRole from the Child Account:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::[AWS-Root-Account-ID]:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, } ] } 4. Accessing Child Account resources using AWSCLI In order to access Child Account using the CLI, we need to configure a new profile for the AWS CLI where we would specify the assumed role and the source account credentials from the root account.\n# ~/.aws/config [profile root-account-profile] region = eu-west-1 output = json [profile developer.child-account-1] role_arn = arn:aws:iam::[Child-Account-Id]:role/Developer source_profile = root-account-profile # ~/.aws/credentials [root-account-profile] aws_access_key_id = [ROOT-Account-Key-Id] aws_secret_access_key = [ROOT-Account-Secret-Key]  Publish Date: 2019-11-24\n "
},
{
	"uri": "https://majdarbash.github.io/tags/backlinks/",
	"title": "backlinks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/browser/",
	"title": "browser",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/ci/",
	"title": "ci",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/clock/",
	"title": "clock",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/commands/",
	"title": "commands",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/concept/",
	"title": "concept",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/continuous-integration/",
	"title": "continuous integration",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/crossdomain/",
	"title": "crossdomain",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/mysql-grant-privelleges/",
	"title": "CS: MySQL",
	"tags": ["mysql", "database", "terminal"],
	"description": "",
	"content": "Couple of useful sql queries we always search about for user management in mySQL:\n# creating a user in mysql CREATE USER '_username_'@'%' IDENTIFIED BY '_password_'; # give user select access on all tables in the database GRANT SELECT ON _database_.* TO '_username_'@'%'; # give user full access on all tables in the database GRANT ALL PRIVILLEGES ON _database_.* TO '_username_'@'%'; # grant special access to user for specific tables GRANT UPDATE ON _database_._tablename_ TO '_username_'@'%'; GRANT DROP, INSERT, DELETE, UPDATE ON _database_._table_ TO '_username_'@'%'; GRANT SELECT ON _database.table_ TO '_username_'@'%'; Determining the size of the database:\nSELECT table_schema \u0026quot;DB Name\u0026quot;, Round(Sum(data_length + index_length) / 1024 / 1024, 1) \u0026quot;DB Size in MB\u0026quot; FROM information_schema.tables GROUP BY table_schema; Dumping Data:\n# dumping specific database mysqldump [database] -u[username] -p[password] -h[hostname] \u0026gt; /tmp/database.sql # dumping certain tables mysqldump [database] --tables [table1] [table2] -u[username] -p[password] -h[host] \u0026gt; /tmp/database_table1_table2.sql # dumping based on query mysqldump [database] -e \u0026quot;[query]\u0026quot; -u[username] -p[password] -h[host] \u0026gt; /tmp/data.sql [query] - example: select * from users where id \u0026gt; 10 Other options for dumping: \u0026ndash;no-data \u0026ndash;skip-comments \u0026ndash;compact \u0026ndash;add-drop-database \u0026ndash;no-create-db\nPublish Date: 2014-09-10\n "
},
{
	"uri": "https://majdarbash.github.io/random/some-useful-commands-for-ubuntu-terminal/",
	"title": "CS: Ubuntu Commands",
	"tags": ["ubuntu", "terminal", "shell", "commands"],
	"description": "",
	"content": " # Checking the current version of installed ubuntu OS: lsb _release -a # checking disk space usage: df -h # checking disk space usage in a directory du -hs * # changing the password of the user: passwd [username ] # listing the cronjobs: crontab -l # editing cronjobs: crontab -e # tailing logs tail -f [filePath ] # tailing last 100 lines of the log file tail -f -n 100 [filePath ] # displaying contents of the file cat [filePath ] more [filePath ] less [filePath ] # showing only lines which have the string \u0026quot;testString\u0026quot; grep \u0026quot;testString\u0026quot; [file ] # grep with OR grep \u0026quot;pattern1 |pattern2\u0026quot; [file ] # extended grep (below does the same like grep with OR, however escaping characters is not required here) grep -e \u0026quot;pattern1|pattern2\u0026quot; [file ] # egrep (same like grep -e, which is extended grep) egrep 'pattern1|pattern2' [file ] # show location in terminal shell prompt (you can add this to ~/.bashrc) export PS1='$(whoami)@$(hostname):$(pwd)' Find where inodes are being used: # navigate to a specific directory cd / find . -printf \u0026quot;%h n\u0026quot; | cut -d/ -f-2 | sort | uniq -c | sort -rn FInd where the storage space is used # navigate to a specific directory cd / du -hs * mv /path/subfolder/{.,}* /path/ # copy files securely via ssh [remote -\u0026gt; local ] (-r for recursive is applicable, -P for port) scp -i test.pem ubuntu@hostname:/remote/location/path /destination/path # copy files securely [local -\u0026gt; remote ] scp -i test.pem /destination/path ubuntu@hostname:/remote/location/path TAR Compress tar -czvf /tmp/destination.tar.gz -C [directory ] [context _directory ] Example:\ntar -czvf /tmp/destination.tar.gz -C /tmp/where _to _compress . Decompress / Extract tar -xzvf /tmp/destination.tar.gz -C /tmp/where _to _extract . Move all files including hidden to parent directory mv /path/subfolder/{.,}* /path/ JSON Manipulations # converting json to ini string echo $json _var | jq -r \u0026quot;to _entries|map( \u0026quot; (.key)= (.value|tostring) \u0026quot;)|. [ ]\u0026quot; How to use screen command  Start a new screen  screen  Detach from the screen  Ctrl + a + d  Attach to the screen  screen -rd  List Screens  screen -ls  Open the next screen to the current  Ctrl + a + n  Publish Date: 2014-10-04\n Execute commands remotely using SSH If you are accessing your SSH using public key authentication:\nsudo ssh -i key.pem ubuntu@host \u0026lsquo;command to be executed remotely\u0026rsquo; ;\nIf you are accessing your SSH using username and password:\nsudo ssh -u root -p ubuntu@host \u0026lsquo;command to be executed remotely\u0026rsquo; ;\n"
},
{
	"uri": "https://majdarbash.github.io/tags/data-fixtures/",
	"title": "data fixtures",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/data-science/",
	"title": "data-science",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/database/",
	"title": "database",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/databases/",
	"title": "Databases",
	"tags": [],
	"description": "",
	"content": "Relational Database on AWS: (RDS) used for OLTP (Online Transaction Processing)  Supported Types  SQL Server Oracle MySQL Server PostgreSQL Aurora MariaDB   Run on virtual machines You cannot log in to these operating systems however Patching of the RDS Operating System and DB is Amazon\u0026rsquo;s responsibility RDS is NOT Serverless Aurora Serverless is Serverless  Read-Replicas vs Muti-AZ  Multi-AZ  For Disaster Recovery AWS handles cross-region replication AWS does an automatic failover of the instance Available for SQL Server, Oracle, MySQL Server, PostgreSQL, MariaDB You can force a failover from one AZ to another by rebooting the RDS instance.   Read Replicas  Improves Performance Up to 5 copies of Read-Replicas All read traffic can be going to read replica Available for MySQL Server, PostgreSQL, MariaDB, Aurora You can create read replicas of read replica as well, however, the latency will be greater then. Read replicas can be promoted to be their own databases - this will break the replication. Read replica can be in a second region. Automated backups should be turned on in order to be able to create read replicas.    RDS Backups\n Automated Backups  Allow you to recover your database to any point in time within a \u0026ldquo;retention period\u0026rdquo;. Retention period can be between 1 and 35 days. Enabled by default Backups stored in S3 Free storage space equal to the size of your database On recovery, RDS will choose the most recent daily backup (full snapshot) and then apply transaction logs relevant to the restore \u0026ldquo;point in time\u0026rdquo;. Backups are taken within the defined window. During the backup window, storage I/O may be suspended and latency may be elevated. Deleted when RDS instance is deleted   Snapshots  Done manually Stored even after deletion of RDS instance   Restoring snapshots or automated backups will create a new RDS instance with a new DNS endpoint.  Encryption:\n Encryption at rest is supported. Encryption is done using AWS Key Management Service (KMS). Automated backups, snapshots and read replicas of encrypted storage will be encrypted as well. Is available for all RDS supported types  DynamoDB (NoSQL database, Non-Relational Databases)  Terms:  Collection Document Key Value Pairs   Fast and flexible NoSQL database service Single-digit millisecond latency at any scale Fully managed database Great fit for mobile, web, gaming, ad-tech, IoT and many other applications Stored on SSD storage Spread across 3 geographically distinct data centers Eventual Consistent Reads (Default) Strongly Consistent Reads (1-second rule)  Allocated resources:\n Read Capacity Unit (for an item up to 4 KB)  One strongly consistent read per second Two eventual consistent reads per second   Write Capacity Unit (for an item up to 1KB)  One write per second    AWS Redshift Data Warehousing\n Business Intelligence Tools Pulling very large and complex data sets. Usually used by management to do queries on data OLTP vs OLAP\n(Online Transaction Processing vs Online Analytics Processing) Data Warehousing databases use a different type of architecture both from a database perspective and infrastructure layer.  Amazon Redshift (OLAP)\n Amazon\u0026rsquo;s Data Warehouse Solution Fast and powerful, fully managed, petabyte-scale data warehouse service in the cloud Configuration  Single Node (160GB) Multi-Node  Leader Node (manages client connections and receives queries) Compute Node (store data and perform queries and computations). Up to 128 Compute Nodes.     Advanced Compression  Columnar data stores can be compressed much more than row-based data stores because similar data is stored sequentially on disk. Does not require indexes or materialized views, so uses less space than traditional relational database systems. When loading data into an empty table, Amazon Redshift automatically samples your data and selects the most appropriate compression scheme.   Massive Parallel Processing (MPP)  Distributes data and query load across all nodes. Makes it easy to add odes to your data warehouse and enables you to maintain fast query performance as your data warehouse grows.   Backups  By default 1 day retention period, up to 35 days. Always attempts to maintain at least three copies of your data (the original and replica on the compute nodes and a backup in Amazon S3) Can also asynchronously replicate your snapshots to S3 in another region for disaster recovery.   Pricing  Compute Node Hours You will not be charged for leader node hours. Backup charges Data transfer (only within a VPC, not outside it)   Security Considerations  Encrypted in transit using SSL Encrypted at rest using AES-256 encryption Redshift takes care of key management  Manage your key through HSM (Hardware Security Module) AWS Key Management Service     Availability  Available in 1AZ Can restore snapshots to the new availability zone    Aurora  Amazon\u0026rsquo;s own proprietary database that is compatible with MySQL and PostgreSQL 2 copies of your data are contained in each AZ with a minimum of 3 AZs. 6 copies of your data. You can share Aurora Snapshots with other AWS accounts. 2 types of replicas available  Aurora Replicas MySQL replicas   Automated failover is only available with Aurora Replicas. You can do migrations from MySQL to Aurora only by creating an Aurora read replica and promoting it. Provides up to 5 times better performance than MySQL. Start with 10GB, Scales in 10GB increments to 64TB (Storage Autoscaling) Compute resources can scale up to 32vCPU and 244GB of Memory Designed to transparently handle the loss of up to two copies of data without affecting database write availability and up to three copies without affecting read availability. Automated backups are always enabled and they do not impact database performance. Taking snapshots does not impact performance.  ElasticCache Web service that makes it easy to deploy, operate and scale an in-memory cache in the cloud. Improves the performance of web applications. Retrieves information from fast, managed, in-memory caches.\n Memcached  Can be scaled horizontally   Redis  Multi-AZ You can do backups and restores of Redis    FAQs  By default customers are allowed to have up to a total of 40 Amazon RDS DB instances. Database limits per RDS instance are imposed based on software limitation. Maintenance windows - defines the time range when DB instance modifications, database engine version upgrades, and software patching occurs, in the event they are requested or required. Auto Minor Version Upgrade setting will automatically schedule the minor version upgrade in the next maintenance window. When a major version is deprecated in RDS a minimum 6 month period will be granted to upgrade to a supporter major version. Billing components include: DB instance hours, Storage, I/O requests per month, Provisioned IOPS per month, Backup Storage, Data transfer. Amazon RDS supported storage types are: Provisioned IOPS (SSD) Storage (OLTP workloads) and General Purpose (SSD) Storage (for moderate I/O requirements) RDS Automated Backups - allows point in time recovery using transaction logs. Snapshots - are user-initiated backups - backing up your DB instance. RDS master user account is a native database user account which you can use to connect to your DB Instance. You can encrypt connections between your application and the DB instance using SSL/TLS. Amazon RDS generates an SSL/TLS certificate for each DB Instance. Once an encrypted connection is established, data transferred between the DB Instance and your application will be encrypted during transfer. Amazon RDS supports encryption at rest for all database engines, using keys you manage using AWS Key Management Service (KMS). By default, Amazon RDS chooses the optimal configuration parameters for your DB Instance taking into account the instance class and storage capacity. When you create or modify your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different Availability Zone. When you run a DB instance as a Multi-AZ deployment, the “primary” serves database writes and reads. In addition, Amazon RDS provisions and maintains a “standby” behind the scenes, which is an up-to-date replica of the primary. The standby is “promoted” in failover scenarios. After failover, the standby becomes the primary and accepts your database operations. You may observe elevated latencies relative to a standard DB instance deployment in a single Availability Zone as a result of the synchronous data replication performed on your behalf. Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. RDS initiates failover in these events: Loss of availability in primary Availability Zone, Loss of network connectivity to primary, Compute unit failure on primary, Storage failure on primary. AWS will emit a DB instance event in case of failover. You can alsouse RDS Event Notifications to be notified of specific events. You can initiate a forced failover when rebooting your instance. Automatic Backups must be enabled on instance to be able to create read replicas. Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle allow you to create up to 5 read replicas for a given source DB instance. Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle allow you to enable Multi-AZ configuration on read replicas to support disaster recovery and minimize downtime from engine upgrades. Amazon Aurora, Amazon RDS for MySQL and MariaDB: You can create a second-tier read replica from an existing first-tier read replica. By creating a second-tier read replica, you may be able to move some of the replication load from the master database instance to a first-tier Read Replica. Please note that a second-tier Read Replica may lag further behind the master because of additional replication latency introduced as transactions are replicated from the master to the first tier replica and then to the second-tier replica.  "
},
{
	"uri": "https://majdarbash.github.io/aws/deep-dive-on-aws-s3-re-invent/",
	"title": "Deep Dive on Amazon S3 &amp; Amazon Glacier Storage Management",
	"tags": [],
	"description": "",
	"content": " Storage Management on S3  User Permission Management By Tagging S3 Inventory Storage Class Analysis Object-Level Logging Cross-Region Replication (CRR) Automate with Trigger-Based Workflow Amazon S3 event notifications Default Encryption Amazon Macie   AlertLogic S3 use-case on AWS  S3 Object Management Tags with Lifecycle Expiration Policies Tags with Lifecycle Transition Policies Demonstrate Scale of Storage Solution (AWS re:Invent 2017)    Storage Management on S3  Organize  Object Tagging   Monitor and Analyze  S3 Inventory Amazon CloudWatch Storage Class Analysis AWS CloudTrail   Act  Cross Region replications Event Notification Lifecycle Policy   Security Management  AWS KMS AWS IAM Bucket Permissions Check Encryption Status in S3 Inventory Default Encryption Trusted advisor Amazon Macie    User Permission Management By Tagging { \u0026#34;version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::Project-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: {\u0026#34;StringEquals\u0026#34;: {\u0026#34;s3:RequestObjectTag/Project\u0026#34;: \u0026#34;x\u0026#34;}} } ] } S3 Inventory  Generates a CSV / ORC file based of all objects in S3 bucket with respect to filter criteria. Triggers business workflows and applications such as secondary index, garbage collection, data auditing and offline analytics.  Features:\n Save time Daily or Weekly delivery Delivery notification Delivery to S3 bucket Same set of metadata as the LIST API Can add size, last modified date, storage class, etag or replication status Object-level Encryption Status Encrypt Inventory with SSE-S3 or SSE-KMS CSV or ORC output format Query with Athena, Redshift Spectrum or any Hive tools  S3 Inventory can be queried with Amazon Athena:\nCREATE EXTERNAL TABLE my_inventory_table( `bucket` string, `key` string, `version_id` string, `is_latest` boolean, `is_delete_marker` boolean, `size` bigint, `last_modified_date` timestamp, `e_tag` string, `storage_class` string, `is_multipart_uploaded` boolean, `replication_status` string, `encryption_status` string ) PARTITIONED BY (dt string) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.ql.io.orc.OrcSerde\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.SymLinkTextInputFormat\u0026#39; LOCATION \u0026#39;s3://bucketname/inventory/output_destination/hive\u0026#39; Storage Class Analysis  Data-driven storage management for S3 Daily Storage Class Analysis Export Analysis data to your S3 Bucket Filter by Bucket, Prefix, or Object Tags  Process:\n Monitors access patterns to understand your storage usage After 30 days, recommends when to move objects to Standard - Infrequent Access Export file includes a daily report of storage, retrieved bytes, and GETs by object age  Object-Level Logging  Allows Logging CloudTrail for Read / Write Events on the Objects  Cross-Region Replication (CRR) Use cases:\n Compliance Lower latency Security  Features:\n Ownership overwrite for cross-account CRR Support SSE-KMS Encrypted objects Choose any S3 Storage Class as target Choose any AWS region as target Bi-directional replication Lifecycle Policy  Automate with Trigger-Based Workflow Amazon S3 event notifications  Notifications when objects are created via Put, Post, Copy, Multipart Upload, or Delete Filter on prefixes and suffixes Trigger workflow with Amazon SNS, Amazon SQS, and Amazon Lambda functions  Default Encryption  Automatically encrypts all objects written to your Amazon S3 bucket Choose SSE-S3 or SSE-KMS Makes it easy to satisfy compliance needs  Amazon Macie  Security service that uses machine learning to automatically discover, classify and protect sensitive data in AWS Recognizes sensitive data Continously monitors data access Provides dashboards and alerts  AlertLogic S3 use-case on AWS S3 Object Management  S3 Object Keys use hash prefix for performance: logmsgs-001:/X-OGA/11543.2016-03/... S3 Objects written with two Tags  Customer identitfier (cid=1234567890) Date (date=2017-06)   AWS KMS used to generate data encryptionkeys  Customer Master Key (CMK) for each data type with automatic rotation enabeld Data Keys generated per-customer/per-month    Tags with Lifecycle Expiration Policies  Per Customer Expiration Rule Uses cid and date tags as filter Indepdendent of object create time  \u0026lt;Rule\u0026gt; \u0026lt;ID\u0026gt;expiration-12345\u0026lt;/ID\u0026gt; \u0026lt;Status\u0026gt;Enabled\u0026lt;/Status\u0026gt; \u0026lt;Filter\u0026gt; \u0026lt;And\u0026gt; \u0026lt;Tag\u0026gt; \u0026lt;Name\u0026gt;cid\u0026lt;/Name\u0026gt; \u0026lt;Value\u0026gt;12345\u0026lt;/Value\u0026gt; \u0026lt;/Tag\u0026gt; \u0026lt;Tag\u0026gt; \u0026lt;Name\u0026gt;date\u0026lt;/Name\u0026gt; \u0026lt;Value\u0026gt;2015-09\u0026lt;/Value\u0026gt; \u0026lt;/Tag\u0026gt; \u0026lt;/And\u0026gt; \u0026lt;/Filter\u0026gt; \u0026lt;Expiration\u0026gt; \u0026lt;!-- Depends entirely on the tag values --\u0026gt; \u0026lt;Days\u0026gt;0\u0026lt;/Days\u0026gt; \u0026lt;/Expiration\u0026gt; \u0026lt;/Rule\u0026gt; Tags with Lifecycle Transition Policies  One Transition Rule per month Uses date tag as filter  \u0026lt;Rule\u0026gt; \u0026lt;ID\u0026gt;transition-ia-3months\u0026lt;/ID\u0026gt; \u0026lt;Status\u0026gt;Enabled\u0026lt;/Status\u0026gt; \u0026lt;Filter\u0026gt; \u0026lt;And\u0026gt; \u0026lt;Tag\u0026gt; \u0026lt;Name\u0026gt;date\u0026lt;/Name\u0026gt; \u0026lt;Value\u0026gt;2016-07\u0026lt;/Value\u0026gt; \u0026lt;/Tag\u0026gt; \u0026lt;/And\u0026gt; \u0026lt;/Filter\u0026gt; \u0026lt;Transition\u0026gt; \u0026lt;StorageClass\u0026gt;STANDARD_IA\u0026lt;/StorageClass\u0026gt; \u0026lt;/Transition\u0026gt; \u0026lt;/Rule\u0026gt; Demonstrate Scale of Storage Solution (AWS re:Invent 2017)  Scaled wrokload 100x successfully  140PB/month of customer data 30k writes/second sustained Write latency 200ms at 95th percentile Read latency 125ms at 95th percentile   Limited only by resources driving traffic  "
},
{
	"uri": "https://majdarbash.github.io/software-architecture-and-design/design-patterns/",
	"title": "Design Patterns",
	"tags": [],
	"description": "",
	"content": " Factory Adapter Active Record Observer Decorator Repository Service Locator Singleton Strategy Facade  Factory In this pattern, the Factory class is responsible for instantiating the defined models. If in future you would like to exchange the model or change the way the model is instantiated\nExample:\nclass Book { private $isbn; private $title; private $author;\npublic function \\_\\_construct($isbn, $title, $author) { $this-\u0026gt;isbn = $isbn; $this-\u0026gt;title = $title; $this-\u0026gt;author = $author; }  }\nclass BookFactory { public static function create($isbn, $title, $author) { return new Book($isbn, $title, $author); } }\nAdapter The adapter pattern adjusts the interface of one class to match that of another.\nDrawback:\n You\u0026rsquo;re just hiding a bad design  class NotificationAdapter { protected $username = \u0026lsquo;'; protected $password = \u0026lsquo;';\npublic function __construct($username, $password) { $this-\u0026gt;username = $username; $this-\u0026gt;password = $password; }\npublic function send($to, $from, $body, $subject = \u0026lsquo;') { if (\u0026rsquo;\u0026rsquo; == $subject) { return $this-\u0026gt;sendSMS($to, $from, $body); } else { return $this-\u0026gt;sendEmail($to, from, $body, $subject); } }\nprotected function sendSMS($to, $from, $body) { echo \u0026lsquo;Sending SMS Implementation \u0026hellip;'; }\nprotected function sendEmail($to, $from, $body, $subject) { echo \u0026lsquo;Sending Email Implementation \u0026hellip;'; } }\n$notificationAdapter = new NotificationAdapter();\n$notificationAdapter-\u0026gt;send(\u0026lsquo;test@test.com\u0026rsquo;, \u0026lsquo;me@test.com\u0026rsquo;, \u0026lsquo;hello!', \u0026lsquo;test message\u0026rsquo;);\nActive Record Database table or view will be mapped into an object.\nDrawbacks:\n* objects are tightly coupled to the database schema\n* objects are tightly coupled to the database itself\nExample:\nclass User { protected $connection = null;\npublic function __construct() { $this-\u0026gt;connection = new PDO(\u0026ldquo;mysql:host=localhost;dbname=development\u0026rdquo;, \u0026lsquo;root\u0026rsquo;, \u0026lsquo;root\u0026rsquo;); }\npublic function load($id) { $sql = \u0026lsquo;SELECT * FROM users WHERE user_id=\u0026rsquo; . (int)$id; $result = $this-\u0026gt;connection-\u0026gt;query($sql); $row = $result-\u0026gt;fetch(PDO::FETCH_ASSOC);\n foreach ($row as $column =\u0026gt; $value) { $this-\u0026gt;column = $value; }  } }\n$user = new User(); $user-\u0026gt;load(2);\nprint_r($user);\nObserver In this design pattern one object (subject) notifies its dependent objects (observers) about any state changes.\nDecorator Decorator allows you to add specific behaviors to the instances of a class instead of attaching them to the object itself.\nDrawbacks:\n testing can be hard  Repository Provides a more object-oriented view of the persistence layer. It encapsulates the set of objects persisted in a data store and the operations performed over them. Supports the objective of clean separation between the domain and data mapping layers.\nExample:\nclass Order { /** * @var int */ private $id; /** * @var string */ private $name; /** * @var DateTime */ private $date;\npublic function __construct(int $id, string $name, DateTime $date) { $this-\u0026gt;id = $id; $this-\u0026gt;name = $name; $this-\u0026gt;date = $date; }\npublic static function fromState(array $state): Order { return new self($state[\u0026lsquo;id\u0026rsquo;], $state[\u0026lsquo;name\u0026rsquo;], $state[\u0026lsquo;date\u0026rsquo;]); }\n/** * @return int */ public function getId(): int { return $this-\u0026gt;id; }\n/** * @return string */ public function getName(): string { return $this-\u0026gt;name; }\n/** * @return DateTime */ public function getDate(): DateTime { return $this-\u0026gt;date; } }\ninterface Persistence { public function generateId();\npublic function persist(array $data);\npublic function retrieve(int $id);\npublic function delete(int $id); }\nclass MemoryPersistence implements Persistence {\nprivate $data = []; public $lastId = 0;\npublic function generateId() { $this-\u0026gt;lastId++; return $this-\u0026gt;lastId; }\npublic function persist(array $data) { $this-\u0026gt;data[$this-\u0026gt;lastId] = $data; }\npublic function retrieve(int $id) { if (!isset($data[$id])) { throw new OutOfBoundsException(\u0026lsquo;Cannot find the requested record!'); } return $this-\u0026gt;data[$id]; }\npublic function delete(int $id) { if (!isset($data[$id])) { throw new OutOfBoundsException(\u0026lsquo;Cannot find the requested record!'); } unset($this-\u0026gt;data[$id]); }\n}\nclass OrderRepository { /** * @var Persistence */ private $persistence;\n/** * OrderRepository constructor. */ public function __construct(Persistence $persistence) { $this-\u0026gt;persistence = $persistence; }\npublic function generateId(): integer { return $this-\u0026gt;persistence-\u0026gt;generateId(); }\npublic function findById(integer $id) { $arrayData = $this-\u0026gt;persistence-\u0026gt;retrieve($id); return Order::fromState($arrayData); }\npublic function save(Order $order) { $this-\u0026gt;persistence-\u0026gt;persist([ \u0026lsquo;id\u0026rsquo; =\u0026gt; $order-\u0026gt;getId(), \u0026lsquo;name\u0026rsquo; =\u0026gt; $order-\u0026gt;getName(), \u0026lsquo;date\u0026rsquo; =\u0026gt; $order-\u0026gt;getDate(), ]); } }\n$orderRepository = new OrderRepository(new MemoryPersistence());\n$order = Order::fromState([ \u0026lsquo;id'=\u0026gt;12, \u0026lsquo;name'=\u0026gt;'Flowers Order\u0026rsquo;, \u0026lsquo;date'=\u0026gt;new DateTime() ]);\n$orderRepository-\u0026gt;save($order); $orderInPersistence = $orderRepository-\u0026gt;findById(12);\nprint_r($order); print_r($orderInPersistence);\nService Locator This is considered as an anti-pattern.\nWith service locators services are usually defined globally throughout the app and are instantiated to be used as dependency wherever required. According to symfony docs: \u0026ldquo;Service locators are a design pattern that encapsulates the processes involved in obtaining a service using a central registry known as service locator\u0026rdquo;.\nSingleton Singleton pattern means allowing access to a single instance of defined class. For example, Mailer class could be reused several times throughout your application to send emails. It doesn\u0026rsquo;t have to be instantiated several times and you should be able to access the same instance once you need to use the Mailer class. The same applies to the database class. You would like to use existing database connection instead of constructing and creating a spare connection to your database every time you want to execute a query.\nThough singleton pattern guarantee that class has only one instance and can be easily accessed a lot of developers consider it as anti-pattern. Singleton pattern may introduce global state in application, unnecessary restrictions and make your code easier to maintain and reuse. Writing tests can be also more complicated.\nStrategy Another name for this pattern is Policy pattern. In this pattern families of Algorithms are grouped together. This pattern is useful in particularly when several algorithms can perform the same operation and we would like the application to pick up the appropriate algorithm based on specific attributes.\nInterfaces serve by providing contracts which must be obeyed by any new concrete implementation.\nExample:\nIn this example there are 2 ways to ship an item: cargo or freight. Based on user\u0026rsquo;s choice of shipping type the order price will vary.\ninterface ShippingStrategy { public function getPrice(); }\nclass ShipByFreight implements ShippingStrategy { public function getPrice() { return 1000; } }\nclass ShipByCargo implements ShippingStrategy { public function getPrice() { return 2000; } }\nclass Order {\nprivate $itemCode; private $itemName; private $shippingType; /\\*\\* \\* Order constructor. \\*/ public function \\_\\_construct($itemCode, $itemName, $shippingType) { $this-\u0026gt;itemCode = $itemCode; $this-\u0026gt;itemName = $itemName; $this-\u0026gt;shippingType = $shippingType; } public function getPrice() { if ($this-\u0026gt;shippingType == 'cargo') { $shipping = new ShipByCargo(); } elseif ($this-\u0026gt;shippingType == 'freight') { $shipping = new ShipByFreight(); } else { throw new Exception('Undefined shipping type: ' . $this-\u0026gt;shippingType); } return $shipping-\u0026gt;getPrice(); }  }\n$order = new Order(\u0026lsquo;10029NA\u0026rsquo;, \u0026lsquo;iPhone X\u0026rsquo;, \u0026lsquo;cargo\u0026rsquo;);\necho $order-\u0026gt;getPrice();\nFacade This pattern belongs to the structural patterns. It specifies how the code should be structured to be organized and maintainable.\nRepetitive code throughout the system can be contained in facade and called with proper arguments to perform specific action.\nExample:\nclass FriendshipFacade { private $notificationService; private $userService; private $friendshipService;\npublic function __construct( NotificationService $notificationService, UserService $userService, FriendshipService $friendshipService ) { $this-\u0026gt;notificationService = $notificationService; $this-\u0026gt;userService = $userService; $this-\u0026gt;friendshipService = $friendshipService; }\npublic function add($userId1, $userId2) { $user1 = $this-\u0026gt;userService-\u0026gt;get($userId1); $user2 = $this-\u0026gt;userService-\u0026gt;get($userId2);\n $this-\u0026gt;friendshipService-\u0026gt;add($user1, $user2); $this-\u0026gt;notificationService-\u0026gt;notify($user1, 'You have a new friend: ' . $user2-\u0026gt;name); $this-\u0026gt;notificationService-\u0026gt;notify($user2, 'You have a new friend: ' . $user1-\u0026gt;name);  } }\nclass UserController { public function addFriend() { $friendshipFacade = new FriendshipFacade(); $friendshipFacade-\u0026gt;add($userId1, $userId2); } }\n"
},
{
	"uri": "https://majdarbash.github.io/tags/design-patterns/",
	"title": "design patterns",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/devops/",
	"title": "devops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/doctrine/",
	"title": "doctrine",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/ec2/",
	"title": "EC2",
	"tags": [],
	"description": "",
	"content": " EC2 Instances  Security Groups   EBS Volumes  5 Types of EBS Storage   AMI\u0026rsquo;s CloudWatch AWS CLI IAM Roles Instance Metadata EFS (Elastic File System) FAQ Notes  Overview  Compute-Optimized Instances General-Purpose Instances High Memory Instances Previous Generation Instances Memory Optimized Instances Storage Optimized instances   Storage Networking and Security Management Billing Platform    Amazon EC2 is a web service that provides resizable compute capacity in the cloud. Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change.\nEC2 Pricing Models   On Demand\nAllows you to pay a fixed rate by the hour with no commitment.\n  no up-front payment required\n  applications with short term, spiky or unpredictable workloads that cannot be interrupted\n  applications being developed or tested on Amazon EC2 for the first time\n  Reserved\nProvides you with a capacity reservation and offer a significant discount on the hourly charge for an instance. Contract Terms are 1 Year or 3 Year Terms.\n applications with steady state or predictable usage applications that require reserved capacity users able to make upfront payments to reduce their total computing costs even further Types of Reserved Pricing  Standard Reserved Instances\nUp to 75% off on-demand instances. The more you pay up front and the longer the contract, the great the discount. Convertible Reserved Instances\nThese offer up to 54% off on demand capability to change the attributes of the Reserved Instance as long as the exchange results in the creation of Reserved Instances of equal or greater value. Scheduled Reserved Instances      Spot\nEnables you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flexible start and end times.\n Applications that have flexible start and end times Applications that are only feasible at very low compute prices Users with urgent computing needs for large amounts of additional capacity If the Spot instance is terminated by EC2 you will not be charged for a partial hour of usage. However, if you terminate the instance yourself, you will be charged for any hour in which instance ran.    Dedicated Hosts Pricing\n Regulatory requirements that may not support multi-tenant virtualization Great for licensing which does not support multi-tenancy or cloud deployments Can be purchased On-Demand (hourly.)    EC2 Instance Types  F1 - FPGA, Field Programmable Gate Array I3 - IOPS, High Speed Storage G3 - Graphics, Graphics Intensive H1 - High Disk Throughput T3 - Cheap general purpose (think T2 Micro) D2 - Density, Dense Storage R5 - RAM, Memory Optimized M5 - Main choice for general purpose apps C5 - Compute, Compute optimized P3 - Graphics (think Pics), Graphics / General Purpose GPU X1 - Extreme Memory, Memory Optimized Z1D - Extreme Memory and CPU, High compute capacity and a high memory footprint A1 - Arm-based workloads U-6tb1 - Bare Metal  EC2 Instances  Termination protection is turned off by default On an EBS-backed instance, the default action is for the root EBS volume to be deleted when the instance is terminated EBS Root Volumes of the default AMI\u0026rsquo;s cannot be encrypted. You can only use a third-party tool to encrypt the root volume. Additional volumes can be encrypted upon launching an instance.  Security Groups   All Inbound traffic is blocked by default\n  All Outbound traffic is allowed\n  If you add / remove a security group to EC2 instance or modify the rules in security group, this change will take place immediately.\n  You can have any number of EC2 instances within a security group\n  You can have multiple security groups attached to EC2 Instances\n  Security Groups are STATEFUL\nIf you create an inbound rule allowing traffic in, that traffic is automatically allowed back out again\n  You cannot block specific IP addresses using Security Groups, instead, use Network Access Control Lists\n  You can specifically allow rules, but not deny rules.\n  Status Checks\n System Status Checks Instance Status Checks    Detailed Monitoring\n  EBS Volumes 5 Types of EBS Storage   General Purpose (SSD)\n API Name: gp2 Up to 16000 IOPS Applications: Most Workloads Storage: 1GB - 16TB    Provisioned IOPS (SSD)\n API Name: io1 Up to 64000 IOPS Highest performance SSD volume designed for mission-critical applications Applications: Databases Storage: 4GB-16TB    Throughput Optimized (HDD)\n API Name: st1 Up to 500 IOPS Low-cost HDD volume for frequently accessed throughput intensive workloads Applications: Big Data \u0026amp; Data Warehouses Storage: 500GB - 16TB    Cold Hard Disk Drive (HDD)\n API Name: sc1 Up to 250 IOPS Lowest cost HDD volume designed for less frequently accessed workloads Applications: File Servers Storage: 500GB - 16TB    Magnetic (HDD)\n API Name: Standard 40-200 IOPS Previous generation HDD Applications: Workloads where data is infrequently accessed Storage: 1GB - 1TB    By default, upon EC2 instance termination:\n root EBS volume will be automatically terminated additional volumes attached to the instance will continue to persist    SSD is good for random access, HDD is better for sequential access\n  EBS Encryption\n Root device cannot be encrypted on the first launch  To encrypt the root volume:  Launch the instance with the root volume Create a snapshot of unencrypted root device volume Create a copy of the Snapshot and select the encrypt option Create an AMI from the encrypted Snapshot Use AMI to launch new encrypted instances\n(only specific instance types will be supported)     Secondary volumes can be encrypted Snapshots of encrypted volumes are encrypted automatically Volume restored from encrypted snapshots are encrypted automatically You can share snapshots, but only if they are unencrypted These snapshots can be shared with other AWS accounts or made public    Volumes exist on EBS\n  Snapshots exist on S3\n  Snapshots are a point in time copies of Volumes\n  Snapshots are incremental - this means that only the locks that have changed since your last snapshot are moved to S3\n  If you take snapshot for the first time it may take some time\n  You can take a snapshot while instance is running, however it\u0026rsquo;s better to stop the instance when taking snapshot from the root Volume to assure consistency\n  You can create AMI\u0026rsquo;s from both Volumes and Snapshots\n  EBS volume sizes can be changed on the fly\n  Volumes will ALWAYS be in the same availability zone as the EC2 instance\n  Moving EC2 volume from one AZ to another\n take snapshot of the volume create AMI from the snapshot launch EC2 instance in a new AZ using the AMI    Moving EC2 volume from one region to another\n take a snapshot of the volume create AMI from the snapshot copy AMI from one region to another use AMI to launch the new EC2 instance in the new region    AMI\u0026rsquo;s AMI can be selected based on:\n Region Operating System Architecture (32-bit / 64-bit) Launch Permissions Storage for the Root Device (Root Device Volume)  Instance Store (Ephemeral Storage)  The root device for an instance launched from the AMI is an instance store volume created from a template stored in Amazon S3 Instance store volumes will not be shown in volumes section Instance store instances cannot be stopped: they can only be terminated or rebooted If the underlying host fails, you will loose your data Cannot keep root volume upon instance termination Data is lost when the instance is restarted These instances are free, you will get charged for the instance usage Instance size is determined by the instance type Temporary block-type storage Disks are physically attached to the hardware (SSD / HDD) These volumes are available to specific instance types   EBS Backed Volumes  Persistent block storage volume The root device for an instance launched from the AMI is an Amazon EBS volume created from an Amazon EBS snapshot Can be attached to any EC2 instance type Can be scaled up and down based on your requirements      CloudWatch  CloudWatch monitors performance CloudWatch monitors most of AWS as well as your applications that run on AWS CloudWatch with EC2 will monitor events every 5 minutes by default You can have 1-minute intervals by turning on Detailed Monitoring You can create CloudWatch alarms with trigger notifications Host Level Metrics  CPU Network Disk Status Check   Alarms\nAllows you to set Alarms that notify you when particular thresholds are hit Dashboards\nCreates awesome dashboards to see what is happening in your AWS environment Logs\nHelps you aggregate, monitor and store your log data Events\nHelp you to respond to state changes in your AWS resources  AWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls.\nAWS CLI  You can interact with AWS from anywhere in the world just by using the command line (CLI) You will need to set up access in IAM  IAM Roles  Roles are more secure than storing your access key and secret access key on individual EC2 instances Roles are easier to manage Roles can be assigned to an EC2 instance after it is created using both the console \u0026amp; command line Roles are universal - you can use them in any region  Instance Metadata  Metadata is used to get information about an instance (such as public ip) Examples  Show the bootstrap script\ncurl http://169.254.169.254/latest/user-data Show the options available\ncurl http://169.254.169.254/latest/meta-data Show the private and public v4 IPs\ncurl http://169.254.169.254/latest/meta-data/local-ipv4\ncurl http://169.254.169.254/latest/meta-data/public-ipv4    EFS (Elastic File System)  File storage service for Amazon EC2 instances Storage capacity is elastic, growing and shrinking automatically as you add and remove files It\u0026rsquo;s a great way to share files among EC2 instances Can be mounted to thousands of instances at the same time Supports the Network File System version 4 (NFSv4) protocol EFS can be mounted using  EFS mount helper EFS mount helper with TLS NFS   You only pay for the storage used Can scale up to petabytes Can support thousands of concurrent NFS connections Data is stored across multiple AZ\u0026rsquo;s within a region Read After Write Consistency  FAQ Notes Overview  Longer EC2, EBS and Storage Gateway resource IDs allow uninterrupted creation of new resources The speed of launching an instance depends on the number of factors, including the size of your AMI, the number of instances you are launching, and how recently you have launched that AMI. Images launched for the first time may take slightly longer to boot. Instance Limits: you are limited to running a total of 20 On-Demand instances across the instance family, purchasing 20 Reserved Instances and requesting spot instances based on your dynamic Spot limit per region. There are limitations of the number of emails that can be sent out from EC2. These limits can be removed when filling the request. EC2 supports a variety of operating systems including Amazon Linux, Ubuntu, Windows Server, Red Hat Enterprise Linux, SUSE Linux Enterprise Server, Fedora, Debian, CentOS, Gentoo Linux, Oracle Linux, and FreeBSD. All AWS underlying hardware in EC2 uses ECC memory (Error-correcting code memory) Service Level Agreement (SLA)  Guarantees a Monthly Uptime Percentage of at least 99.99% for EC2 and EBS within a Region SLA credit will be provided if the region you are operating in has a Monthly Uptime Percentage of less than 99.95% during any monthly billing cycle.   Accelerated Computing Instances\nUse hardware accelerators or co-processors to perform some functions, such as floating point number calculation and graphics processing, more efficiently than is possible in software running on CPUs. EC2 provides 3 types of accelerated computing instances: GPU compute instances, GPU graphics instances, FPGA programmable hardware compute instances. GPU Instances  workloads with massive parallelism good for Graphics processing good for applications where the throughput of a pipeline is more important than the latency of the individual operations   G3 Instances  G3 provided a high-performance platform for applications using DirectX or OpenGL G3 instances support DirectX 12, OpenGL 4.5, CUDA 8, and OpenCL 1.2   P3 Instances  P3 are the next-generation of EC2 general-purpose GPU computing instances P3 instances support CUDA 9 and OpenCL P3 have new features like Streaming Multiprocessor (SM) architecture for machine learning (ML)/deep learning (DL) performance optimization, second-generation NVIDIA NVLink high-speed GPU interconnect and highly tuned HBM2 memory for higher-efficiency NVIDIA Tesla V100 accelerator adds features that improve programmability - advances will supercharge HPC, data center, supercomputer, and deep learning systems and applications. P3 Instances benefit Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL) and High-Performance Computing (HPC) applications. Users includes data scientists, data architects, data analysts, scientific researchers, ML engineers, IT managers and software developers Use-cases for P3 instances: autonomous vehicle platforms, speech, image and text recognition systems, intelligent video analytics, molecular simulations, drug discovery, disease diagnosis, weather forecasting, big data analytics, financial modeling, robotics, factory automation, real-time language translation, online search optimizations, and personalized user recommendations P3 Instances will support VPC only.   P2 instances  P2 instances support CUDA 8 and OpenCL 1.2   NVIDIA drivers for P3 and G3 instances  Can be obtained from AWS Marketplace by using AMIs with the pre-installed drivers Alternatively, you can use HVM AMIs and install the drivers yourself   EC2 F1 Instance  The instance with programmable hardware that can be used for application acceleration High performance, easy to access FPGA for developing and deploying custom hardware accelerations   Amazon FPGA Image (AFI) - the design that you create to program your FPGA  Compute-Optimized Instances  C5 Instances offer a 25% price/performance benefit over C4 instances  General-Purpose Instances  A1 Instances  Using Graviton Processors ARM ecosystem Applications based on interpreted or run-time compiled languages should run without modifications. Other applications may require to be recompiled to run on ARM instead of x86. arm64 AMIs should be used x86 AMIs are not compatible EBS volumes are supported, through Non-Volatile Memory Express (NVMe) interface. Blkfront interface is not supported Supports ENA based Enhanced Networking - up to 10Gbps of network bandwidth Support AWS Nitro System - a combination of dedicated hardware and Nitro hypervisor   M5 Instances  Good choice for running development and test environments, web, mobile and gaming applications, analytics applications Provides instances with a higher memory footprint Customers using M4 Instances should move to M5 Delivers up to 14% improvement in price/performance compared to M4 EBS Volumes is the only storage supported Supports ENA Enhanced Networking with up to 25 Gbps of network bandwidth when launched within a placement group Up to 27 EBS Volumes can be used Uses lightweight Nitro Hypervisor, based on core KVM technology   Fixed performance instances are C, M and R Burstable performance instance (T2)  Bursting CPU Credits CPU Credit Balance and other related metrics can be monitored from CloudWatch When running low on credits t2 instance provides a baseline CPU performance of 10% of a physical CPU core. The stopped instance does not retain the previously earned credit balance    High Memory Instances  6 TB, 9 TB, or 12 TB of memory in a single instance Useful for running large in-memory databases Uses ENA-based Enhanced Networking Delivers high networking throughput and low-latency with 25 Gbps bandwidth Certified by SAP for running Business Suite on HANA Instance Types: u-6tb1.metal, u-9tb1.metal and u-12tb1.metal 448 logical processors, a total of 224 CPU cores EBS Volumes supported, EBS-optimized instances by default These instances are bare metal instances, they do not run on a hypervisorAva Available on EC2 Dedicated Hosts on a 3-year Reservation You can launch, stop/start, and terminate instances on your EC2 Dedicated Hosts using AWS CLI/SDK.  Previous Generation Instances  M1, C1, CC2 and HS1  Memory Optimized Instances  Large memory size for memory intensive applications X1 instances are ideal for in-memory databases X1e instances offer twice the memory per vCPU compared to the X1 instances X1e instances - 32 GB of memory per vCPU, from 4 vCPUs to 128 vCPUs X1 instances - 65 GB of memory per vCPU Intel E7 processors SSD based instance store and EBS (Instances are EBS-Optimized by default)  Storage Optimized instances  Dense-storage instance - for workloads with high sequential read and write access to very large data sets HDD-storage instances d2.8xlarge - up to 3.5 GBps read and 3.1 GBps write disk throughput with a 2MiB block size, h1.16xlarge - up to 1.15 GBps read and write. H1 and D2 instances are EBS-optimized by default  Storage  Attaching multiple instances to one volume is not supported at this time Though EBS snapshots reside in S3, they cannot be accessed using S3 API, and can be accessed using EC2 APIs Snapshots can be done while volume is attached an in use You can find the snapshots shared with you by using the \u0026ldquo;Private Snapshots\u0026rdquo;, while public using \u0026ldquo;Public Snapshots\u0026rdquo; from the viewing dropdown in the console  Networking and Security  Elastic Fabric Adapter (EFA) - uses a custom-built operating system bypass technique to enhance the performance of inter-instance communications which is critical to scaling HPC applications. All accounts are limited to 5 Elastic IP addresses per region. You will be charged a small hourly rate for each unassigned IP address. Enhanced networking provides significantly improved performance, consistency of performance and scalability.  can be enabled when launching an HVM AMI with the appropriate drivers    Management  Amazon CloudWatch receives and aggregates data at 1-minute intervals CloudWatch metrics for terminated resources are available for 2 weeks Instance Hibernation  useful when the instance takes a long time to bootstrap can be considered as pre-warmed instances RAM data is persisted to the root EBS volume during the hibernation You do not incur instance usage fees while an instance is hibernating Hibernation should be enabled when launching an instance Hibernated instances are in ‘Stopped’ state EBS volume data is persisted as in hibernate state as in stopped state RAM data is encrypted when moved to EBS Instances can be hibernated for a maximum of 60 days Hibernation is supported for specific instance types   VM Import/Export  Enables Importing Virtual Machine (VM) images in order to create Amazon EC2 instances You can export EC2 instances as well to create Virtual Machines Import / Export commands are not available in the management console. You have to use EC2 CLI and API    Billing  Billing starts when the instance transitions to the Running state Reserved instance is associated with a spefic region, which is fixed for the duration of the reservation\u0026rsquo;s term EC2 Fleet lets you provision compute capacity across difference instance types, AZs and On-Demand, Reserved and Spot Instances with a single API call Reserved Instance Marketplace - provides AWS customers the flexibility to sell their EC2 RIs to other businesses and organizations. Spot Instances  spare EC2 capacity can save you up to 90% off on-Demand prices AWS can interrupt them with a 2-minute notification    Platform  Amazon Time Sync Service - available on 169.254.169.123. NTP clients can be configured Cluster Compute Instances - provide similar functionality to other Amazon EC2 instances but have been specifically engineered to provide high-performance networking High Memory Cluster Instances provide customers with large amounts of memory and CPU capabilities per instance in addition to high network capabilities EC2 Compute Unit  represents CPU resource is used to provide a consistent amount of CPU capacity no matter what the actual underlying hardware    "
},
{
	"uri": "https://majdarbash.github.io/tags/encore/",
	"title": "encore",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/error/",
	"title": "error",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/event-emitter/",
	"title": "event emitter",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/event-driven/",
	"title": "event-driven",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/event-driven-architectures/",
	"title": "Event-Driven Architecture",
	"tags": [],
	"description": "",
	"content": "Structures:\n Command Bus  Command Command Handler   Event Store Event Bus  Event Event Handler    Event-Driven Systems:\n Event Notification\nWill help decouple received from the sender, however it makes it difficult to understand overall behavior of the system. Event-carried State Transfer\nDecouples and reduces load from supplier by each system keeping a copy of the data it requires in its storage. Due to the fact that data is replicated, higher availability will lead to Eventual Consistency. Event Sourcing\nApplication state is can be reproduced through replaying the Event log. Starting from customer\u0026rsquo;s input, all events are stored in the Event Store and can be replayed at later stage. In addition to Auditing, this allows easy Debugging through replaying the events and monitoring system\u0026rsquo;s behavior. Snapshots will accelerate the application state restoration process as only the Events after snapshot date will have to be replayed.\nIn this aspect we can benefit from Memory Image: application state does not need to be stored on persistent storage and can stored in-memory.\nEvent sourcing does not mean Asynchronous event handling, as you can still use it in Monolith applications. Application versioning adds extra complexity to handle the replay functionality of past Events. CQRS (Command Query Responsibility Segregation)\nSeparating writing systems from the reading systems. In this context Command Model (write) and Query Model (read) are separate components. You can scale read and write components independently and have a different representation of application data in each of the systems. However CQRS adds an extra complexity so should be used in caution where applicable.  Publish Date: 2019-06-16\n "
},
{
	"uri": "https://majdarbash.github.io/tags/filter/",
	"title": "filter",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/fixtures/",
	"title": "fixtures",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/flash-messages/",
	"title": "flash messages",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/for-loop/",
	"title": "for loop",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/form/",
	"title": "form",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/form-class/",
	"title": "form class",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/formbuilder/",
	"title": "formBuilder",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/ftp-server/",
	"title": "ftp server",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/fuzzy-text-search/",
	"title": "fuzzy text search",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/genetics-algorithm/",
	"title": "genetics algorithm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/github/",
	"title": "github",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csap/glossary/",
	"title": "Glossary",
	"tags": [],
	"description": "",
	"content": " Fault Tolerance High Availability Storage Gateway AWS Trusted Advisor  "
},
{
	"uri": "https://majdarbash.github.io/tags/google/",
	"title": "google",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/ha-arhictecture-high-availability/",
	"title": "HA Arhictecture: High Availability",
	"tags": [],
	"description": "",
	"content": "ELB, ALB (Elastic Load Balancer, Application Load Balancer)  a physical/virtual device designed to balance network load across multiple web servers at least 2 public Subnets are required when provisioning a load balancer ELBs do not have a pre-defined IPv4 addresses; you resolve to them using a DNS name types of load balancers  Application Load Balancers  Best suited for HTTP / HTTPS traffic Operates at Layer 7 Application-aware Advanced request routing, sending specified requests to specific web servers Register target groups Load balancing rules are supported   Network Load Balancers  Load balancing TCP traffic Operates at Layer 4 Capable of handling millions of requests per seconds with ultra-low latencies   Classic Load Balancers  Legacy / previous generation load balancers HTTP / HTTPS / TCP traffic Operates at layer 4, supports Layer-7 specific features, such as X-Forwarded and sticky sessions Errors: 504 - Gateway timeout - ec2 instances are not responding Registers instances     Health Checks  Checks the instance health by talking to it Instances are reported as InService, OutofService LBs have their own DNS name. You are never given an IP address Connection Draining - the number of seconds to allow traffic to be flowing (300 seconds by default)    Auto-Scaling  Auto-Scaling Group  subnets should be assigned - instances will be distributed on these subnets Scaling Policy  Target tracking scaling policy Simple scaling policy Scaling policy with steps   Scaling warm-up time\nthe time required to warm up an instance - during this time instance won\u0026rsquo;t contribute to the metrics When deleting the auto-scaling group, the instances beneath it will be deleted as well.   Launch Configuration  IP Address Type  Default public IP assignment (as per subnet) Assign public IP to every instance Do not assign a public IP to an instance      HA Architecture  You should plan for failure Netflix (Simian Army Projects)\nInjecting failure into production systems\n(https://medium.com/netflix-techblog/the-netflix-simian-army-16e57fbab116) More than one AZ should be used (2 AZs at least) Always Design for failure Use Multiple AZ\u0026rsquo;s and Multiple Regions wherever you can In RSD Read Replica - creates a replica of AWS for performance purposes and not for HA Multi-AZ configuration means replication across different AZs supporting seamless failover Scaling out - adding ec2 instances using ASGs Scaling up - increasing resources of ec2 instances (modifying instance type) S3 storage classes  Highly Available  Standard S3 Standard S3 Infrequently access   Non-HA  Reduced redundancy storage S3 single AZ      HA Wordpress Site  2 x Cloud Front distributions  Cloud Front distribution for static media content Cloud Front distribution for the main site   2 x S3 Buckets  S3 Bucket for media S3 Bucket for code synchronization   RDS MySQL/Aurora with Mutli-AZ enabled Writer Node  LAMP installed CRON commands  Sync/var/www/html/wp-content/uploads to media bucket\naws s3 sync \u0026ndash;delete /var/www/html/wp-content/uploads s3://bucket-for-media-name/ Sync /var/www/html to code bucket\naws s3 sync \u0026ndash;delete /var/www/html s3://bucket-for-code-name/     Reader Node(s)  LAMP installed Scales in 3 AZs using ASG, min size is 2 instances Traffic received by Cloud Front distribution for the main site  Route 53 domain record points at Cloud Front distrbution ALB is defined as an origin ALB forwards traffic to Target Groups   On user-data instances will pull the code content from S3 bucket-for-code Periodically instances will run sync commands to get any updates done to the code from the Writer Node CRON command  aws s3 sync \u0026ndash;delete s3://bucket-for-code-name/ /var/www/html   .htaccess rewrite rule is added to serve the wp-content/uploads files from Cloud Front distribution serving the static media bucket from bucket-for-media-name   Simulating failure  Terminate an EC2 instance RDS, Multi-az  You may reboot with failover to simulate the failure This may still take your site offline for a couple of minutes      Cloud Formation  Is a way of completely scripting your cloud environment From AWS website:  \u0026ldquo;AWS CloudFormation provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This file serves as the single source of truth for your cloud environment.\u0026rdquo;   https://aws.amazon.com/quickstart/  Built by solution architects and partners Cloud Formation templates to help you build your production or test environment quickly and starting using it immediately    "
},
{
	"uri": "https://majdarbash.github.io/random/accessing-multiple-github-accounts-from-one-computer/",
	"title": "How to accessing multiple Git accounts from the same computer using different private keys?",
	"tags": ["github", "account"],
	"description": "",
	"content": "In order to access multiple github accounts from one computer you need to :\n generate ssh keys add ssh keys to your github account create a configuration file to manage the separate keys update stored identities  Step 1 - Generating SSH keys:\ncd ~/.ssh ssh-keygen -t rsa -C \u0026quot;[email associated with your github account #1]\u0026quot; # save it as id_rsa_git_account1 when prompted ssh-keygen -t rsa -C \u0026quot;[email associated with your github account #2]\u0026quot; # save it as id_rsa_git_account2 when prompted As the result of executing these commands, the following files will be generated:\n id_rsa_git_account1 id_rsa_git_account1.pub id_rsa_git_account2 id_rsa_git_account2.pub  Step 2 - Adding ssh keys to your github account\nUsing this command you can copy the key to your clipboard:\npbcopy \u0026lt; ~/.ssh/id_rsa_git_account1.pub Now as the key is in your account, all you have to do is to open your github account and:\n Click on Account Settings Click \u0026ldquo;SSH Keys\u0026rdquo; -\u0026gt; \u0026ldquo;Add SSH Key\u0026rdquo; Paste your key into key field and add a relevant title Click \u0026ldquo;Add key\u0026rdquo; then enter your Github password to confirm.  Step 3 - Create a configuration file to manage the separate keys\n Navigate to ~/.ssh/ touch config Enter the configuration to the config file:  # githubAaccount1 Host account1 HostName github.com User git IdentityFile ~/.ssh/id_rsa_git_account1# github Account2 Host account2 HostName github.com User git IdentityFile ~/.ssh/id_rsa_git_account2 Step 4 - Update Stored Identities:\nssh-add -D Add new keys:\nssh-add id_rsa_git_account1 ssh-add id_rsa_git_account2 Test to make sure new keys are stored\nssh-add -l Testing Push / Pull\ntouch readme.md $ git ini $ git add . $ git commit -am \u0026quot;first commit\u0026quot; $ git remote add origin git@account1:githubPersonal/test-personal.git $ git push origin master  Publish Date: 2015-04-08\n "
},
{
	"uri": "https://majdarbash.github.io/random/backing-mysql-database/",
	"title": "How to Backup MySQL database using Percona&#39;s innobackupex",
	"tags": [],
	"description": "",
	"content": "Backup using innobackupex This script backups your database and uploads it to Amazon S3:\n#!/bin/bash\ntimestamp=`date \u0026ldquo;+%Y-%m-%d_%H-%M-%S\u0026rdquo;`\ninnobackupex \u0026ndash;user=[username] \u0026ndash;password=[password] \u0026ndash;parallel=3 \u0026ndash;safe-slave-backup \u0026ndash;slave-info \u0026ndash;no-timestamp /data/backup/${timestamp}_slave innobackupex \u0026ndash;user=[username] \u0026ndash;password=[password] \u0026ndash;apply-log /data/backup/${timestamp}_slave/\ntar czf /data/backup/${timestamp}_slave.tgz /data/backup/${timestamp}_slave\n/usr/local/bin/aws \u0026ndash;region [region] s3 cp /data/backup/${timestamp}_slave.tgz s3://[bucket]/[backup-path]/`date \u0026ldquo;+%Y\u0026rdquo;`/`date \u0026ldquo;+%m\u0026rdquo;`/`date \u0026ldquo;+%d\u0026rdquo;`/\nrm -rf /data/backup/${timestamp}_slave rm -f /data/backup/${timestamp}_slave.tgz\nYou can easily write this script in backup.sh and schedule a cronjob using crontab -e for performing this backup on the daily basis\nIn order to restore the backup downloaded you have to:\n# extracting backup contents to BACKUP-DIR tar -xzvf /data/backup/${timestamp}_slave.tgz innobackupex \u0026ndash;copy-back /path/to/BACKUP-DIR\ninnobackupex: Finished copying back files. 111225 01:08:13 innobackupex: completed OK! You may need to change ownership of files from: chown -R mysql:mysql /var/lib/mysql\nDownloading innobackupex on Mac:\ncd /tmp curl -O https://www.percona.com/downloads/XtraBackup/XtraBackup-0.8/MacOS/xtrabackup-0.8-macos.tar.gz tar -xzf xtrabackup-*-macos.x86_64.tar.gz cd xtrabackup*\nchecking the files downloaded ls cp * /usr/local/bin\nNow you can call the backup perl script, using innobackupex-1.5.1 command\n# will restore the backup to MySQL datadir (determined by value is mysql my.cnf) innobackupex \u0026ndash;copy-back [path/to/Backup-dir]\nchange the ownership in data-dir to make sure it\u0026rsquo;s correctly set up chown -R mysql:mysql /var/lib/mysql\nspecify the configuration file location innobackupex \u0026ndash;copy-back \u0026ndash;defaults-file=/etc/my.cnf \u0026ndash;[path/to/backup-dir]\nBackup using mysqldump # dumping the database mysqldump -u[user] -p[password] -h[hostname] [database] \u0026gt; /tmp/backup.sql\nrestoring the database mysql -u[user] -p[password] -h[hostname] [database] \u0026lt; /tmp/backup.sql\n"
},
{
	"uri": "https://majdarbash.github.io/random/find-backlinks-for-your-site/",
	"title": "How to find the backlinks to your site which are indexed by Google?",
	"tags": ["seo", "google", "backlinks"],
	"description": "",
	"content": "I was trying to find backlinks for my site on google search. The best way to do this was to do a search, using the following parameters:\n[your Keywords or your Sitename] -site:yoursitename.com\nBasically this will located all the words which match the search terms, excluding your own site -\u0026gt; which are actually the backlinks to your site.\nPublish Date: 2014-10-19\n "
},
{
	"uri": "https://majdarbash.github.io/random/levenshtein-distance-in-mysql-measuring-distance-between-the-strings/",
	"title": "How to measure distance between strings? What is Levenstein Distance?",
	"tags": ["mysql", "database", "levenshtein distance", "fuzzy text search"],
	"description": "",
	"content": "Levenshtein distance function can be used to measure distance between the strings. Smaller distance will mean that string 1 is closer to string 2. This function can be used to implement \u0026ldquo;fuzzy\u0026rdquo; text search in mySQL where you can search for records and order by levenshtein distance in ascending order. In this case the closest matches (more relevant results) will appear on top and then the less relevant ones.\nDELIMITER ;; DROP FUNCTION IF EXISTS `levenshtein`;; CREATE FUNCTION `levenshtein`(s1 VARCHAR(255), s2 VARCHAR(255)) RETURNS INT(11) DETERMINISTIC BEGIN DECLARE s1_len, s2_len, i, j, c, c_temp, cost INT; DECLARE s1_char CHAR; DECLARE cv0, cv1 VARBINARY(256); SET s1_len = CHAR_LENGTH(s1), s2_len = CHAR_LENGTH(s2), cv1 = 0x00, j = 1, i = 1, c = 0; IF s1 = s2 THEN RETURN 0; ELSEIF s1_len = 0 THEN RETURN s2_len; ELSEIF s2_len = 0 THEN RETURN s1_len; ELSE WHILE j \u0026lt;= s2_len DO SET cv1 = CONCAT(cv1, UNHEX(HEX(j))), j = j + 1; END WHILE; WHILE i \u0026lt;= s1_len DO SET s1_char = SUBSTRING(s1, i, 1), c = i, cv0 = UNHEX(HEX(i)), j = 1; WHILE j \u0026lt;= s2_len DO SET c = c + 1; IF s1_char = SUBSTRING(s2, j, 1) THEN SET cost = 0; ELSE SET cost = 1; END IF; SET c_temp = CONV(HEX(SUBSTRING(cv1, j, 1)), 16, 10) + cost; IF c c_temp THEN SET c = c_temp; END IF; SET c_temp = CONV(HEX(SUBSTRING(cv1, j + 1, 1)), 16, 10) + 1; IF c c_temp THEN SET c = c_temp; END IF; SET cv0 = CONCAT(cv0, UNHEX(HEX(c))), j = j + 1; END WHILE; SET cv1 = cv0, i = i + 1; END WHILE; END IF; RETURN c; END;; DELIMITER ;  Publish Date: 2014-09-12\n "
},
{
	"uri": "https://majdarbash.github.io/random/apache-configuration-logging-traffic-from-x-forwarded-for-header/",
	"title": "How to properly log ClientIP on Apache2 behind Load Balancer?",
	"tags": ["apache2", "log", "X-Forwarded-For"],
	"description": "",
	"content": "This case will help you to log to apache server any traffic coming from the load balancer, proxy or any IDS.\nLogFormat \u0026quot;%h %l %u %t \u0026quot;%r\u0026quot; %\u0026gt;s %b \u0026quot;%{Referer}i\u0026quot; \u0026quot;%{User-Agent}i\u0026quot;\u0026quot; combined LogFormat \u0026quot;%{X-Forwarded-For}i %l %u %t \u0026quot;%r\u0026quot; %\u0026gt;s %b \u0026quot;%{Referer}i\u0026quot; \u0026quot;%{User-Agent}i\u0026quot;\u0026quot; proxy SetEnvIf X-Forwarded-For \u0026quot;^.*..*..*..*\u0026quot; forwarded CustomLog ${APACHE_LOG_DIR}/access_log combined env=!forwarded CustomLog ${APACHE_LOG_DIR}/access_log proxy env=forwarded Where are Apache2 Logs? The default location of apache log files on ubuntu server apache2 installation is:\n/var/www/apache2/access.log in order to view the log file contents you can use any of the following commands:\ndisplays the whole contents of the log file\ncat /var/www/apache2/access.log shows the contents from the end of access.log file and wait to display any additional logs appended to the end of the file\ntail -f /var/www/apache2/access.log displays the last 100 mb of the apache2 log file\ntail -c 104857600 /var/log/apache2/access.log If you would like to extract the last 100 mb of the log file to some other file, you can achieve this by executing the command:\ntail -c 204857600 /var/log/apache2/access_sacoffice_log \u0026gt; log.log After this, you can download the log file and check the logs ! :)\nPublish Date: 2014-11-18\n "
},
{
	"uri": "https://majdarbash.github.io/random/password-protect-directory-apache-configurations/",
	"title": "How to protect directory with a password in Apache2?",
	"tags": ["ubuntu server", "web server", "apache", "password protect", "security"],
	"description": "",
	"content": "A usual scenario encountered by every web server is administrator is to password protect specific directory and all it\u0026rsquo;s resources. We need to add some lines in the configuration file where our web server / virtual host is defined. This is how we do it:\nCreating the password file: (the system will ask you to enter the password which will be saved in encrypted format)\nsudo htpasswd -c [filename] [username] Apache BASIC HTTP Authentication using the created password file\nAuthUserFile [the path to the password file] AuthType Basic AuthName \u0026quot;RESTRICTED ACCESS\u0026quot; Require valid-user  Publish Date: 2014-10-27\n "
},
{
	"uri": "https://majdarbash.github.io/random/running-apache-2-proxy-mod_proxy/",
	"title": "How to run Apache2 as proxy using modproxy module?",
	"tags": [],
	"description": "",
	"content": "One other way to fix CORS issue and prevent your API endpoints to be exposed is to configure Apache2 web server to proxy certain requests, thus making it serve the requests to different domain through the apache installation of the current domain. The other application I can think about would be to hide the web service endpoint.\nIn order to make apache act as a \u0026ldquo;proxy\u0026rdquo; you need to make sure that proxy_http extension is there. In any case, please execute the commands below to install apache proxy module, assuming you are running your apache2 installation on Ubuntu Server Edition.\nsudo a2enmod proxy sudo service apache2 restart a2enmod proxy_http sudo a2enmod proxy_http sudo service apache2 restart\nAfter enabling the required proxy_http module, you need to add the following line to the .htaccess file:\nRewriteRule /path/to/request/to/be/routed/(.*) http://remote/server/mapped/url/$1 [P]\n"
},
{
	"uri": "https://majdarbash.github.io/random/setting-squid-proxy-on-ubuntu-server/",
	"title": "How to Setup Squid proxy on Ubuntu Server?",
	"tags": ["ubuntu", "squid", "proxy"],
	"description": "",
	"content": "In order to install a proxy service on Ubuntu Server edition, please execute the following commands:\nInstalling Squid:\nsudo aptitude -y install squid3\nEditing the configuration file:\nsudo vim /etc/squid3/squid.conf // add a line in order to enable global access: http_access allow all\nRestart the squid to reflect the newly changed configuration settings\ninitctl restart squid3\nAs the squid is running now, you can access the proxy by setting the default port as 3128 and using the host ip address.\nIn order to be able to access squid make sure to open the port 3128 externally for the server you have installed the squid on.\n"
},
{
	"uri": "https://majdarbash.github.io/random/docker-network-conflicts/",
	"title": "How to solve Network IP conflict when running Docker?",
	"tags": [],
	"description": "",
	"content": "Sometimes docker container networks created will conflict with the network configuration you have.\nTo fix it, I\u0026rsquo;m using the following configuration on my local machine:\n# /etc/docker/daemon.json { \u0026quot;bip\u0026quot;: \u0026quot;192.168.199.1/28\u0026quot;, \u0026quot;dns\u0026quot;: [\u0026quot;8.8.8.8\u0026quot;, \u0026quot;8.8.4.4\u0026quot;] } This configuration will limit the IP addresses of the docker containers to 192.168.199.1 CIDR block. In addition you may want to troubleshoot. This may help you identify any potential conflict between your network / DNS server addresses and your existing networks / bridges created by docker.\n# list the interfaces can help you identify what networks do you have ifconfig # will display DNS servers used and other information of the used network interface nmcli device show [interface name] # investigate the routing table to make sure that the destination interfaces are correct # and none of the DNS servers used will get blocked by a network / bridge route -n # display the bridge networks using bridge administrator tool sudo apt-get install bridge-utils brctl show # delete any docker networks which may be conflicting with the current settings docker network prune I have done these steps several times and found it helpful to solve my issues.\nHope this helps !\nPublish Date: 2019-07-17\n "
},
{
	"uri": "https://majdarbash.github.io/random/transferring-wordpress-one-domain-another/",
	"title": "How to transfer a Wordpress site from one domain to another?",
	"tags": ["database", "wordpress", "migration"],
	"description": "",
	"content": "Explanation Other than uploading your wordpress sites to the new domain, you will need to do some changes in your database. Therefore, when you want to move the wordpress website to a new domain, you will find out that existing database records are still pointing at the old domain. So how do we properly migrate our wordpress site to the new domain name?\nI have seen a lot of developers going straight to the database and updating records to a new domain. Some just write update queries to change the domain. Usually they end up with corrupt wordpress site - which may not be functioning. If you are lucky the site may be still working but you may experience other issues with the website later on.\nSo what makes changing the wordpress domain so complex? And is it really such a big deal? Not really - the answer lies in serialised strings in the database. Because you cannot manually modify serialized strings, you will need a script to do this for you.\nAfter searching I found out some useful script that will help you replace all occurrences of the old domain to the new one.\nAction Points Follow these steps for proper migration of your wordpress site:\n Before starting I prefer to backup the existing database. You can do this from the control panel, or phpmyadmin / other database access interfaces - if something goes wrong you can always restore your wordpress site database. Download the script from this link:\n[download-attachment id=\u0026quot;323\u0026rdquo; title=\u0026quot;Search-Replace-DB-master.zip\u0026rdquo;] Extract the contents to where you have uploaded your website. Access the url which will lead you to installed script location Carefully choose the old site name in the strings to replace, including http:// and www in case it was used in the old wordpress configuration. For example: http://www.[oldsitename].com Submit the new site name with the full url : http://www.[newsitename].com Submit the database credentials Dry run the script to check what changes will be made after the script is running\n(You can click on specific change to see the differences which will be made in the database script) Click \u0026ldquo;Update\u0026rdquo; and confirm to update the database record Enjoy your properly migrated wordpress site.  \n"
},
{
	"uri": "https://majdarbash.github.io/random/htaccess-how-to-redirect-non-www-url-to-www/",
	"title": "htaccess : how to redirect non-www url to www",
	"tags": [],
	"description": "",
	"content": "Place the following configuration in your .htaccess file:\nRewriteEngine On RewriteCond %{HTTP_HOST} !^www\\. RewriteCond %{HTTP_HOST} !=localhost RewriteRule ^(.*)$ http://www.%{HTTP_HOST}/$1 [R=301,L]\nThis line is placed to make sure that redirection does not occur on your localhost during the development process. You can remove this line if you want to place the .htaccess configuration directly on your production server. Otherwise you can leave it and change \u0026ldquo;localhost\u0026rdquo; to any other local web server you are running.\nRewriteCond %{HTTP_HOST} != localhost\nRedirects from .htaccess file # permanent redirect (301) Redirect 301 /path new_Url\ntemporary redirect (302) Redirect 302 /path new_Url\n"
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/iam/",
	"title": "IAM",
	"tags": [],
	"description": "",
	"content": "IAM (Identity Access Management)\n Centralized control of your AWS account Shared Access to your AWS account Granular Permissions Identity Federation (including Active Directory, Facebook, etc\u0026hellip;) Multifactor Authentication Temporary access to users/devices and services Password rotation policy PCI DSS Compliance  Models:\n Users Groups Policies Roles  AWS provides IAM Pre-defined Roles per Job Function, otherwise custom roles and policies can be defined.\n IAM is universal. It does not apply to regions. The root account is the account created when first setting up AWS account - It has complete Admin access. Users have no permissions by default. New Users are assigned Access Key ID \u0026amp; Secret Access Keys when programmatic access is enabled. Access Key Id and Secret Access Key will be displayed once. If lost, they will have to be regenerated. MFA should be set up on the root account.  "
},
{
	"uri": "https://majdarbash.github.io/software-architecture-and-design/important-concepts-every-dev-should-know/",
	"title": "Important Concepts Every Developer should know",
	"tags": ["software development", "design patterns", "programming"],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/quickbooks-php-integration/",
	"title": "Initial steps towards successful integration Quickbooks &amp; PHP?",
	"tags": [],
	"description": "",
	"content": "Integration: PHP and Quickbooks using v3-php-sdk-2.0.5\nThis was a bit of headache. In this article I explain how to configure your quickbooks php sdk, issues faced, fixes until successful integration.\nSome useful references to accomplish php / quickbook integration using the php sdk.\n- download php sdk\nhttp://bit.ly/IppPhpSdkV3\n- PHP SDK for QuickBooks V3 documentation and getting started\nhttps://developer.intuit.com/docs/0100_accounting/0500_developer_kits/0210_ipp_php_sdk_for_quickbooks_v3\n*** Trying to Integrate\nAfter download the v3-php-sdk-2.0.5 in order to accomplish the quickbooks integration, i have faced some serious problems to be able to integrate. After the installation I have set some settings in App.Config and directly accessed _Samples folder and decided to run the safest CustomerQuery.php - assuming it doesn\u0026rsquo;t cause any modification to our existing records :).\nThe first issue encountered was this:\nFatal error: Uncaught exception \u0026lsquo;UnexpectedValueException\u0026rsquo; with message \u0026lsquo;DirectoryIterator::__construct(/var/folders/bn/gqvb2w4x7n3573t34xd1sdn00000gq/T): failed to open dir: Permission denied\u0026rsquo; in /var/www/test/v3-php-sdk-2.0.5/Core/LogRequestsToDisk.php:65 Stack trace: #0 /var/www/test/v3-php-sdk-2.0.5/Core/LogRequestsToDisk.php(65): DirectoryIterator-\u0026gt;__construct('/var/folders/bn\u0026hellip;') #1 /Applications/XAMPP/xamppfiles/htdocs/test/v3-php-sdk-2.0.5/Core/RestCalls/SyncRestHandler.php(227): LogRequestsToDisk-\u0026gt;LogPlatformRequests(\u0026lsquo;SELECT * FROM C\u0026hellip;', \u0026lsquo;https://sandbox\u0026hellip;', Array, true) #2 /var/www/test/v3-php-sdk-2.0.5/DataService/DataService.php(572): SyncRestHandler-\u0026gt;GetResponse(Object(RequestParameters), \u0026lsquo;SELECT * FROM C\u0026hellip;', NULL) #3 /var/www/test/v3-php-sdk-2.0.5/_Samples/CustomerQuery.php(37): DataService-\u0026gt;Query(\u0026lsquo;SELECT * FROM C\u0026hellip;') #4 {main} thrown in /var/www/test/v3-php-sdk-2.0.5/Core/LogRequestsToDisk.php on line 65\nAfter tracing the code i found out that the situation is caused by the following setting in sdk.config:\nSo basically I thought, let\u0026rsquo;s turn the logging off. The result was still the same after mofifying the configuration:\nAfter tracing the code I discovered that a fix should be applied to overcome this problem.\n*** Applied FIX:\n/Core/LogRequestsToDisk.php / line 57:\nif ($this-\u0026gt;EnableServiceRequestsLogging)\nreplaced by\nif ($this-\u0026gt;EnableServiceRequestsLogging == \u0026lsquo;true\u0026rsquo;)\nDescription:\nEnableServiceRequestsLogging was interpreted as string when using false value and on casting to bool the string word \u0026lsquo;false\u0026rsquo; was considered to be true: (bool)\u0026lsquo;false\u0026rsquo; \u0026ndash;\u0026gt; true\nAfter applying this fix we are ready to go and start setting up our configuration and the environment.\n*** Quickbooks developer account - Creating an App\nThe first thing you want to do is to create an app on quickbooks developer account by connecting to which you will be integrating with your quickbooks online account. In order to achieve this, access:\nhttps://developer.intuit.com\nFrom the top right menu, click \u0026ldquo;My Apps\u0026rdquo; and then create a new App.\nFor this tutorial choose the \u0026ldquo;Just start coding\u0026rdquo; option on the right and then check \u0026ldquo;Accounting\u0026rdquo; out of Quickbooks API. So far you have an app and some keys for development. Now you need to click on the \u0026ldquo;Keys\u0026rdquo; tab and you will see the App Token , OAuth Consumer Key and OAuth Consumer Secret.\nBasically the idea here is to generate AccessToken and AccessTokenSecret using the existing (App Token , OAuth Consumer Key and OAuth Consumer Secret). This will mean that your application with (App Token) will be granted access to your account data and services and will be authenticated by AccessToken and AccessTokenSecret, that you will use to connect from your application to the SDK.\nIntegration scenario\nPHP CODE + SDK \u0026ndash;(connected to)\u0026ndash;\u0026gt; Developer Application \u0026ndash;(connected to)\u0026ndash;\u0026gt; Quickbooks APIs\n*** Getting authentication tokens\nIn order to generate access token for your app to your whole account, you need to do this using the url:\nhttps://appcenter.intuit.com/Playground/OAuth\nfollowing step-by step instructions, starting from the already defined App Token , OAuth Consumer Key and OAuth Consumer Secret, we will continue until we get AccessToken and AccessTokenSecret. After this point we have all the authentication tokens we need in order to configure our SDK.\n*** Configuration\nwe need 2 configuration files:\nApp.Config - located at the path of the index file you are working on\nsdk.config - located at the root path of the SDK\nApp.Config should containg the settings:\n- RealmID (your company id, can include the sandbox company id)\n- AccessToken\n- AccessTokenSecret\n- ConsumerKey\n- ConsumerSecret\nsdk.config should be adjusted:\nAs we are integrating with quickbooks, we are concerned about \u0026ldquo;qbo\u0026rdquo; attribute of \u0026ldquo;baseUrl\u0026rdquo;.\nIn case we are on development environment and would like to use the actual company keys, we would need to use the default url:\nhttps://quickbooks.api.intuit.com/\nIn case we are connecting to the sandbox company this url has to be changed to:\nhttps://sandbox-quickbooks.api.intuit.com/\nSample App.Config file:\nSample sdk.config file:\nIf you have any ideas or suggestions or other experiences about PHP - Quickbooks SDK integration please comment below. :)\nHappy integration !\n"
},
{
	"uri": "https://majdarbash.github.io/tags/installation/",
	"title": "installation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/ubuntu-vsftpd-installation/",
	"title": "Installing VsFTPd on Ubuntu Server",
	"tags": ["ubuntu", "vsftpd", "ftp server", "ubuntu server"],
	"description": "",
	"content": "A small tutorial on how to install vsftpd and setup a separate user.\nPlease follow these steps to install vsftpd service on your ubuntu machine.\n# adding a new local user useradd -g ubuntu ftpuser # change the password of the new user passwd ftpuser # checking the user information id ftpuser # checking the user group groups ftpuser # changing the group of the user usermod -g ubuntu ftpuser # installing and configuring vsftpd sudo apt-get install vsftpd sudo vim /etc/vsftpd.conf # anonymous_enable=NO # local_enable=YES # write_enable=YES # chroot_local_user=NO§ # to display the hidden files using the ftp client: # hide_file=none # force_dot_files=YES # local_umask=000 sudo service vsftpd restart # pam_service_name=ftp # Enabling passive ports: # pasv_enable=YES # pasv_min_port=64000 # pasv_max_port=64321 # port_enable=YES # pasv_address=[your public ip] # pasv_addr_resolve=YES after this, make sure that the port range 64000 - 64321 is enabled in your amazon ec2 security groups or iptables\n# configuring ftp user home directory: sudo mkdir /home/ftpuser chown ftpuser:ubuntu /home/ftpuser  Connect using filezilla Use active ftp connection  Publish Date: 2014-09-10\n "
},
{
	"uri": "https://majdarbash.github.io/random/hybrid-app-development-testing-on-google-chrome/",
	"title": "Ionic app testing - Cross domain issues",
	"tags": ["ionicframework", "browser", "testing", "crossdomain"],
	"description": "",
	"content": "I have encountered this issue several times while I was trying to run my hybrid mobile app on google chrome browser. In particular, an app based on ionic framework was used. Basically, I was building the HTML part of the hybrid app, using cordova and phonegap. The app is accessing the web service using jquery calls: $.ajax. The issue was that in order to test the app on my google chrome browser, I had to bypass the cross domain request restriction. I tried several extensions, however the ultimate way way just to run google chrome, while disabling the security switch. For whoever needs it:\n/Applications/Google Chrome.app/Contents/MacOS/Google Chrome --disable-web-security The same can be applied to the safari browser, using:\n/Applications/Safari.app/Contents/MacOS/Safari --disable-web-security  Publish Date: 2014-10-14\n "
},
{
	"uri": "https://majdarbash.github.io/tags/ionicframework/",
	"title": "ionicframework",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/js/",
	"title": "js",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/installing-apache2-mysql-php-on-ubuntu/",
	"title": "LAMP Installation on Ubuntu Server",
	"tags": ["ubuntu", "mysql", "apache2", "php", "installation"],
	"description": "",
	"content": "Found it useful to include these command I\u0026rsquo;m always using whenever I need to create an ec2 instance:\nsudo apt-get update sudo apt-get install apache2 sudo apt-get install libapache2-mod-php5 sudo apt-get install mysql-server libapache2-mod-auth-mysql php5-mysql # installing php / curl sudo apt-get install php5-curl other php modules sudo apt-get update sudo apt-cache search php5  Publish Date: 2014-10-04\n "
},
{
	"uri": "https://majdarbash.github.io/tags/levenshtein-distance/",
	"title": "levenshtein distance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/log/",
	"title": "log",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/machine-learning/",
	"title": "machine learning",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/migration/",
	"title": "migration",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/multi-tenant-saas-storage/",
	"title": "Multi-Tenant SaaS Storage Strategies",
	"tags": [],
	"description": "",
	"content": " SaaS Paritioning Models  Silo Model Pool Model Bridge Model   Migration and Multitenancy Security Considerations Management and Monitoring Tiered Storage Models Developer Experience Linked Account Silo Model Multitenancy on DynamoDB  Silo Model Bridge Model Pool Model   Mutlitenancy on RDS  Silo Model Bridge Model Pool Model Single Instance Limits   Mutlitenancy on Amazon Redshift  Silo Model Bridge Model Pool Model   Agility  The goal is to find the best intersection of your storage and tenant partitioning needs. Consider how the strategy impacts your ability to build, deliver and deploy versions in zero downtime environment. Assess the regulatory, business and legacy dimensions of a given environment.\nSaaS Paritioning Models  Silo  Separate database for each tenant Addresses concerns on sharing infrastructure with other tenants Great for migration from existing solution to multi-tenant solution   Bridge  Single database, multiple schemas   Pool  Shared database, single schema Requires introduction of partitioning key to scope and control access to tenant data Fits with continuous delivery and agility goals that are essential to Saas providers     Silo Model Pros\n Compliance alignment No cross-tenant impacts Tenant-level tuning Tenant-level availability  Cons\n Compromised agility Centralized management Deployment complexity  Automating creation and configuring database on per-tenant basis adds a layer of complexity and a potential point of failure in your SaaS environment.   Cost  Pool Model Pros\n Agility Cost optimization Centralized management Simplified deployment  Cons\n Cross-tenant impacts Compliance challenges All or nothing availability  Bridge Model  Hybrid model combining pros and cons of both Silo and Pool model extremes.  Hybrid Silo/Pool Storage\n One possible solution is to build a solution that fully supports pooled storage as your foundation. Then you can carve out a separate database for those tenants that demand a siloed storage solution.   Migration and Multitenancy  Minimize invasive changes Favor data changes which have backward compatibility with earlier changes  Silo/Bridge Models\n Data can be migrated on tenant-by-tenant basis Allows careful migration of each SaaS tenant without exposing all tenants to possibility of migration error Introduces complexity into overall orchestration of your deployment lifecycle  Pool Model\n Easier migration process, all tenants are migrated at once Any migration error would impact all tenants  Security Considerations  Robust security strategy to ensure that tenant data is effectively protected from unauthorized access Adopting common security patterns supported by AWS  encrypt data at rest utilize IAM policies to limit access to tenant data  works great with Silo and Bridge model to limit database access in Pool model responsibility shifts to authorization models of your application\u0026rsquo;s services     Research on how isolation is achieved on each of the used AWS Services  Management and Monitoring  Building effective metrics and dashboard for aggregating storage trends  With siloed storage, data should be collected from each isolated database and aggregated in an aggregate model   Tenant-centric Views of Activity  represents the ability to drill down into tenant-centric storage activity Silo models align more naturaly with constructing this view Pool models will require some tenant-filtering mechanism   Policies and Alarms  More moving parts on a tenant-by-tenant basis will affect the complexity and manageability of your storage monitoring strategy Overall goal of the policies to set proactive rules to anticipate and react to health events    Tiered Storage Models  It\u0026rsquo;s not uncommon to find a spectrum of different storage solutions in use across the set of microservices that make up your application Storage can be used as another way to tier the SaaS solution Each tier can leverage a separate storage strategy, offering varying levels of performance and SLAs  Developer Experience  Developers typically introduce layers of frameworks that centralize and abstract away horizontal aspects of their applications Centralize and standardize policies and tenant resolution strategies Data access layer would inject tenant context into data access requests  Linked Account Silo Model  Need to provision separate Linked Account for each tenant Entire infrastructure of a tenant is isolated from other tenants Linked approach relies on Consolidated Billing More complex provisioning process Automate creation of each Linked Account and adjust any limits as needed AWS has constraints on the number of Linked Accounts - won\u0026rsquo;t be a good strategy for creating a large number of SaaS tenants  Multitenancy on DynamoDB  Schema-less nature of DDB makes migration easy  Silo Model  No notion of database instance, all tables are created globally in the region Requires grouping tables belonging to a single tenante, e.g. prefix by tenant identifier Access to the tables is controlled through IAM policies Provisioning process should automate generation of tables and IAM policies Tuning can be done on tenant-by-tenant basis  RCU and WCU, set on table level Amazon CloudWatch Metrics that are captured on table level   Number of tables can drastically grow in DDB with each microservice introducing new set of tables for each particular tenant Another approach to be considered is to have a single table for all data per tenant  Simplifies provisioning, management and migration profile of your solution     Bridge Model  Relaxing some isolation requirements through eliminating the introduction of any table-level IAM policies Removing IAM policies could simplify your provisioning scheme  Pool Model  For evenly distributed data across tenant performance optimization can be achieved by simply relying on underlying partioning scheme For SaaS environments which don\u0026rsquo;t have uniform multi-tenant data distribution you need to introduce a mechanism to better control the distribution of your data  One way would be to introduce shards per tenant and make it a parition key Gives us control on how much data a shard should contain and make the distribution of data uniform across partitions Tenants with large data footprint will be given more shards Introduces level of indirection that has to be addressed in data access layers (tenant-shard resolution)    Introducing a tenant lookup table:  Mutlitenancy on RDS Silo Model  Creating separate instances for each tenant Typically satisfies the compliance needs of customers without the overhead of provisioning entirely separate account   Bridge Model  Leverage a single instance for all tenants Create separate representation for each tenant Requires provisioning and runtime resolution for each tenant Requires adopting policies to limit schema changes Some RDS containers limit the number of database/schemas that you can create for an instance   Pool Model  Tenant data is stored in a single RDS instance Tenants share custom tables Tenant identifier is used to access each tenant\u0026rsquo;s data   Single Instance Limits  Storage Amount Limits  MySQL, MariaDB, Oracle, PostgreSQL - 6TB SQL Server - 4TB Aurora - 64TB   Consider sharding tenants data and distributing accross multiple instances  Mutlitenancy on Amazon Redshift  Focuses on building high-performance clusters to house large-scale data warehouses Places some limits on the constructs that you can create for each cluster  60 databases per cluster, 250 schemas per db, etc\u0026hellip;    Silo Model  Requires provisioning separate clusters for each tenant Access can be controlled and restricted using IAM policies and database priveleges Ability to create tuned experience per tenant Per tenant provisioning process adds extra complexity to your deployment footprint  Bridge Model  Create separate schemas for each tenant You will run into 256 limit with Redshift Redshift security grants all access to databases inside the cluster  SaaS application will be responsible for enforcing finer-grained access controls   The isolation profile of this solution is likely unacceptbale by customers  Pool Model  All tenants share databases and tables Overall management, monitoring and agility are improved by using a single Redshift Cluster Upper limit of 500 concurrent connections can be a bottleneck  SaaS developer defines an effective strategy to manage the connection, e.g. implementing client-based caching    Agility Storage technology and isolation model directly impacts your ability to easily deploy new features. Underlying storage model must accomodate the required changes without requiring downtime. The storage model picked today might not be a good fit for tomorrow.\n"
},
{
	"uri": "https://majdarbash.github.io/tags/mysql/",
	"title": "mysql",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/neural-networks/",
	"title": "neural networks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/nodejs/",
	"title": "nodejs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/node-js-npm-package-manager/",
	"title": "NodeJS 8: NPM - Package Manager",
	"tags": [],
	"description": "",
	"content": "NPM - Node Package Manager : npm makes it easy for JavaScript developers to share and reuse code, and it makes it easy to update the code that you\u0026rsquo;re sharing.\nThe official documention is available at : http://docs.npmjs.com\nhttps://www.npmjs.org - centralized repository of public modules\nNPM modes of operation global / local - Node modules can be installed locally or globally. Global installation of modules makes them available throughout any NodeJS Application you are writing on the same system. Local installation includes the module files into the same working directory and makes them available to your application (in the working directory). Default mode of operation is local -\u0026gt; doesn\u0026rsquo;t make system wide changes - always use the default local mode if you are in a doubt\nNPM - The Global Mode All packages will be installed into /usr/local/lib/node_modules, uses –g flag for Global mode. For example:\nnpm install -g sax\nin any script file, you can then use sax module by using var sax = require(\u0026lsquo;sax\u0026rsquo;);\nNPM - The Local Mode default mode, downloads the modules into node_modules directory of your current working directory. For example:\nnpm install sax\nNPM - Installing specific version of the module npm install sax@0.2.5 // will install a 0.2.5 version npm install sax@0.2.x // installs the latest release of the 0.2 branch npm install sax@\u0026ldquo;\u0026lt;0.3\u0026rdquo; // latest version before 0.3 npm install sax@\u0026ldquo;\u0026gt;=0.1.0 \u0026lt;0.3.1\u0026rdquo; // more complicated requirements\nNPM - Uninstalling a Module npm uninstall // removes a module locally npm -g uninstall // removes a module globally\nNPM - Updating a Module npm update // updates an installed module, if the package was not installed the command will install it npm update -g // update a globally installed module\nPackage.json Basically this can be considered as an alternative to composer.\npackage.json can be use this one to define the dependencies, which will be then installed by executing the following command in your working directory:\nnpm install\nexample of using package.json:\n{ \u0026ldquo;name\u0026rdquo; : \u0026ldquo;MyApp\u0026rdquo;, \u0026ldquo;version\u0026rdquo; : \u0026ldquo;1.0.0\u0026rdquo;, \u0026ldquo;dependencies\u0026rdquo; : { \u0026ldquo;sax\u0026rdquo; : \u0026ldquo;0.3.x\u0026rdquo;, \u0026ldquo;nano\u0026rdquo; : \u0026ldquo;*\u0026quot;, \u0026ldquo;request\u0026rdquo; : \u0026ldquo;\u0026gt;0.2.0\u0026rdquo; } }\n"
},
{
	"uri": "https://majdarbash.github.io/random/node-js-notes/",
	"title": "NodeJS 8: Overview",
	"tags": ["nodejs", "event-driven", "event emitter", "asynchronous programming", "concept"],
	"description": "",
	"content": "Introduction Node.js is an open source, cross-platform runtime environment for server-side and networking applications. Node.js is becoming more and more popular as NodeJS applications are written in JavaScript, and can be run within the Node.js runtime on any operating system.\nThe main features of Node JS area:\nEvent-driven / asynchronous programming\nInstead of retuning values for the functions and determining the program-flow, you basically define functions that are called by the system when an interesting event occurs (event callbacks).\nEvents are executed by an event loop - it\u0026rsquo;s a continuous loop which performs event detection and event triggering.\nEvent-driven programming is a programming style whereby the flow is determined by the occurrence of events. Programmers register callbacks to be used as event handlers for events they are interested in, and the system invokes these handlers when those events occur. This model of programming has some advantages over the traditional blocking paradigm where, to scale, you have to use multiple processes or threads.\nJavaScript is a powerful language, which is well suited for this style of programming, mainly because it has first-class functions and closures.\nCore Concept Defining feature of Node: Event-driven / asynchronous programming\nHere, instead of using the return value, you define functions that are called by the system when interesting event occurs (event callbacks). this is accompanied by an event loop - it\u0026rsquo;s a continuous loop which performs event detection and event triggering.\n\u0026ldquo;Event-driven programming is a programming style whereby the flow is determined by the occurrence of events. Programmers register callbacks to be used as event handlers for events they are interested in, and the system invokes these handlers when those events occur. This model of programming has some advantages over the traditional blocking paradigm where, to scale, you have to use multiple processes or threads.\nJavaScript is a powerful language, which is well suited for this style of programming, mainly because it has first-class functions and closures.\u0026rdquo;\nprints the message to the console console.log(\u0026lsquo;Hello World!');\nNPM (Node package manager) https://www.npmjs.org - centralized repository of public modules\nNPM modes of operation: global / local\nDefault - local, i.e. doesn\u0026rsquo;t make system wide changes - always use the default local mode if you are in a doubt.\nNPM - The Global Mode All packages will be installed into /usr/local/lib/node_modules, uses –g flag for Global mode.\n# insalling sax module globally sudo npm install -g sax\nNow in any script file, you can then use sax module by using var sax = require(\u0026lsquo;sax\u0026rsquo;);\nNPM - The Local Mode Default mode, downloads the modules into node_modules directory of your current working directory.\n# installing sax module using local mode sudo npm install sax\nNPM - Installing specific version of the module # install 0.2.5 version of sax module sudo npm install sax@0.2.5 # installs the latest release of the 0.2 branch sudo npm install sax@0.2.x\nlatest version before 0.3 sudo npm install sax@\u0026ldquo;\u0026lt;0.3\u0026rdquo;\n=0.1.0 \u0026lt;0.3.1\\\u0026rdquo; - more complicated requirement\u0026rdquo;}\u0026rdquo; data-sheets-userformat=\u0026rdquo;{\u0026ldquo;2\u0026rdquo;:641,\u0026ldquo;3\u0026rdquo;:{\u0026ldquo;1\u0026rdquo;:0},\u0026ldquo;10\u0026rdquo;:1,\u0026ldquo;12\u0026rdquo;:0}\u0026ldquo;\u0026gt;# you can have more complicated requirements sudo npm install sax@\u0026ldquo;\u0026gt;=0.1.0 \u0026lt;0.3.1\u0026rdquo;\nNPM - Uninstalling a Module # remove a module locally sudo npm uninstall # remove a module globally sudo npm -g uninstall\nNPM - Updating a Module # updates an installed module\nif the package was not installed the command will install it npm update # update a globally installed module \u0026ldquo;}\u0026rdquo; data-sheets-userformat=\u0026rdquo;{\u0026ldquo;2\u0026rdquo;:641,\u0026ldquo;3\u0026rdquo;:{\u0026ldquo;1\u0026rdquo;:0},\u0026ldquo;10\u0026rdquo;:1,\u0026ldquo;12\u0026rdquo;:0}\u0026ldquo;\u0026gt;npm update -g\nPackage.json You can use package.json one to define the dependencies and packages which you want to install. Later, you can use the \u0026ldquo;npm install\u0026rdquo; command which will read the contents of the package.json file and install accordingly.\nExample of package.json file:\n{ \u0026ldquo;name\u0026rdquo; : \u0026ldquo;MyApp\u0026rdquo;, \u0026ldquo;version\u0026rdquo; : \u0026ldquo;1.0.0\u0026rdquo;, \u0026ldquo;dependencies\u0026rdquo; : { \u0026ldquo;sax\u0026rdquo; : \u0026ldquo;0.3.x\u0026rdquo;, \u0026ldquo;nano\u0026rdquo; : \u0026ldquo;*\u0026quot;, \u0026ldquo;request\u0026rdquo; : \u0026ldquo;\u0026gt;0.2.0\u0026rdquo; } }\nLoading Modules You can load module by referencing: file path / name.\n# including the module in your code, which exposes it\u0026rsquo;s public API for use var module = require(\u0026lsquo;module_name\u0026rsquo;);\nExporting a Module # exports the instance type Circle function Circle(){ } module.exports = Circle;\n# exports function printA function printA(){} module.exports.printA = printA;\nLoading Core Modules # loads the core http module var http = require(\u0026lsquo;http\u0026rsquo;)\nUsing Buffers # creates a binary buffer from the utf-8 encoded string var buf = new Buffer(\u0026lsquo;Hello World!');\n# creates a binary buffer from base 64 encoded string # accepted encodings: base64, utf8, ascii var buf = new Buffer(\u0026lsquo;8b76fde713ce\u0026rsquo;, \u0026lsquo;base64\u0026rsquo;); # creating a 1024-byte buffer var buf = new Buffer(1024);\n# accessing an element within the buffer array of bytes console.log(buf[10]);\n# manipulating contents of the buffer (setting the byte at the position 12 to 125) buf[12] = 125\n# slicing a buffer - obtaing the bytes from the position 8 to 20 of the original buffer buf.slice(8, 20)\ncopying a buffer buf from bytes 5 to 10, to the buf2 starting from position byte 0 of the buf2 var buf2 = new Buffer(10); buf.copy(buf2, 0, 5, 10);\nbuf.copy(buf2, targetStart, sourceStart, sourceEnd);\nDecoding a Buffer # converting the buffer into a utf-8 encoded string var string = buf.toString();\n# converting the buffer into a base64 encoded string var b64Str = buf.toString(\u0026lsquo;base64\u0026rsquo;);\nEvent Emitter Pattern Event emitters allow programmers to subscribe to events they are interested in. Asynchronous programming uses the continuation-passing style (CPS) - where style takes as an argument an explicit continuation - a function of one argument.\nExample of callback after finishing reading the file:\nvar fs = require(\u0026lsquo;fs\u0026rsquo;); fs.readFile('/etc/passwd\u0026rsquo;, function(err, fileContent) { if (err) { throw err; } console.log(\u0026lsquo;file content\u0026rsquo;, fileContent.toString()); });\nResponse object is an event emitter here, and it can emit the data and end events:\nvar req = http.request(options, function(response) { response.on(\u0026ldquo;data\u0026rdquo;, function(data) { console.log(\u0026ldquo;some data from the response\u0026rdquo;, data); }); response.on(\u0026ldquo;end\u0026rdquo;, function() { console.log(\u0026ldquo;response ended\u0026rdquo;); }); }); req.end();\nUse CPS when you want to regain control after the requested operation completes and use the event emitter pattern when an event can happen multiple times.\nEvent Emitter API Any object that implements the event emitter pattern will implement a set of methods.\n.addListener and .on — To add an event listener to an event type\n.once - To attach an event listener to a given event type that will be called at most once\n.removeEventListener - To remove a specific event listener of a given event\n.removeAllEventListeners - To remove all event listeners of a given event type\nInheriting from Node Event Emitter Here we say that MyClass inherits from EventEmitter - i.e the method of MyClass will be able to emit events.\nutil = require(\u0026lsquo;util\u0026rsquo;); var EventEmitter = require(\u0026lsquo;events\u0026rsquo;).EventEmitter;\n// Here is the MyClass constructor: var MyClass = function() { }\nutil.inherits(MyClass, EventEmitter);\nMyClass.prototype.someMethod = function() { this.emit(\u0026ldquo;custom event\u0026rdquo;, \u0026ldquo;argument 1\u0026rdquo;, \u0026ldquo;argument 2\u0026rdquo;); };\nvar myInstance = new MyClass(); myInstance.on(\u0026lsquo;custom event\u0026rsquo;, function(str1, str2) { console.log(\u0026lsquo;got a custom event with the str1 %s and str2 %s!', str1, str2); });\n"
},
{
	"uri": "https://majdarbash.github.io/aws-csap/notes/",
	"title": "Notes",
	"tags": [],
	"description": "",
	"content": "Datastore\n Files stored in S3 can be served over Bittorrent to decrease costs File Gateway (Storage Gateway) can expose S3 bucket files in the office through NFS http://registry.opendata.aws contains publically open databases AWS Glue allows you to extract data from S3 Bucket to a Table that can be queried using AWS Athena Graph databases are best a storing complex relationship data and AWS Neptune is a graph database. While other options might be able to work, none would work as well as a true graph database and we can run such a database like SAP HANA or Neo4j on EC2. Secondary Indexes and DynamoDB Accelerator (DAX) - in-memory cache in front of DDB can accelerate DynamoDB performance Gateway Stored Volume Mode, or Volume Gateway Stored Mode as its also called, would be a way to maintain a full local copy of the data and have it replicated asynchronously to S3. Amazon ElastiCache offers a fully managed Memcached and Redis service. Although the name only suggests caching functionality, the Redis service in particular can offer a number of operations such as Pub/Sub, Sorted Sets and an In-Memory Data Store. However, Elasticache is only a key-value store and cannot therefore store relational data. A global secondary index can be used to speed up queries against non-primary key items. A local secondary index, on the other hand, must retain the partition key of the table. Hash key is another term for partition key. If you make a HEAD or GET request for the S3 key name before creating the object, S3 provides eventual consistency for read-after-write. As a result, we will get a 404 Not Found error until the upload is fully replicated. However, this replication usually only takes a few seconds and we might get the metadata after all. The ACID consistency model is Atomic, Consistent, Isolated and Durable.  Networking\n Only two components allow VPC to Internet communication using IPv6 addresses and those are \u0026ldquo;Internet Gateways\u0026rdquo; (inbound) and \u0026ldquo;Egress-Only Internet Gateways\u0026rdquo; (outbound). \u0026ldquo;NAT Instances\u0026rdquo; and \u0026ldquo;NAT Gateways\u0026rdquo; explicitly do not support IPv6 traffic and a \u0026ldquo;Direct Connection\u0026rdquo; carries data between a Data Centre and an AWS VPC, but does not travel over the Internet. You can use DHCP Option Sets to configure which DNS is issued via DHCP to instances. This can be any DNS address. So long as its reachable from the VPC, instances can use it to resolve.  Security\n By default, CloudTrail will log all regions and store them in a single S3 location. It can however be configured to only log specific regions. OAuth 2.0 provides authorization only. Service Control Policy is the best way to implement restriction on OU level for allowed regions.  ACLs and Resource-based policies apply to assets and not users or groups. Identity-based policies using the aws:RequestedRegion condition key could do the job but since we are trying to control at the OU level, an SCP would require less management and localized care. We can use a DENY with StringNotEqualsIfExists conditional against aws:ReqeustedRegion for allowed regions.   DDoS Layer 7 Attacks: The challenge with layer 7 detection is telling an attack from normal user traffic. CloudFront in conjunction with AWS WAF can be an effective way to create DDoS resilience at Layer 7. Network Load Balancers are NACLs are Layer 4 solutions, and would have no visibility of Layer 7 DDoS. CloudTrail and GuardDuty are focused on the security of the AWS account, and would not be suitable in isolation for securing at Layer 7 Further information: https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf  Business Continuity\n Via Aurora Global Database, an Aurora PostgreSQL database does support automatic failover to a secondary region. AWS does not recommend the use of RAID on EBS as it greatly affects the IOPS RAID0 offers no drive fault-tolerance. RAID1, also known as mirroring, requires 2x the required volume space. RAID5 requires 3 volumes at a minimum. Elasticache for Redis supports multi-AZ failover Recovery Point Objective will define the potential for data loss during a disaster. This can inform an expectation of manual data re-entry for BC planners. Redshift currently only supports single-AZ deployments but you can run multiple clusters in different AZs. Both spread placement groups and horizontal scaling spread risk across more resources. These are reasonable approaches if hardware failure is a concern. RAID0, sometimes known as striping, provides the highest write performance of these options because writes are distributed across disks and no parity is required.  Deployment and Operations\n CloudFormation Stack Policy should have \u0026ldquo;Allow\u0026rdquo; statement to whitelist what actions can be done Once applied, stack policy can be updated only using the CLI Continuous Deliver differs from Continuous Deployment in that Delivery still includes a manual check before release to production. A Canary Release is a way to introduce a new version of an application into production with limited exposure. AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. A Disposable Upgrade is one were a new release is deployed on new instances while instances containing the old version are terminated. AWS EKS runs the Kubernetes management platform for you on AWS across multiple AZs. Because its Kubernetes conformant, you can use third-party add-ons. Service Discovery makes it easy for containers within an ECS cluster to discover and connect with each other, using Route 53 endpoints. Task Definitions define the resource utilisation and configuration of tasks, using JSON templates. Task Scheduling allows you to run batch processing jobs run on a schedule. File Storage is not a component of ECS. Storage within ECS is handled by EBS volumes attached to the underlying EC2 instances and not by ECS itself. OpsWorks is a global service but when creating a stack you must specify a region and it will not allow you to clone to another region. AWS CodeDeploy does not provide Scaling or Provisioning of the deployment. Elastic Beanstalk, CloudFormation and OpsWorks can do this.  Cost Management\n The primary value proposition around cost for AWS is that it creates the opportunity for agility using a pay-as-you-go model. Traditional CapEx models make it difficult to quickly test new ideas. You should first get a solid understanding of current costs. It may turn out that a move to the cloud is not warranted even with financial evidence so the other activities would be waste. Bulk buys are almost always cheaper than on-demand, so RIs can be a good proxy. Managed services will be more cost-effective than just mimicking a pure on-prem server farm. Additionally, soft costs like agility or maintenance should be accounted for in the model. Tagging can be directly used for all of these purposes except Purchasing. However, indirectly, I could configure a CloudWatch event to trigger some action when a tag changes. That action might be a call to an API that places an order with a vendor. Right sizing is using the lowest cost resource that still meets the technical specifications of a specific workload. CloudWatch and Trusted Advisor are the most direct tools for this. Dedicated Hosts reserve capacity because you are paying for the whole physical server that cannot be allocated to anyone else. Dedicated Instances are available as on-demand, reserved and spot instances. Costs will most certainly increase during a migration given items like training, dual environments, lease penalties, consulting and planning. AWS calls this period the migration bubble. Regional RIs are not specific to an AZ and can be consumed across a region. Zonal RIs can be modified for use in another AZ using the console of ModifyReserveInstances API. Consolidated Billing is a feature of AWS Organizations. Once enabled and configured, you will receive a bill containing the costs and charges for all of the AWS accounts within the Organization. Although each of the individual AWS accounts are combined into a single bill, they can still be tracked individually and the cost data can be downloaded in a separate file. Using Consolidated Billing may ultimately reduce the amount you pay, as you may qualify for Volume Discounts. There is no charge for using Consolidated Billing. Consolidated Billing allows you to potentially realize lower prices on some services with tiered pricing. A buffering pattern is useful in smoothing demand. We can do this with SQS using FIFO to satisfy the in order requirement. If we solely use a spot fleet, we might be outbid and not have available instances. So, we can use a RI instead.  Areas of Focus:\n Fault Tolerance, High Availability, Disaster Recovery AWS Organizations, Security Compliance Policy AWS Support Plans AWS Trusted Advisor Direct Connect, VPN (Gateway)  "
},
{
	"uri": "https://majdarbash.github.io/tags/notice/",
	"title": "notice",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/password-protect/",
	"title": "password protect",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/elasticache-at-scale/",
	"title": "Performance at Scale with Amazon ElastiCache",
	"tags": [],
	"description": "",
	"content": " Memcached vs Redis ElastiCache for Memcached  Caching Design Patterns Consistent Caching (Sharding) Lazy Caching Write On Through Expiration Date The Thundering Herd Cache (Almost) Everything   Elastic Cache for Redis  Distributing Reads and Writes Mutli-AZ with Auto Failover Sharding with Redis Advanced Datasets with Redis   Monitoring and Tuning  Monitoring Cache Efficiency Watching for Hotspots Memory Optimization Redis Backup and Restore Cluster Scaling and Auto Discovery    Overview\n ElastiCache deploys one or more cache clusters for your application ElastiCache automates resources provisioning, failure detection and recovery, and software patching Supports Redis and Memcached engines  Alternatives to ElastiCache\n Amazon CloudFront - cache images, web pages, static data at the edge Amazon RDS Read Replicas - distributing data to remote apps On-host caching - this approach lacks of efficiency - cannot reuse existing cache entries and maintain consistency in validation of the cache keys across all hosts  Memcached vs Redis  Due to replication and persistence features of Redis, Redis is managed as relational database Memcached is designed as pure caching solution with no persistence - is managed as pool of nodes that can grow and shrink, similar to Amazon EC2 Auto Scaling Group  Important questions to consider impacting the choice of the caching engine:\nMemcached:\n Object caching as a primary goal? Offload database? Simplest caching model? Large cached nodes, multi-threaded performance with utilization of multiple cores Scale cache horizontally? Atomically increment / decrement counters?  Redis\n More advanced types, e.g. lists, hashes, bit arrays, HyperLogLogs and sets? Sorting and ranking datasets in memory? Pub/Sub capabilities in your application? Persistence of the key store? Run in multiple AZs with failover? Geospatial support? Encryption and compliance standards? PCI DSS, HIPAA, FedRAMP?  ElastiCache for Memcached  Considerably cheaper to add an in-memory cache then to scale up to a larger database cluster Easier to distribute an in-memory cache horizontally in comparison to relational database Choose the same AZs for ElastiCache as your application servers  Specify Preferred Zones option during cache cluster creation Spread Nodes Across Zones tells ElastiCache to distribute nodes to AZs evenly   Expect slightly higher latency for cross-zone AZ requests  Cache Node Size\n M5 or R5 families support the latest generation CPUs and networking capabilities Delivers up to 25Gbps of aggregate networking bandwidth with enhanced networking and over 600 GiB of memory M5.large single node can be a good starting point Track resource utilization through CloudWatch metrics Estimate the memory requirements by calculating the size consumed per cache item x number of items you want to cache  Security Groups and VPC\n ElastiCache supports security groups Advised to launch in a private subnet with no public connectivity Memcache doesn\u0026rsquo;t have any serious authentication or encryption capabilities Create a security group for ElastiCache cluster and allow traffic from \u0026ldquo;application tier\u0026rdquo; security group Test connectivity from an application instance to your cache cluster in VPC, using netcat:  nc -z w5 [cache endpoint] 11211 # will return 0 if connection was successful (the exist code of last command) echo $? Caching Design Patterns Some questions you need to think of:\n Is it safe to use a cached value? Is caching effective for that data? Is the data structured well for caching?  Problem Overview Objective: Splitting cache keys across multiple nodes to make use of multiple ElastiCache nodes\n Naive approach is to randomly distribute cache keys  Based on this approach hash key is generated from random CRC32 Node corresponding to hash key % (modulo) number of nodes will contain the key In the event of scaling, you will have to remap some keys, i.e. old count / new count If Scaling from 9 to 10 nodes, you will have to remap 90% of your keys Bad approach as scaling the nodes introduces more load on the database    Consistent Caching (Sharding)  Consistent Hashing  Alternative approach to spreading cache keys across your cache nodes Creating internal ring with a pre-allocated number of partitions that can hold hash keys There\u0026rsquo;s mathematical calculation involved to preallocate a set of random integers and assign cache nodes to the random integers In this case you find the closest integer in the ring for a given cache key and use the associated cache node Many Client Libraries support consistent hashing  Make sure that consistent hashing is enabled in the client library  For example in PHP: $memcached-\u0026gt;setOption(Memcached::OPT_LIBKETAMA_COMPATIBLE, true);   If possible use ElastiCache Clients with Auto Discovery to support Auto Discovery of new nodes as they are added to the cluster       Lazy Caching  Populate the cache only when an object is requested by the application Cache only contains objects that application requests, keeping the cache size managable Cache expiration is easily handled by deleting the cached object  Write On Through  Cache is updated realtime when the database is updated Advantages  Avoids cache misses Shifts any application delay to the write operation, which maps better with user expectations Simplifies cache expiration (cache is always up to date)   Disadvantages  Cache may be filled with unnecessary objects and may evict more frequently accessed objects out of cache If cache node fails you need to apply lazy cache mechanism to populate the cache    Expiration Date  Always apply TTL for all cache keys, except those updated by write-through caching For rapidly changing data add a TTL of few seconds to minimize the load on database Russian doll caching pattern: Nested records are managed with their own cache key and top-level resource is a collection of those cache keys When not sure, delete a cache key - Lazy Caching should refresh the key when needed  The Thundering Herd  The Thundering Herd effect happens when high number of users request the same piece of data with a cache miss Usually happens in highly concurrent environment This effect can also happen when adding a new cache node - as it has an empty memory Possible Solutions  Prewarm the cache using a script that hits a set of URLs  Prewarming can be automated by triggering the script to run whenever the app receives a cluster reconfiguration event through Amazon SNS   Add a bit of randomness to cache TTLs to mitigate simultaneous expiry event  ttl = 3600 + rand() * 120      Cache (Almost) Everything  Caching should be applied for the heavy queries of database Consider caching other less heavy queries as well, whenever appropriate Monitor cache misses to determine the effectiveness of your cache  Elastic Cache for Redis  Redis data structures cannot be sharded  Redis ElastiCache clusters are always a single node   Primary node can have one or more read replica  Replication group consists of a primary and up to five read replicas Number of replicas attached will affect the performance of primary node One or two read replicas in a different Availability Zone are sufficient for availability   With Multi-AZ enabled will automatically failover  Primary Endpoint is a DNS name of current Redis primary node In event of failover Primary Endpoint will be updated to point to new node   Supports persistence, backup and recovery  Distributing Reads and Writes  Requires configuring the application to write to primary endpoint and read from read replicas endpoint Read workloads can be separated from write workloads Read Replicas may return data slightly out of date from the primary node  There\u0026rsquo;s a short lag between the write operation to be reflected on the read replicas    Reading data from replica?\n Is the value being used only for display purposes? Is the value a cached value, for example a page fragment? Is the value being used on the screen where the user might have just edited it? - using outdated value will look like a bug Is the value being used for application logic? - using outdated value can be risky Are multiple processes using the value simultaneously, such as a lock or queue? - using outdated value can be risky  Mutli-AZ with Auto Failover  AWS ElastiCache will detect a failure of the primary node and transfer the primary endpoint to point into failover instance Failover process can take several minutes All production systems should use multi-az with auto-failover In case of the failover, the read replica selected to be promoted may be slightly behind master Slight data loss may be expected in case of rapidly changing data  Sharding with Redis  Simple keys and counters - support sharding Multidimensional sets, lists and hashes - don\u0026rsquo;t support sharding Redis client has to be configured to shard between redis clusters Horizontal sharding can be combined with split reads and writes  Sharded masters and sharded replicas   Designing the application to support read/write design in future you can add multiple clusters in future  Advanced Datasets with Redis Game Leaderboards\n Redis sorted sets simultaneously guarantee both uniqueness and ordering of elements Commnads start with Z, e.g. ZADD, ZRANGE, ZRANK On insertion element is reranked and assigned a numeric position  Recommendation Engines\n Some recommendation algorithmns, e.g. Slope One, require in-memory access to every item ever rated before by anyone in the system Data should be loaded in the memory to run the algorithm Redis counters can be used to increment / decrement the number of likes or dislikes for a given item Redis hashes can be used to manitain a list of everyone who liked / disliked an item Open source projects like Recommendify and Recommendable use Redis this way Persistence can be used to move keep this data in Redis  INCR \u0026quot;item:89287:likes\u0026quot; HSET \u0026quot;item:89287:ratings\u0026quot; \u0026quot;Susan\u0026quot; 1 INCR \u0026quot;item:89287:dislikes\u0026quot; HSET \u0026quot;item:89287:ratings\u0026quot; \u0026quot;Tommy\u0026quot; -1 Chat and Messaging\n Provides lightweight pub/sub mechanism Well-suited to simple chat and messaging needs In-app messaging, real-time comment streams Use PUBLISH and SUBSCRIBE commands Pub/sub messaging doesn\u0026rsquo;t get persisted to disk  You will loose the data if the cache node fails   Amazon SNS can be considered as a reliable alternative topic-based system  SUBSCRIBE \u0026quot;chat:15\u0026quot; PUBLISH \u0026quot;chat:15\u0026quot; \u0026quot;How are you?\u0026quot; UNSUBSCRIBE \u0026quot;chat:15\u0026quot; Queues\n Redis lists can be used to hold items in a queue When process picks up an item, item is pushed to in-progress queue and then deleted when the work is done Resque open source project (uses by Github) uses Redis as a queue. Redis queue has certain advantages  Very fast speed Once and only once delivery Guaranteed message orderding   ElastiCache for Redis backup and recovery options should be configured with Queue persistence in mind  Client Libraries and Consistent Hashing\n Redis client libraries support most popular programming languages Redis libraries rarely support consistent hashing as advanced types cannot be horizontally sharded Redis cannot be horizontally scaled easily Redis can only scale up to a larger node size, because its data structures must reside in a single memory image in order to perform properly  Monitoring and Tuning Monitoring Cache Efficiency  Use CloudWatch Metrics Watch CPU Usage  CPUUtilization EngineCPUUtilization Evictions  Large number of evictions indicates that your cache is running out of space   CacheMisses  Large number of CacheMissed combined with large number of Evictions indicates that the cache is thrashing due to the lack of memory   BytesUsedForCacheItems  Indicates the total amount of memory used by Memcahced / Redis. Both try to use as much memory as possible.   SwapUsage  In normal usage, neither Redis nor Memcached should be performing swaps.   Currconnections  An increasing number of connections might indicate a problem with your application. This value can be used as a threshold for alarm.     Scaling  For read intensive workloads, consider adding read replicas For write intensive workloads, consider adding more shards to distribute the workloads    Watching for Hotspots  Hotspots are nodes in your cache that receive higher load than other nodes Hotkeys - are cached keys that are access more frequently than others To investigate the hotspots is to track cache key access counts in application log  Will significantly affect performance, so should not be done unless you are very suspicious of hotspots One possible solution is to create a mapping table to remap very hot keys to separate set of cache nodes Another is to add additional layer of smaller caches in front of your main nodes to act as a buffer - gives more flexibility but introduces additional latency   Papers for researching on Hotspot issues:  Relieving Hot Spots on the World Wide Web Characterizing Load Imbalance in Real-World Networked Caches    Memory Optimization Memcached\n Uses slab allocator, allocates memory in fixed chunks When launching ElastiCache cluster, max_cache_memory parameter is set automatically chunk_size and chunk_size_growth_factor parameters work together to control how memory chunks are allocated  Redis\n Redis exposes a number of Redis configuraiton variables that will affect how Redis balances CPU and memory  Redis Backup and Restore  AWS automatically takes snapshots of your Redis Cluster and saves them to AWS S3 Redis backups require more memory to be available for the background Redis backup process For production - enable Redis backup with minimum 7 days retention  Cluster Scaling and Auto Discovery  AWS does not currently support auto-scaling Number of cluster nodes can be changed from AWS console/API During changing the cluster nodes, some of the cache keys will be remapped to new nodes - impacting performance of your application ElastiCache clients support auto-discovery of Memcached nodes Auto-discovery enables your application to auto-locate and connect to the Memcached nodes  Cluster Reconfiguration Events from Amazon SNS\n Your application can be configured to dynamically detect nodes being added or removed by reacting to Events through SNS ElastiCache:AddCacheNodeComplete and ElastiCache:RemoveCacheNodeComplete events are published when nodes are added and removed to the cluster Follow the steps:  Create AWS SNS topic for ElastiCache node additional and removal Modify application code to subscribe to the SNS topic When node is added or removed, re-run auto-discovery code to get the updated cache list Application adds the new list of cache nodes and reconfigured Memcached client accordingly    "
},
{
	"uri": "https://majdarbash.github.io/tags/php/",
	"title": "php",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/php-running-scripts-as-root/",
	"title": "PHP - Running scripts as root",
	"tags": [],
	"description": "",
	"content": "Very simple situation and very simple fix.\nWhether you are using apache of php-fpm, you need to grant your web server with sudo privilege. This can be done by adding the following line to /etc/sudoers file:\napache ALL=NOPASSWD: /usr/local/bin/my_command.sh\nDone !\n"
},
{
	"uri": "https://majdarbash.github.io/tags/programming/",
	"title": "programming",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/proxy/",
	"title": "proxy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/pybrain/",
	"title": "pybrain",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/python/",
	"title": "python",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-exercise-pyglatin-translator/",
	"title": "Python 2.7: Exercise: PygLatin Translator",
	"tags": [],
	"description": "",
	"content": "I was reviewing python tutorials in codeacademy which I really recommend as a great beginning to get yourself familiar with any language. These guys have got some interactive tutorials and exercises which makes it pleasure to start with something new.\nOf course you will need some broader sources and reference informaiton after completing the tutotrials in codeacademy. So I got this exercise from codeacademy - let\u0026rsquo;s see how we can solve it.\nProblem:\nNow let\u0026rsquo;s take what we\u0026rsquo;ve learned so far and write a Pig Latin translator.\nPig Latin is a language game, where you move the first letter of the word to the end and add \u0026ldquo;ay.\u0026rdquo; So \u0026ldquo;Python\u0026rdquo; becomes \u0026ldquo;ythonpay.\u0026rdquo; To write a Pig Latin translator in Python, here are the steps we\u0026rsquo;ll need to take:\n Ask the user to input a word in English. Make sure the user entered a valid word. Convert the word from English to Pig Latin. Display the translation result.  Solution:\ndef convert_to_pig_latin(word): first = word[0]\n# appending the string with the first letter and \u0026quot;ay\u0026quot; new\\_word = word + first + \u0026quot;ay\u0026quot; # removing the first character as it's already appended to the end new\\_word = new\\_word\\[1:len(new\\_word)\\] return new\\_word  input = raw_input(\u0026ldquo;Enter a word: \u0026ldquo;)\ninput should not be blank, and should consists of alpha characters if not (len(input) \u0026gt; 0 and input.isalpha()): print \u0026ldquo;This is not a word!\u0026rdquo; else: translated = convert_to_pig_latin(input) print \u0026ldquo;The word %s is translated to pig latin: %s\u0026rdquo; % (input, translated)\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-file-input-output/",
	"title": "Python 2.7: File Input / Output",
	"tags": [],
	"description": "",
	"content": "Writing to file Following example will populate the output.txt file with the squares of numbers from 1 - 10.\nmy_list = [i**2 for i in range(1,11)]\nGenerates a list of squares of the numbers 1 - 10 f = open(\u0026ldquo;output.txt\u0026rdquo;, \u0026ldquo;w\u0026rdquo;)\nfor item in my_list: f.write(str(item) + \u0026ldquo;\\n\u0026rdquo;)\nf.close()\nYou have probably noticed that we are opening the file in \u0026ldquo;w\u0026rdquo; mode, i.e. for writing. We can use \u0026ldquo;r+\u0026rdquo; flag if we would like to read and write from the file.\nIn this example, we are using with and as syntax. Using this statement python will execute the required operations on the file and will close it automatically:\nwith open(\u0026ldquo;output.txt\u0026rdquo;, \u0026ldquo;w\u0026rdquo;) as file: file.write(\u0026ldquo;File will be automatically closed!\u0026quot;)\nif not file.closed: print \u0026ldquo;Closing file \u0026hellip;\u0026rdquo; file.close() else: print \u0026ldquo;Skipping file closing, it was closed for us!\u0026rdquo;\nReading from file my_file = open(\u0026ldquo;output.txt\u0026rdquo;, \u0026ldquo;r\u0026rdquo;)\nwill output the contents of a single line while the pointer is and moves the pointer to the next line output: one line contents print my_file.readline()\noutput: the contents of output.txt file from the pointer location to the end of file - will move the pointer to the end of file print my_file.read()\nmy_file.close()\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-importing-modules/",
	"title": "Python 2.7: Importing Modules",
	"tags": [],
	"description": "",
	"content": "Modules make your code reusable and sharable amount different files. Module is a file that contains definitions - including variables and functions - that you can use once its imported.\nSome modules are built-in and will expose you to their functions once imported. Example below shows how sqrt function becomes available once the math module is imported:\n# importing math module import math print math.sqrt(25)\nimporting specific functions / variables frmo math module from math import sqrt print sqrt(25)\nimporting all the definitions form match module and unwrapping math. from math import * print sqrt(25)\nOf course, universal importing is not safe. You may fill your code with unnecessary variables and definitions which may conflict by using the same definition names between the modules.\nDisplays what is inside the module (all definitions in form of array of strings):\nimport math everything = dir(math) print everything\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-lambda-and-bitwise-operators/",
	"title": "Python 2.7: Lambda and Bitwise Operators",
	"tags": [],
	"description": "",
	"content": "Lambda functions Using lambda functions we can create functions in runtime, and use them as we go. Here\u0026rsquo;s an example of lambda function and how it is used to filter the array. filter() function takes the lambda function as the first parameter, passes items of the list sequentially and filters the list to retain the items which pass the filtration function passed as first argument.\n# will assign a range of numbers from 0 to 15 to numbers list numbers = range(16)\noutput: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] print numbers\nfiltering only numbers which can be divided by 3 filtered_numbers = filter(lambda x: x % 3 == 0, numbers)\noutput: [0, 3, 6, 9, 12, 15] print filtered_numbers\nBitwise Operators Bitwise operators directly manipulate bits. Python provides certain set of operatos. Example:\nprint 5 \u0026raquo; 4 # Right Shift print 5 \u0026laquo; 1 # Left Shift print 8 \u0026amp; 5 # Bitwise AND print 9 | 4 # Bitwise OR print 12 ^ 42 # Bitwise XOR print ~88 # Bitwise NOT\noutput: 0 output: 10 output: 0 output: 13 output: 38 output: -89 Base 2 number system # convert binary to decimal print 0b1 # output 1 print 0b10 # output :2 print 0b11 # output: 3 print 0b10 + 0b01 # output: 3\nconvert decimal to binary output:0b1111 print bin(15) convert decimal to base-8 output: 017 print oct(15)\nconvert decimal to base-16 / hexedecimal output: 0xe print oct(14)\nConverting string to number # converting string from base 10 number\noutput: 10 print int(\u0026ldquo;10\u0026rdquo;)\nconverting string from base 2 number output: 2 print int(\u0026ldquo;10\u0026rdquo;, 2)\nconverting string from base 16 number output: 204 print int(\u0026ldquo;cc\u0026rdquo;, 16)\nOther Operators # bitwise AND\noutput: 0b001 print bin(0b101 \u0026amp; 0b011)\nbitwise OR output: 0b111 print bin(0b101 | 0b011)\nbitwise XOR output: 0b110 print bin(0b101 ^ 0b011)\nbitwise NOT flips all the bits in a number output: -0b110 print bin(~0b101)\nCheck if the bit #5 is on or off def check_bit5(number): if number \u0026amp; 0b10000 \u0026gt; 0: return \u0026lsquo;on\u0026rsquo; else: return \u0026lsquo;off\u0026rsquo;\noutput: on print check_bit5(0b111101)\noutput: off print check_bit5(0b101101)\nFlipping the nth bit in a number # will flip the nth bit in the number def flip_bit(number, n): mask = 1 \u0026laquo; (n - 1) result = number ^ mask return bin(result)\nwill flip the 3rd bit in the number output: 0b100100 print flip_bit(0b100000, 3)\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-lists-dictionaries/",
	"title": "Python 2.7: Lists &amp; Dictionaries",
	"tags": [],
	"description": "",
	"content": "Lists Lists are the same as arrays in other programming languages. List can be defined using assigment:\nfruits = [\u0026ldquo;banana\u0026rdquo;, \u0026ldquo;apple\u0026rdquo;, \u0026ldquo;strawberry\u0026rdquo;]\nand can be accessed by indices, similar to any other programming language:\n# output: strawberry print fruits[2]\nsubstitution of existing element will replace apple with kiwi in fruits list fruits[1] = \u0026ldquo;kiwi\u0026rdquo;\ndefining an empty list:\nvegetables = []\nappending items to the existing list:\nvegetables.append(\u0026ldquo;cucumber\u0026rdquo;) vegetables.append(\u0026ldquo;eggplant\u0026rdquo;)\nobtaining the length of the list, i.e. number of elements in the list:\nprint len(vegetables)\nList slicing You can extract certain chunks of the list, using \u0026ldquo;: \u0026quot; syntax. You have indicate from and to index, to represent the index range.\n# output: cucumber, eggplant print vegetables[0:2]\noutput: apple, strawberry print fruits[1:3]\nanimals = \u0026ldquo;catdogfrog\u0026rdquo; cat = animals[:3] # slicing elements until the third dog = animals[3:6] # slicing based on range frog = animals[6:] # slicing elements starting from 7th\nSearching through the list You can search through the list using .index(item).\nfruits = [\u0026lsquo;apple\u0026rsquo;, \u0026lsquo;kiwi\u0026rsquo;, \u0026lsquo;banana\u0026rsquo;, \u0026lsquo;peach\u0026rsquo;]\noutput: 1 - i.e. index of element kiwi kiwi_index = fruits.index(\u0026lsquo;kiwi\u0026rsquo;) print \u0026ldquo;Kiwi found at position %s\u0026rdquo; % kiwi_index\ninsert watermelon in the kiwi position, pushing everything down fruits.insert(kiwi_index, \u0026lsquo;watermelon\u0026rsquo;)\noutput: apple, watermelon, kiwi, banana, peach print fruits\nTraversing through the list You can traverse through the list using the for loop.\n# output: 2,4,6,8,10 my_list = [1,2,3,4,5] for number in my_list: print 2 * number\nmy_list = [5,6,3,2,1] for i in range(0, len(my_list)): print my_list[i]\nRemove specific item from the list: furniture = [\u0026lsquo;sofa\u0026rsquo;, \u0026lsquo;chair\u0026rsquo;, \u0026lsquo;table\u0026rsquo;, \u0026lsquo;bed\u0026rsquo;] furniture.remove(\u0026lsquo;chair\u0026rsquo;)\noutput: sofa, table, bed print furniture\nOther list functions # sorting array in ascending order\noutput: 1,2,3,5,8 my_list = [2,5,1,3,8] my_list.sort() print my_list\nusing .pop(index) - returns element from the list and removes it furniture = [\u0026lsquo;sofa\u0026rsquo;, \u0026lsquo;chair\u0026rsquo;, \u0026lsquo;table\u0026rsquo;, \u0026lsquo;bed\u0026rsquo;] item = furniture.pop(2)\noutput: table print item\noutput: sofa, chair, bed print furniture\n# output: list of numbers from 2 to 8, excluding 8\noutput: 2,3,4,5,6,7 list = range(2, 8) print list\nJoining lists\nLists can be joined using summation operator\nn = [1,2,3] m = [7,4,5]\ndef join_lists(x, y): return x + y\noutput: 1,2,3,7,4,5 print join_lists(n, m)\nAlso list can be multiplied by a certain number and that it will join with itself several times\nm = [\u0026ldquo;A\u0026rdquo;]\noutput: [\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;] print m * 10\nList elements can be \u0026ldquo;imploded\u0026rdquo; to a string with a specified separator using .join(list) function.\nmy_list = [\u0026ldquo;I\u0026rdquo;, \u0026ldquo;believe\u0026rdquo;, \u0026ldquo;I\u0026rdquo;, \u0026ldquo;can\u0026rdquo;, \u0026ldquo;fly\u0026rdquo;] joint_string = \u0026quot; \u0026ldquo;.join(my_list)\noutput: \u0026ldquo;I believe I can fly\u0026rdquo; print joint_string\nCasting list elements to other data types\nmy_list_int = [1,2,3]\nwill return the same array of int, but casted to str my_list_str = map(str, my_list_int)\nCheck if number is a member of the list:\nimport random my_list = [] my_list.append(random.randint(0,10)) my_list.append(random.randint(0,10)) my_list.append(random.randint(0,10))\nguess_number = int(raw_input(\u0026ldquo;Guess a number between 0 and 10: \u0026ldquo;))\nif guess_number not in range(0,10): print \u0026ldquo;The number is even not between 0 and 10\u0026rdquo; else: if guess_number in my_list: print \u0026ldquo;Congratulations, you found the number!\u0026rdquo; else: print \u0026ldquo;Sorry, you missed it!\u0026rdquo; print \u0026quot; \u0026ldquo;.join(map(str, my_list))\nDictionaries Dictionaries are similar to associative arrays. They consist of key-value pairs.\nscore = {\u0026ldquo;James\u0026rdquo;: 12, \u0026ldquo;Samantha\u0026rdquo;: 43, \u0026ldquo;Andre\u0026rdquo;: 81}\noutput: 12 print score[\u0026ldquo;James\u0026rdquo;]\nIn contrast to lists which are enclosed by [] brackes, dictionaries are enclosed by curly braces {}.\nElements can be appened to dictionaries using dictionary[key] assignment\nscore[\u0026ldquo;Kate\u0026rdquo;] = 34\noutput: 4 print len(score)\nRemoving items from the dictionary:\nscore[\u0026ldquo;Rene\u0026rdquo;] = 29\noutput: 5 print len(score)\ndel score[\u0026ldquo;Rene\u0026rdquo;]\noutput: 4 print len(score)\nTraversing through the dictionary # output: Price of apple is 32.5\noutput: Price of potato is 20 prices = {\u0026ldquo;potato\u0026rdquo;: 20, \u0026ldquo;apple\u0026rdquo;: 32.5} for key in prices: print \u0026ldquo;Price of %s is %s\u0026rdquo; % (key, prices[key])\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-loops/",
	"title": "Python 2.7: Loops",
	"tags": [],
	"description": "",
	"content": "While Loops count = 0\noutput: 0,1,2,3,4 while count \u0026lt; 5: print count count += 1\ncouple of empty lines print print\ncount = 0 while count \u0026lt; 5: if count == 3: count +=1 continue print count count += 1\noutput: 0,1,2,4 - will skip the number 3 print print\ncount = 0 while True: if count == 8: break print \u0026ldquo;Count %s\u0026rdquo; % count count += 1\nPython supports while \u0026hellip; else structure. In this case the else block will execute when loop condition is evaluated to False - this means the while block never entered or the loop exited normally after the loop condition was evaluated to False. However python will not execute the else block if the loop exit is due to break.\ncount = 0 while count \u0026lt; 5: count += 1 print count else: print \u0026ldquo;Done!\u0026rdquo;\nFor Loops # output: 0,1,2,3 for i in range(4): print i\nLooping through the list fridge = [\u0026ldquo;chicken\u0026rdquo;, \u0026ldquo;tomato\u0026rdquo;, \u0026ldquo;pasta\u0026rdquo;]\noutput: chicken, tomato, pasta for item in fridge: print item\nLooping through the dictionary menu = { \u0026lsquo;fajita\u0026rsquo;: 23.5, \u0026lsquo;chicken alfredo\u0026rsquo;: 45, \u0026lsquo;fish platter\u0026rsquo;: 34 }\nprint \u0026ldquo;Welcome to our restaurant, check our menu:\u0026rdquo; for key in menu: print key + \u0026quot; \u0026quot; + str(menu[key])\nEnumerate function Using for loop to loop through a list you wouldn\u0026rsquo;t know the index of the element you are accessing at each iteration. Enumarate function will help us with this, as it will supply a corresponding index to each element in the list that you pass it.\nchoices = [\u0026lsquo;pasta\u0026rsquo;, \u0026lsquo;pizza\u0026rsquo;, \u0026lsquo;chicken alfredo\u0026rsquo;]\nbasic for loop for item in choices: print \u0026ldquo;Item %s\u0026rdquo; % item\nlooping using enumerate function for index, item in enumerate(choices): print \u0026ldquo;Item %s: %s\u0026rdquo; % (index, item)\nMultiple lists Zip function will create pairs of elemnts when passed two lists. It will stop at the end of shorter list.\nlist1 = [1,2,3,4,5] list2 = [4,6,2]\noutput: 1,4 output: 2,6 output: 3,2 for el1, el2 in zip(list1, list2): print el1, el2\nFor / else For loop may end with else. Else clause will be executed only when the for loop ends normally, without break.\nlist = [3,5,2,1]\nfor number in list: print number else: print \u0026lsquo;all numbers were printed\u0026rsquo;\nDictionary functions: items(), keys(), values() employees = { \u0026ldquo;John\u0026rdquo;: \u0026ldquo;Manager\u0026rdquo;, \u0026ldquo;Bob\u0026rdquo;: \u0026ldquo;Assistant\u0026rdquo;, \u0026ldquo;Kate\u0026rdquo;: \u0026ldquo;Secretary\u0026rdquo; }\noutput: the list of key, value pairs output: [(\u0026lsquo;Bob\u0026rsquo;, \u0026lsquo;Assistant\u0026rsquo;), (\u0026lsquo;John\u0026rsquo;, \u0026lsquo;Manager\u0026rsquo;), (\u0026lsquo;Kate\u0026rsquo;, \u0026lsquo;Secretary\u0026rsquo;)] print employees.items()\noutput: [\u0026lsquo;Bob\u0026rsquo;, \u0026lsquo;John\u0026rsquo;, \u0026lsquo;Kate\u0026rsquo;] print employees.keys()\noutput: [\u0026lsquo;Assistant\u0026rsquo;, \u0026lsquo;Manager\u0026rsquo;, \u0026lsquo;Secretary\u0026rsquo;] print employees.values()\nBuilding Lists # output: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] numbers = range(1,11) print numbers\noutput: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] numbers = [x for x in range(1,11)] print numbers\noutput: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] numbers = [x ** 2 for x in range(1,11)] print numbers\noutput:[4, 16, 36, 64, 100] numbers = [x ** 2 for x in range(1, 11) if x % 2 == 0] print numbers\nSlicing Lists Syntax: [start:end:stride]\nstart - where the slicing starts (inclusive)\nend - where the slicing ends (exclusive)\nstride - space between items in the sliced list (i.e. step)\npositive stride length - traverses the list from left to right\nnegative stride length - traverses the list from right to left\nnumbers = [1,3,4,5,6,7,8,9,10]\noutput: [5, 6, 7, 8, 9, 10] print numbers[3:]\noutput: every second element output: [1, 4, 6, 8, 10] print numbers[::2]\noutput: [5, 6, 7] print numbers[3:6]\noutput: [5, 7, 9] print numbers[3:9:2]\noutput: [10, 9, 8, 7, 6, 5, 4, 3, 1] reversed = numbers[::-1] print reversed\n"
},
{
	"uri": "https://majdarbash.github.io/random/python-for-data-science/",
	"title": "Python for Data Science",
	"tags": ["data-science", "python"],
	"description": "",
	"content": "ipython - Interactive Python Interpreter\nVariables  type(var_name) returns the type of the variable, e.g. float, string, \u0026hellip;  Type Conversion  str(var_name) float(var_name)  Math  Exponentiation 4**2 will give 16 Modulo 18 % 7 will give 4  Generating Output  print(string)  Publish Date: 2019-12-02\n "
},
{
	"uri": "https://majdarbash.github.io/tags/ready/",
	"title": "ready",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/reinforced/",
	"title": "reinforced",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/remove-svn-files-unverison-subversion-working-copy/",
	"title": "Remove .svn files - unverison subversion working copy",
	"tags": ["subversion", "version control system", "vcs", ".svn files"],
	"description": "",
	"content": "In order to remove .svn files you need to execute the following command on your terminal:\nfind . -name .svn -exec rm -rf {};  Publish Date: 2015-01-02\n "
},
{
	"uri": "https://majdarbash.github.io/tags/review/",
	"title": "review",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/deploybot-com-review/",
	"title": "Review of DeployBot.com",
	"tags": [],
	"description": "",
	"content": "There\u0026rsquo;s tons of deployment tools available these days. Deploybot.com (previously known as Dploy.io) is one of the nice tools I used since a while.\nBasically the tool offered as capability to configure SSH connection to statically defined instances, perform code upload, apply configuration, run before and after install scripts and trigger other deployments. In addition they provide what they call \u0026ldquo;Atomic\u0026rdquo; deployment. This assures zero downtime and code consistency throughout the deployment nodes as the document root is switched from one release to another using softlink.\nRecently deploybot.com launched more tools like running before deployment script in custom build environment. You can actually choose your required environment: PHP, NodeJS etc.. and run your build generation scripts. Afterwards the build will be deployed to the instances directly.\nIf something is not right you can rollback to the previous version. The easy to use interface allows you to view the git commit difference being deployed in each release. You don\u0026rsquo;t need to manually track it - your deployment history can be easily accessed. The nice feature I found is ability to create user accounts with different access levels, e.g. View only, View and Deploy or Full Access.\nPost deployment we usually like to be informed on Slack. There\u0026rsquo;s nothing easier then adding slack webhook into DeployBot.com and you get instance updates.\nWith DeployBot.com you can deploy directly to AmazonS3 buckets and they are expanding their AWS integrations. Unless you are using AWS Auto-Scaling, utilizing tools like Jenkins, CodePipeline and CodeDeploy, DeployBot.com is one of the tools which may be useful to consider\nPublish Date: 2013-04-09\n "
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/route53/",
	"title": "Route53",
	"tags": [],
	"description": "",
	"content": " Route53 gets its name from port 53 of DNS server IPv4 space is 32 bit fields having over 4 billion different addresses. IPv6 solves the depletion issue and has as address space of 128 bits For each public hosted zone Route 53 automatically created NS record and SOA record (Start of Authority Record) NS Record - is used by Top Level Domain servers to direct traffic to the Content DNS server which contains the authoritative DNS records When creating a public hosted zone, Route 53 gives you NS records in different Top-Level Domains. The start of authority (SOA) record identifies the based DNS information about the domain. SOA record components  NS that created the SOA record Email of the administrator The current version of the data file The default number of seconds for the time-to-live file on resource records   Domain to IP request flow  Top Level Domain NS Records SOA   The lower the TTL (time to live), the faster changes to DNS records take to propagate through the internet CNAME records can\u0026rsquo;t be used for naked domain names (zone apex record). It must be either A record or an Alias Given a choice between Alias record and CNAME always choose an Alias record Common DNS Types  SOA Records NS Records A Records CNAMES MX Records PTR Records   You can buy domain names directly with AWS It can take up to 3 days to register depending on the circumstances Routing Policies  Simple Routing\nYou can only have one record with multiple IP addresses. If you specify multiple values in a record, Route 53 returns all values to the user in a random order.\nSimple routing cannot be associated with a health check. Weighted Routing\nTraffic can be split based on different weights assigned. Latency-based Routing\nAllows you to route your traffic based on the lowest network latency for your end user. Failover Routing  Active/passive set up Specifying primary and secondary records Health check is associated with the primary record   Geolocation Routing\nLets you choose where your traffic will be sent based on the geographic location of your users.  This is not a latency based routing - the routes are locked down based on geolocation.   Geoproximity Routing (Traffic Flow Only)\nGeorproximity routing lets Route53 route traffic to your resources based on the geographic location of your users and your resources. You can optionally choose to route more traffic or less to a given resource by specifying a value, know as bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource. To use this you must use Route53 Traffic flow. Multivalue Answer Routing\nExactly the same as Simple Routing, however, allows you to use health checks on each record. Route53 will return values for healthy resources.   Health Checks  You can set health checks on individual record sets If a record set fails a health check it will be removed from Route53 until it passes the health check You can set SNS notifications to alert you if a health check is failed   Health Checks can be created and associated with Route 53 records  FAQs  Route 53 is built using AWS\u0026rsquo;s highly available and reliable infrastructure. Each Amazon Route 53 hosted zone is served by its own set of virtual DNS servers. They are assigned by the system when the hosted zone is created. Amazon Route 53 charges are based on actual usage of the service for Hosted Zones, Queries, Health Checks, and Domain Names. Access to Route53 can be controlled using IAM. You can configure Amazon Route 53 to log information about the queries that Amazon Route 53 receives including date-time stamp, domain name, query type, location etc. to CloudWatch Logs. Amazon Route53 uses anycast network - is a networking and routing technology that helps your end users\u0026rsquo; DNS queries get asnwered from the optimal Route53 location given network conditions. Route53 account is limited to a maximum of 500 hosted zones and 10,000 resource record sets per hosted zone and 50 domains. Multiple hosted zones can be created for domain. In addition to standard record types supported by Route53, alias records are supported, which is Route 53-specific extension to DNS. Alias can be used to map your entires to AWS Resources. Wildcard entries are supported. Zone apex can be mapped to AWS Resources by using alias records. Traffic Flow makes it easy for developers to create policies that route traffic based on constraints like: latency, endpoint health, multivalue; answers, weighted round robin, and geo. In addition to these, Traffic Flow also supports geoproximity based routing with traffic biasing. Traffic Policy is the set of rules that routes end users\u0026rsquo; request to one of the application\u0026rsquo;s endpoints. Policy Record associates the traffic policy with the appropriate DNS name within an Amazon Route 53 hosted zone that you own. You are billed from Traffic Flow per Policy Record. Private DNS should be attached to a VPC Route53 Health checks could verify the expected content of the web server by using the \u0026ldquo;Enable String Matching\u0026rdquo; option. Amazon Route 53’s metric based health checks let you perform DNS failover based on any metric that is available within Amazon CloudWatch, including AWS-provided metrics and custom metrics from your own application. Domains registered are configured to renewal automatically. Route53 provides privacy protection at no additional charge. Route53 registers top-level domains through either Amazon Registar or Gandi. Route53 Resolver is a regional DNS service that provides recursive DNS lookups for names hosted in EC2 as well as public names on the internet. Route 53 is Authoritative and Recursive DNS. Authoritative DNS - contains the final answer to a DNS query. Recursive DNS - forwards the query directly to a specific recursive DNS server.  "
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/s3/",
	"title": "S3",
	"tags": [],
	"description": "",
	"content": " Billing S3 (Simple Storage Service) Usage Patterns Storage Classes S3 Glacier S3 Billing Access \u0026amp; Encryption Versioning Lifecycle Management Tools \u0026amp; Glacier Cross Region Replication Amazon S3 Transfer Acceleration Amazon S3 Notifications CloudFront Snowball Storage Gateway FAQs  Billing  Billing Alarms can be created from CloudWatch. Billing Notifications should be enabled from the Billing Preferences section.  S3 (Simple Storage Service)  Provides developers and IT teams with secure, durable, highly scalable object storage Provides simple web services interface to store and retrieve data S3 is a safe place to store the files S3 and Glacier are not block storages S3 is Object-Based - allows you to upload files Files can be 0 bytes to 5TB Successful uploads will generate an HTTP 200 code Unlimited storage Files are stored in Buckets Objects consist of:  Key (name of the object) Value (the data) Version ID (important for versioning) Metadata (data about data you are storing) Subresources  Access Control List Torrent     Files can be from 0 Bytes to 5TB There is unlimited storage Files are stored in Buckets S3 is a universal namespace. Names must be unique globally. Data Consistency  Read after Write consistency for PUTS of new Objects\nIf you write a new file and read it immediately after, you will be able to view that data Eventual Consistency for overwrite PUTS and DELETEs (can take some time to propagate)\nIf you update an existing file or delete a file and read it immediately, you may get the older version, or you may not. Changes to objects can take a little bit of time to propagate.   Tiered Storage Available Lifecycle Management Versioning Encryption MFA Delete Secure your data using Access Control Lists and Bucket Policies Support BitTorrent peer-to-peer protocol  Allows cost saving when distributing content at high scale   Amazon S3 can be paired with Amazon CloudSearch / DynamoDB or RDS for ease of querying metadata and locating the object reference.  Usage Patterns  Store and distribute static web content and media Host entire static website Data store for computation and large-scale analytics, allowing concurrent access to multiple computing nodes Highly durable, scalable and secure solution for backup and archiving of critical data  Storage Classes  S3 Standard  99.99% availability 99.999999999% durability for S3 information. (11x9s) Stored redundantly across multiple devices in multiple facilities Designed to sustain a loss of 2 facilities concurrently   S3 - IA (Infrequently Accessed)  For data that is accessed less frequently but requires rapid access when needed Lower fee that S3, but you are charged a retrieval fee   S3 One Zone - IA (Infrequently Accessed, was called before RRS - Reduced Redundancy Storage)  Lower-cost option for infrequently accessed data, but do not require the multiple Availability Zone data resilience.   S3 - Intelligent Tiering  Uses machine learning Optimizes costs automatically by moving data to the most cost-effective access tier, without performance impact or operational overhead   S3 Glacier  secure, durable and low-cost storage class for backup and data archiving retrieval times are configurable from minutes to hours retrieval puts a copy of retrieved object in S3 Reduced Redundancy Storage (RRS) for a specified retention period (original object remains in Glacier) expedited, standard and bulk retrievals data is encrypted by default   S3 Glacier Deep Archive  lowest-cost storage class where a retrieval time of 12 hours is acceptable    S3 Glacier  Single Archive Limited to 40TB in size There\u0026rsquo;s no limit on total amount of data you can store in S3 Glacier Vaults can be locked by using lockable policies  You can specify \u0026ldquo;undeletable records\u0026rdquo; or \u0026ldquo;time-based data retention\u0026rdquo; in \u0026ldquo;Glacier Vault Lock\u0026rdquo; policy After policy is locked it becomes immutable and Amazon Glacier enforces the controls to help achieve compliance objectives   Can be integrated with CloudTrail to help control access Can be interfaces using REST web services, or as a storage class in S3 Objects archived to Glacier using S3 Lifecycle policies can be accessed only from S3 API and not from Glacier API Amazon Glacier performs regular systematic data integrity checks and is built to be automatically self-healing  S3 Billing  Storage Number of requests Storage Management Pricing Data Transfer Pricing Transfer Acceleration Cross-Region Replication  Access \u0026amp; Encryption  By default, all newly created buckets are PRIVATE Control access to the buckets using:  Bucket Policies Access Control Lists   Encryption in Transit  SSL/TLS (HTTPS)   Encryption At Rest (Server Side)\n(SSE = Server Side Encryption)  SSE-S3, S3 Managed Keys - AES-256 SSE-KMS, AWS Key Management Service SSE-C, Customer Provided Keys    Versioning  Stores all versions of an object (including all writes and even if you delete an object) Great backup tool Once enabled, Versioning cannot be disabled, only suspended. Integrates with Lifecycle rules Versioning\u0026rsquo;s MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security. Size of the bucket is a sum of all versions of the files stored in the bucket A specific version of the file can be deleted Deletion of a file will place a delete marker  Lifecycle Management Tools \u0026amp; Glacier  Allows you to automate moving your objects between the different storage tiers Can be used in conjunction with versioning Can be applied to current versions and previous versions  Cross Region Replication  Versioning must be enabled on both the source and destination buckets for CRR to work Regions must be unique CRR will not replicate the objects created before the CRR Rule was added Delete markers are not replicated Deleting individual versions or delete markers will not be replicated All subsequently updated files will be replicated automatically  Amazon S3 Transfer Acceleration  takes advantage of CloudFront\u0026rsquo;s globally distributed edge locations can improve upload and access times can be tested using speed comparison tool:\n(http://s3-accelerate-speedtest.s3-accelerate.amazonaws.com)  Using S3 Transfer Acceleration:\n Enable Transfer Acceleration on S3 Bucket Modify Amazon S3 PUT and GET requests to use s3 accelerate endpoint domain name (.s3-accelerate.amazonaws.com) - Regular endpoint will still be accesible Some customers measured performance to exceed 500% percent  Amazon S3 Notifications  Can be issued when certain events happen in your bucket Notifications can be issued to Amazon SQS, SNS Topics and Lambda functions  CloudFront  Edge Location - the location where the content will be cached Origin - the origin of all the files that the CDN will distribute. It can be an S3 Bucket, an EC2 Instance, an Elastic Load Balancer, or Route 53 Distribution - The name that is given to the CDN which consists of a collection of Edge Locations If Edge Location does not have a file in the cache, it will download it from the Origin using optimized networks Objects are cached for the life of the TTL (Time to Live) Edge locations are not just read-only, you can write to them to Types of Distribution supported:  Web Distribution RTMP - Used for Media Streaming   Invalidation  Clears the cache from the Edge Locations   You can invalidate cached objects, but you will be charged  Snowball Petabyte-scale data transporter solution that uses secure appliances to transfer large amounts of data into and out of AWS.\n Snowball  Import to S3 Export from S3 Types  50TB 80TB   Using it can be cheaper than using high-speed internet   Snowball Edge  is a 100TB data transfer device with on-board storage and compute capabilities. Can be used to move large amounts of data into and out of AWS. Applications will continue to run even when they are not able to access the cloud   Snowmobile  Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. Can transfer up to 100 PB per SnowMobile, 45-foot long ruggedized shipping container, pulled by a semi-trailer truck.    Cost Model\n Service Fee (per job) Extra day charges as required (first 10 days of onsite usage are free) Data Transfer  Storage Gateway Connects on-premise software appliance with cloud-based storage to provide seamless and secure integration between an organization\u0026rsquo;s on-premises IT environment and AWS\u0026rsquo;s storage infrastructure.\nThe service enables you to securely store data to the AWS cloud for scalable and cost-effective storage.\nCan be installed as a VM image on a host in a data center. Supports either VMware ESXi or Microsoft Hyper-V hypervisors.\nPhysical appliances are available as well.\nTypes of Storage Gateways:\n File Gateway (NFS)\nFor files: files are stored as objects in your S3 buckets and accessed through an NFS mount point. Ownership, permissions, and timestamps are stored in S3 user-metadata. Volume Gateway (iSCSI)\nAn application can use the disk volumes using iSCSI block protocol.\nData written to the volumes can be asynchronously backed up as point-in-time snapshots of your volumes, and stored in the cloud as Amazon EBS snapshots.\nSnapshots are incremental backups and only changed blocks will be charged.  Stored Volumes\nDAta will be stored locally and asynchronously backed-up to S3 in the form of EBS. (1GB - 16TB volume size) Cached Volumes\nData is stored on AWS S3, while retianing frequently accessed data locally in your storage gateway. This minimizes the need to scale on-premises storage infrastructure, while still providing your applications with low-latency access to their frequently accessed data. (1GB - 32TB volume size)   Tape Gateway (TPL)\nData archiving to AWS Cloud. Lets you leverage your existing tape-based backup application infrastructure to store data on virtual tape cartridges that you create on your tape gateway.\nTape gateway is preconfigured with a media changer and tape drives, which are available to your existing client backup applications as iSCSI devices.  FAQs  The total volume of data is unlimited Individual objects can have a max size of 5Tb Largest object uploaded in a single put is 5Gb For objects larger then 100Mb users should consider using multi-part upload functionality Amazon uses S3 for its developers and a wide variety of projects Amazon S3 is a simple key-based object store. Tags can be added to the objects to organized the data. Pricing components include: storage used, data transfer and data requests Amazon Macie - AI-powered security service that helps you prevent data loss by discovering, classifying, and protecting sensitive data stored in Amazon S3 Amazon S3 uses a combination of Content-MD5 checksums and cyclic redundancy checks (CRCs) to detect data corruption AZs are automatically assigned in Amazon S3 based on the storage class used If the source object is uploaded using the multipart upload feature, then it is replicated using the same number of parts and part size. For example, a 100 GB object uploaded using the multipart upload feature (800 parts of 128 MB each) will incur request cost associated with 802 requests (800 Upload Part requests + 1 Initiate Multipart Upload request + 1 Complete Multipart Upload request) when replicated. You will incur a request charge of $0.00401 (802 requests x $0.005 per 1,000 requests) and a charge of $2.00 ($0.020 per GB transferred x 100 GB) for inter-region data transfer. After replication, the 100 GB will incur storage charges based on the destination region.  "
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/exam-overview/",
	"title": "SAA Exam Overview",
	"tags": [],
	"description": "",
	"content": " Intended for people performing a Solutions Architect role. Valid for 2 years. Question Domains  Design Resilient Architectures Define Performant Solutions Specify Secure Applications and Architectures Design Cost-optimized Architectures Define Operationally Excellent Architectures   Multiple choice questions  single selection multiple selection    Design Resilient Architectures  Choose reliable/resilient storage Determine how to design decoupling mechanisms using AWS services Determine how to design a multi-tier arhictecture solution Determine how to design high availability and/or fault tolerant solutions  High Availability vs Fault Tolerance\n High Availability means that system is up and available but it might perform in degraded state Fault Tolerant is a higher bar - it means that the user does not experience any impact of the fault - the SLA is met  RTO / RPO\n RTO - Recovery Time Objective  how long does it take for system to be back online   RPO - Recovery Point Objective  how much data is lost if the system fails    Design Performant Architectures   Choose performant storage and databases\n  Apply caching to improve performance\n  Design solutions for elasticity and scalability\n  EBS SSD volumes offer better performance then HDD\n  Static content can be offloaded to S3 instead of keeping them on web servers\n  Data Stored on EBS is automatically replicated within an AZ\n  Databases\n Amazon RDS  Complex transactions or complex queries   Amazon DynamoDB  Massive read/write rates Scalability   Amazon Redshift    Useful for analytic queries      Caching\n CloudFront Edge Locations ElasticCache  Memcached  Multithreading, Low maintenance, Horizontal scaling   Redis  Support for data structures, Persistence, Read replicas/failover, Cluster mode/sharded clusters      CloudFront\n Can be used to serve dynamic content with TTL of 0 Improves security, integrates with AWS WAF and AWS Shield Advanced Can serve static content and cache response on the Edge nodes  AutoScaling\n Launch Configuration  EC2 instance type and AMI   Auto Scaling Group  Defines Launch Configuration Min, max and the desired size of the ASG May reference an ELB Health check type   Auto Scaling Policy  Scaling In / Out Uses CloudWatch alarms take an Auto Scaling action   CloudWatch  Monitors CPU, Network, Queue Size CloudWatch Logs, Metrics (Default / Custom)    Secure Architectures Shared Responsibility Model  AWS Responsibility\n(Security of the Cloud)  AWS Global Infrastructure AWS Foundation Services (Compute, Storage, Database, Networking)   Customer Responsibility\n(Their Security in the Cloud)  Client Side Encryption, Server Side Encryption, Network Traffic Protection Operating System, Network and Firewall Configuration Platform, Applications, Identity and Access Management Customer Content    Principle of Least Privilege Granting the minimum access and permissions to a person / system required to perform a certain task\n AWS IAM  central permission management in AWS can be integrated with Active Directory and AWS Directory Service using SAML identity federation    VPC Security  Subnets  Private Subnet\nRoute Table does not have an entry to the Internet Gateway, internet is accessed using the NAT Gateway Public Subnet\nRoute Table has an entry to the Internet Gateway   Security Groups (Apply to ENIs)  Use security group membership to grant access to members of the security group   Network ACLs (Apply to Subnets) VPC Connections  Internet Gateway: Connect to the internet Virtual private gateway: Connect to VPN AWS Direct Connect: Dedicated pipe VPC Peering: Connect to other VPCs NAT gateways: Allow internet traffic from private subnets   Securing Data Tier  Securing data in transit  SSL over web VPN for IPSec IPSec over AWS Direct Connect Import/Export/Snowball  AWS API calls use HTTPS/SSL by default   S3 supports ACL and policies   Securing data at rest  Server-side encryption  Amazon S3-Managed Keys (SSE-S3) KMS-Managed Keys (SSE-KMS) Customer Provided Keys (SSE-C)   Client-side encryption  KMS managed master encryption keys (CSE-KMS) Customer managed master encryption keys (CSE-C)       Storing keys  Key Management Service  Customer software-based key management Integrated with many AWS services Use directly from application Integrates with EBS, S3, RDS, RedShift, Elastic Transcoder, Workmail, EMR   AWS CloudHMS  Hardware based key management Use directly from application FIPS 140-2 compliance      Design Cost-Optimized Architectures  AWS Pricing  Pay as you go Pay less when you reserve Pay even less per unit using more   Fundamental Pricing Characteristics  Compute Pricing  Duration of using an instance Machine configuration Purchase type Number of instances Load balancing Detailed monitoring Auto Scaling Elastic IP Operating system and software packages   Storage Pricing  S3  Storage class Storage Requests Data transfer   EBS  Volumes Input/output operations per second (IOPS) Snapshots Data transfer     Data Transfer Pricing   Serverless Architecture  Reducing cost spent through using Lambda, S3, DynamoDB and API Gateway CloudFront Pricing  Traffic distribution Number of requests Data transfer out      Operational Excellence  Cycle  Prepare Operate Evolve   Design Principles  Perform operations with code Annotate documentation Make frequent, small, reversible changes Refine operations procedures frequently Anticipate failure Learn from all operational failures   Services  AWS Config\nTracks resources such as EBS volumes and EC2 instances, verifies that resources comply with configuration rules AWS CloudFormation\nConverts Yaml and Json templates into cloud resources AWS Trusted Advisor\nChecks account for best practices on security, reliability, performance, cost and service limits AWS Inspector\nChecks EC2 instances for security vulnerabilities VPC Flow Logs\nLogs network traffic AWS Cloud Trail\nLogs API calls AWS CloudWatch\nTracks metrics and triggers alarms when metrics are exceeded    Test Axioms Design Resilient Architectures  Expect \u0026lsquo;Single AZ\u0026rsquo; will never be a right answer Using AWS managed services should always be preferred Fault tolerant and high availability are not the same thing Expect that everything will fail at some point and design accordingly   Design Performant Architectures  If data is unstructured, Amazon S3 is the storage solution Use caching to strategically improve performance Know when and why to use Auto Scaling Choose the instance and database type that makes the most sense for your workload and performance need   Secure Architectures  Lock down the root user Security groups only allow. Network ACLs allow explicit deny. Prefer IAM Roles to access keys   Cost-optimized Architectures  Reserve resources to save costs Any unused CPU time is a waste of money Use the most cost-effective data storage service and class Determine the most cost-effective EC2 pricing model and instance type for each workload   Operational Excellence  IAM roles are easier and safer than keys and passwords Monitor metrics across the system Automate responses to metrics where appropriate Provide alerts for anomalous conditions    "
},
{
	"uri": "https://majdarbash.github.io/tags/school-timetabling-problem/",
	"title": "school timetabling problem",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/security/",
	"title": "security",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/seo/",
	"title": "seo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/server/",
	"title": "server",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/serverless/",
	"title": "Serverless",
	"tags": [],
	"description": "",
	"content": "Lambda  Compute service to upload and run your code AWS Lambda takes care of provisioning and managing the underlying infrastructure Lambda scales out (not up) automatically Usage  Event-driven compute service which runs code in response to events. Events could be internal AWS events. Compute service to run your code in response to HTTP requests using AWS API Gateway or API calls made using AWS SDKs.   Lambda functions are independent, 1 event = 1 function Lambda is serverless Lambda functions can trigger other lambda functions, 1 event can trigger multiple functions Architectures can get extremely complicated, AWS X-ray allows you to debug what is happening Lambda can do things globally, you can use it to back up S3 buckets to other S3 buckets, etc Lambda is cost effective No servers, no maintenance is required  Lambda is the Ultimate Extraction Layer  Data Centres Hardware Assembly Code/Protocols High-Level Languages Operating Systems Application Layer/AWS APIs AWS Lambda  Languages supported by Lambda  Node.js Java Python C# Go PowerShell  Pricing  Number of Requests Duration\n(from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms)  How to Build Alexa Skill  Skill Service  AWS Lambda   Skill Interface  Invocation Name Intent Schema Slot Type Utterances    Cloud Architecture  Traditional\nELB -\u0026gt; EC2 Instances -\u0026gt; RDS Serverless\nAPI Gateway -\u0026gt; Lambda -\u0026gt; DynamoDB  "
},
{
	"uri": "https://majdarbash.github.io/tags/service/",
	"title": "service",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/shell/",
	"title": "shell",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/software-development/",
	"title": "software development",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/solution-architect/",
	"title": "solution architect",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/squid/",
	"title": "squid",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/subversion/",
	"title": "subversion",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/supervised/",
	"title": "supervised",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/svn-commands/",
	"title": "SVN commands",
	"tags": ["terminal", "shell", "commands", "subversion", ".svn"],
	"description": "",
	"content": "Adding all un-versioned files under the current directory to the SVN:\ncommand 1:\nsvn add `svn status .|grep \u0026quot;^?\u0026quot;|awk '{print $2}'` command 2:\nsvn add . --force Adding specific files to subversion:\nsvn add file1 file2 folder1 path/to/folder2 Commit specific folder in svn:\nsvn ci -m \u0026quot;commit message\u0026quot; file1 file2 folder1 path/to/folder2  Publish Date: 2014-10-12\n "
},
{
	"uri": "https://majdarbash.github.io/tags/symfony/",
	"title": "symfony",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-assets/",
	"title": "Symfony 3: Assets",
	"tags": ["symfony", "assets", "webpack", "encore", "ready"],
	"description": "",
	"content": " Composer manages PHP dependencies NPM or Yarn would be used for Frontend assets  Webpack\nWebpack is a node.js library which can bundle your javascript and CSS files into bundles. You can ready detailed documentation about it on https://webpack.js.org.\nEncore\nEncore is a recommended library which is built on top of webpack. Encore is also written in node.js. This library makes it simpler to integrate webpack into your Symfony application.\n1) Installing the Dependency\ncomposer require symfony/webpack-encore-pack yarn add @symfony/webpack-encore --dev After running this command the following files will be changed:\n./composer.json: \u0026ldquo;symfony/webpack-encore-pack\u0026rdquo; dependency added ./package.json: dev dependencies to @symfony/webpack-encore are added ./webpack.config.js: we will define the build process parameters in this file\n2) Install bootstrap and compiling assets\nyarn add bootstrap@4.0.0 yarn add holderjs yarn add popper.js@1.12.9 yarn add jquery@3.3.1 This command will automatically add yarn.lock file which will be used to lock the versions\nduring when running yarn install command.\n3) Add the installed asset dependencies to the entries of webpack.config.js\nIn addition let\u0026rsquo;s add app.js and app.css files as well.\n./assets/css/app.css (will contain custom styling css) ./assets/js/app.js (will contain custom js code)\nNow let\u0026rsquo;s adjust the webpack.config.js to include all the files required:\n# ./webpack.config.js var Encore = require('@symfony/webpack-encore'); Encore // directory where compiled assets will be stored .setOutputPath('public/build/') // public path used by the web server to access the output path .setPublicPath('/build') .addEntry('js/app', [ './node_modules/jquery/dist/jquery.slim.js', './node_modules/popper.js/dist/popper.min.js', './node_modules/bootstrap/dist/js/bootstrap.min.js', './node_modules/holderjs/holder.min.js', './assets/js/app.js' ]) .addStyleEntry('css/app', [ './node_modules/bootstrap/dist/css/bootstrap.min.css', './assets/css/app.css' ]) .cleanupOutputBeforeBuild() .enableSourceMaps(!Encore.isProduction()) // enables hashed filenames (e.g. app.abc123.css) .enableVersioning(Encore.isProduction()) ; module.exports = Encore.getWebpackConfig(); 4) Compile the assets\n./node_modules/.bin/encore dev # compiling the assets and watch files for changes ./node_modules/.bin/encore dev --watch 5) Including styles and javascript files in the template\n# ./templates/base.html.twig \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;{% block title %}Welcome!{% endblock %}\u0026lt;/title\u0026gt; {% block stylesheets %} \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;{{ asset('build/css/app.css') }}\u0026quot;/\u0026gt; {% endblock %} \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {% block body %}{% endblock %} {% block javascripts %} \u0026lt;script src=\u0026quot;{{ asset('build/js/app.js') }}\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; {% endblock %} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-basics/",
	"title": "Symfony 3: Basics",
	"tags": [],
	"description": "",
	"content": "Installation sudo mkdir -p /usr/local/bin sudo curl -LsS https://symfony.com/installer -o /usr/local/bin/symfony sudo chmod a+x /usr/local/bin/symfony Checking requirements php ./bin/symfony_requirements "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-configuration/",
	"title": "Symfony 3: Configuration",
	"tags": [],
	"description": "",
	"content": "# Will print the bundle names and aliases php bin/console config:dump-reference # Dumps configuration options for specific bundle php bin/console config:dump-reference [extension alias] php bin/console config:dump-reference framework "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-controllers-routing-views/",
	"title": "Symfony 3: Controllers, Routing, Views",
	"tags": ["twig", "template inheritance", "symfony", "service"],
	"description": "",
	"content": "This example demonstrates routing using annotation as well as different ways to pass arguments to the controller.\n\u0026lt;?php // src/Service/TestService.php namespace App\\Service; class TestService { public function message($name) { return \u0026quot;Hello $name\u0026quot;; } } \u0026lt;?php // src/Controller/TestController namespace App\\Controller; use Symfony\\Component\\HttpFoundation\\Request; use Symfony\\Component\\HttpFoundation\\Response; use Symfony\\Component\\Routing\\Annotation\\Route; use App\\Service\\TestService; use Symfony\\Bundle\\FrameworkBundle\\Controller\\AbstractController; class TestController extends AbstractController { /** * @var TestService */ private $testService; /** * TestController constructor. */ public function __construct(TestService $testService) { $this-\u0026gt;testService = $testService; } /** * @Route(\u0026quot;/test1\u0026quot;, name=\u0026quot;test1\u0026quot;) * @param Request $request * @return Response */ public function test1(Request $request) { return new Response($this-\u0026gt;testService-\u0026gt;message( $request-\u0026gt;get('name') )); } /** * @Route(\u0026quot;/test2/{name}\u0026quot;, name=\u0026quot;test2\u0026quot;) * @param $name * @return Response */ public function test2($name) { return new Response($this-\u0026gt;testService-\u0026gt;message($name)); } } Annotation configuration can be done from:\n# config/routes/annotations.yaml controllers: resource: ../../src/Controller/ type: annotation Route annotation set on the class level. Applies defined annotation as prefix to all inner class annotations.\n\u0026lt;?php namespace App\\Controller; use Symfony\\Bundle\\FrameworkBundle\\Controller\\AbstractController; use Symfony\\Component\\HttpFoundation\\Request; use Symfony\\Component\\Routing\\Annotation\\Route; /** * @Route(\u0026quot;/blog\u0026quot;) * Class BlogController * @package App\\Controller */ class BlogController extends AbstractController { public function __construct() { } /** * @Route(\u0026quot;/\u0026quot;, name=\u0026quot;blog_index\u0026quot;) * @param Request $request * @return \\Symfony\\Component\\HttpFoundation\\Response */ public function index(Request $request) { return $this-\u0026gt;render('base.html.twig', ['message' =\u0026gt; 'index page']); } /** * @Route(\u0026quot;/hello\u0026quot;, name=\u0026quot;blog_hello\u0026quot;) * @param Request $request * @return \\Symfony\\Component\\HttpFoundation\\Response */ public function hello(Request $request) { return $this-\u0026gt;render('base.html.twig', ['message' =\u0026gt; 'hello page']); } } The index action in example above will be accessible using /blog/ url, while hello action will require /blog/hello.\nThis example shows the crud for the posts which uses session to store data.\n * @Route(\u0026quot;/blog\u0026quot;) * Class BlogController * @package App\\Controller */ class BlogController extends AbstractController { /** * @var \\Twig_Environment */ private $twig; /** * @var SessionInterface */ private $session; /** * @var RouterInterface */ private $router; public function __construct(\\Twig_Environment $twig, SessionInterface $session, RouterInterface $router) { $this-\u0026gt;twig = $twig; $this-\u0026gt;session = $session; $this-\u0026gt;router = $router; } /** * @Route(\u0026quot;/\u0026quot;, name=\u0026quot;blog_index\u0026quot;) */ public function index() { $html = $this-\u0026gt;twig-\u0026gt;render('blog/index.html.twig', [ 'posts' =\u0026gt; $this-\u0026gt;session-\u0026gt;get('posts') ]); return new Response($html); } /** * @Route(\u0026quot;/add\u0026quot;, name=\u0026quot;blog_add\u0026quot;) */ public function add() { $posts = $this-\u0026gt;session-\u0026gt;get('posts'); $posts[uniqid()] = [ 'title' =\u0026gt; 'Random title' . rand(0, 1000), 'text' =\u0026gt; 'Random text' . rand(0, 1000), ]; $this-\u0026gt;session-\u0026gt;set('posts', $posts); return new RedirectResponse($this-\u0026gt;router-\u0026gt;generate('blog_index')); } /** * @Route(\u0026quot;/show\u0026quot;, name=\u0026quot;blog_show\u0026quot;) */ public function show($id) { $posts = $this-\u0026gt;session-\u0026gt;get('posts'); if (!$posts || !isset($posts['id'])) { throw new NotFoundHttpException('Post not found'); } $html = $this-\u0026gt;twig-\u0026gt;render('blog/post.html.twig', [ 'id' =\u0026gt; $id, 'post' =\u0026gt; $posts[$id] ]); return new Response($html); } } Template Inheritance: Twig parent template can be inherited and reused.\nbase.html.twig\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;{% block title %}Welcome!{% endblock %}\u0026lt;/title\u0026gt; {% block stylesheets %}{% endblock %} \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {% block body %} {{ message }} {% endblock %} {% block javascripts %}{% endblock %} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; # child.html.twig {% extends 'base.html.twig' %} {% block stylesheets %} {{ parent() }} \u0026lt;link type=\u0026quot;text/css\u0026quot; href=\u0026quot;{{ asset('css/child.css') }}\u0026quot; rel=\u0026quot;stylesheet\u0026quot;/\u0026gt; {% endblock %} {% block title %}{{ page_title }}{% endblock %} {% block body %} here we go {% block body %} "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-data-fixtures/",
	"title": "Symfony 3: Data fixtures",
	"tags": ["symfony", "ready", "fixtures", "data fixtures", "doctrine"],
	"description": "",
	"content": "1) Installation\ncomposer require --dev doctrine/doctrine-fixtures-bundle 2) Fixtures class\n\u0026lt;?php # ./src/DataFixtures/PostFixtures.php namespace App\\DataFixtures; use App\\Entity\\Post; use Doctrine\\Bundle\\FixturesBundle\\Fixture; use Doctrine\\Common\\Persistence\\ObjectManager; class PostFixtures extends Fixture { public function load(ObjectManager $manager) { $plan = new Post(); $plan-\u0026gt;setName('My first post'); $plan-\u0026gt;setDescription('This is my first post'); $manager-\u0026gt;persist($plan); $manager-\u0026gt;flush(); } } 3) Loading fixtures\nphp bin/console doctrine:fixtures:load php bin/console doctrine:fixtures:load --purge-with-truncate "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-error-handling/",
	"title": "Symfony 3: Error Handling",
	"tags": [],
	"description": "",
	"content": "throw new NotFoundHttpException('Post not found');\nThis exception will be handled using default exception handler, which will try to locate a template from twig-bundle/Resources/views/Exception/error...html.twig.\nIt will first try to locate a template using the following order:\nerror[error_code].html.twig error.json.twig error.html.twig These templates can be customized when mimic the structure of the bundle templates.\n{# templates/bundles/TwigBundle/Exception/error404.html.twig #} {% extends 'base.html.twig %} {% block body %} \u0026lt;h1\u0026gt;Page not found.\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;{{ status_text }}, {{status_code}}\u0026lt;/p\u0026gt; {% endblock %} "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-flash-messages/",
	"title": "Symfony 3: Flash messages",
	"tags": ["twig", "symfony", "ready", "templates", "flash messages", "error", "notice"],
	"description": "",
	"content": "1) Setting the message\n\u0026lt;?php namespace App\\Controller; use Symfony\\Bundle\\FrameworkBundle\\Controller\\Controller; use Symfony\\Component\\HttpFoundation\\Session\\Flash\\FlashBagInterface; class FlashBagController extends Controller { /** * @var FlashBag */ private $flashBag; public function __construct(FlashBagInterface $flashBag) { $this-\u0026gt;flashBag = $flashBag; } /** * @Route('/index', name=\u0026quot;flash_index\u0026quot;) */ public function index() { return $this-\u0026gt;render('flash/index.html.twig'); } /** * @Route('/setFlash') */ public function setFlashMessage() { $this-\u0026gt;flashBag-\u0026gt;add('notice', 'Here is the flash message.'); $this-\u0026gt;redirectToRoute('flash_index'); } } 2) Reading the message in the template\n{# ./templates/flash/index.html.twig #} {% for message in app.flashes('notice') %} \u0026lt;div class=\u0026quot;alert alert-info\u0026quot;\u0026gt; {{ message }} \u0026lt;/div\u0026gt; {% endfor %} "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-forms/",
	"title": "Symfony 3: Forms",
	"tags": ["symfony", "ready", "form", "form class", "validation", "formBuilder"],
	"description": "",
	"content": "The code-snippets in this article are obtained from: https://symfony.com/doc/current/forms.html\n1) Installation\ncomposer require symfony/form 2) Building form in controller\n\u0026lt;?php // src/Entity/Task.php namespace App\\Entity; class Task { protected $task; protected $dueDate; public function getTask() { return $this-\u0026gt;task; } public function setTask($task) { $this-\u0026gt;task = $task; } public function getDueDate() { return $this-\u0026gt;dueDate; } public function setDueDate(\\DateTime $dueDate = null) { $this-\u0026gt;dueDate = $dueDate; } } \u0026lt;?php // src/Controller/DefaultController.php namespace App\\Controller; use App\\Entity\\Task; use Symfony\\Bundle\\FrameworkBundle\\Controller\\Controller; use Symfony\\Component\\HttpFoundation\\Request; use Symfony\\Component\\Form\\Extension\\Core\\Type\\TextType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\DateType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\SubmitType; class DefaultController extends Controller { public function new(Request $request) { // creates a task and gives it some dummy data for this example  $task = new Task(); $task-\u0026gt;setTask(\u0026#39;Write a blog post\u0026#39;); $task-\u0026gt;setDueDate(new \\DateTime(\u0026#39;tomorrow\u0026#39;)); $form = $this-\u0026gt;createFormBuilder($task) -\u0026gt;add(\u0026#39;task\u0026#39;, TextType::class) -\u0026gt;add(\u0026#39;dueDate\u0026#39;, DateType::class) -\u0026gt;add(\u0026#39;save\u0026#39;, SubmitType::class, array(\u0026#39;label\u0026#39; =\u0026gt; \u0026#39;Create Task\u0026#39;)) -\u0026gt;getForm(); $form-\u0026gt;handleRequest($request); if ($form-\u0026gt;isSubmitted() \u0026amp;amp;\u0026amp;amp; $form-\u0026gt;isValid()) { // $form-\u0026gt;getData() holds the submitted values  // but, the original `$task` variable has also been updated  $task = $form-\u0026gt;getData(); // ... perform some action, such as saving the task to the database  // for example, if Task is a Doctrine entity, save it!  // $entityManager = $this-\u0026gt;getDoctrine()-\u0026gt;getManager();  // $entityManager-\u0026gt;persist($task);  // $entityManager-\u0026gt;flush();  return $this-\u0026gt;redirectToRoute(\u0026#39;task_success\u0026#39;); } return $this-\u0026gt;render(\u0026#39;default/new.html.twig\u0026#39;, array(\u0026#39;form\u0026#39; =\u0026gt; $form-\u0026gt;createView(),)); } } {# templates/default/new.html.twig #} {{ form_start(form) }} {{ form_widget(form) }} {{ form_end(form) }} {# form_start - renders the start tag of the form, with enctype form_widget - fields, validation errors form_end - eng tag of the form + automatic CSRF protection #} \u0026lt;!-- templates/default/new.html.php --\u0026gt; \u0026lt;?php echo $view[\u0026#39;form\u0026#39;]-\u0026gt;start($form) ?\u0026gt; \u0026lt;?php echo $view[\u0026#39;form\u0026#39;]-\u0026gt;widget($form) ?\u0026gt; \u0026lt;?php echo $view[\u0026#39;form\u0026#39;]-\u0026gt;end($form) ?\u0026gt; {% endraw %} 3) Validation\ncomposer require symfony/validator Annotation is used for validation\n\u0026lt;?php // src/Entity/Task.php  namespace App\\Entity; use Symfony\\Component\\Validator\\Constraints as Assert; /** * Class Task * @property string $task * @property \\DateTime $dueDate * @package App\\Entity */ class Task { /** * @Assert\\NotBlank() */ protected $task; /** * @Assert\\NotBlank() * @Assert\\Type(\u0026#34;\\DateTime\u0026#34;) */ protected $dueDate; /** * @return string */ public function getTask(): string { return $this-\u0026gt;task; } /** * @param string $task */ public function setTask($task): void { $this-\u0026gt;task = $task; } /** * @return \\DateTime */ public function getDueDate(): \\DateTime { return $this-\u0026gt;dueDate; } /** * @param \\DateTime $dueDate */ public function setDueDate(\\DateTime $dueDate): void { $this-\u0026gt;dueDate = $dueDate; } } {# templates/default/new.html.twig #} {{ form_start(form, {\u0026#39;attr\u0026#39;: {\u0026#39;novalidate\u0026#39;: \u0026#39;novalidate\u0026#39;}}) }} {{ form_widget(form) }} {{ form_end(form) }} Symfony Field Types https://symfony.com/doc/current/reference/forms/types.html\nPassing options to Fields\n-\u0026gt;add(\u0026#39;dueDate\u0026#39;, DateType::class, array( \u0026#39;widget\u0026#39; =\u0026gt; \u0026#39;single_text\u0026#39;, \u0026#39;required\u0026#39; =\u0026gt; true // applies client-side validation if enabled, \u0026#39;label\u0026#39; =\u0026gt; \u0026#39;Due Date\u0026#39;, )) -\u0026gt;add(\u0026#39;task\u0026#39;, null, array(\u0026#39;attr\u0026#39; =\u0026gt; array(\u0026#39;maxlength\u0026#39; =\u0026gt; 4))) Creating Form Class\n\u0026lt;?php // src/Form/TaskType.php  namespace App\\Form; use App\\Entity\\Task; use Symfony\\Component\\Form\\AbstractType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\CheckboxType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\SubmitType; use Symfony\\Component\\Form\\FormBuilderInterface; use Symfony\\Component\\OptionsResolver\\OptionsResolver; class TaskType extends AbstractType { public function buildForm(FormBuilderInterface $builder, array $options) { $builder -\u0026gt;add(\u0026#39;task\u0026#39;) -\u0026gt;add(\u0026#39;dueDate\u0026#39;, null, [\u0026#39;widget\u0026#39; =\u0026gt; \u0026#39;single_text\u0026#39;]) -\u0026gt;add(\u0026#39;agreeTerms\u0026#39;, CheckboxType::class, [\u0026#39;mapped\u0026#39; =\u0026gt; false]) -\u0026gt;add(\u0026#39;save\u0026#39;, SubmitType::class); } public function configureOptions(OptionsResolver $resolver) { // indicates the type of class which holds data  $resolver-\u0026gt;setDefaults([ \u0026#39;data_class\u0026#39; =\u0026gt; Task::class ]); } } \u0026lt;?php // src/Controller/TaskController.php  namespace App\\Controller; use App\\Form\\TaskType; use Symfony\\Component\\Routing\\Annotation\\Route; use App\\Entity\\Task; use Symfony\\Bundle\\FrameworkBundle\\Controller\\Controller; use Symfony\\Component\\Form\\Extension\\Core\\Type\\DateType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\SubmitType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\TextType; use Symfony\\Component\\HttpFoundation\\Request; class TaskController extends Controller { /** * @Route(\u0026#34;/task/new2\u0026#34;) */ public function newFromFormClass(Request $request) { $task = new Task(); $task-\u0026gt;setTask(\u0026#34;Here is the new task\u0026#34;); $task-\u0026gt;setDueDate(new \\DateTime(\u0026#39;tomorrow\u0026#39;)); $form = $this-\u0026gt;createForm(TaskType::class, $task); $form-\u0026gt;handleRequest($request); if ($form-\u0026gt;isSubmitted() \u0026amp;amp;\u0026amp;amp; $form-\u0026gt;isValid()) { $task = $form-\u0026gt;getData(); echo $form-\u0026gt;get(\u0026#39;agreeTerms\u0026#39;)-\u0026gt;getData(); return $this-\u0026gt;redirectToRoute(\u0026#39;task_success\u0026#39;); } return $this-\u0026gt;render(\u0026#39;task/new.html.twig\u0026#39;, [ \u0026#39;form\u0026#39; =\u0026gt; $form-\u0026gt;createView() ]); } } "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-overview/",
	"title": "Symfony 3: Overview",
	"tags": [],
	"description": "",
	"content": "Symfony is a PHP framework for creating websites and web applications. Symfony provides you with a set of reusable components: Validators, HTTP Kernel, etc. Symfony Embraces best software development practices.\nSymfony was first released in 2005. The framework releases have got LTS - Long Term Support and with the help of the huge community and documentaion you are not alone! What is noticably great is that symfony uses best practices for application development.\nOne of the prominent default symfony components: Twig templating engine, Doctrine 2 (database ORM). You can use any other bundles (libraries) in symfony.\nLet\u0026rsquo;s get some terminology here:\nConsole\nThe Symfony framework provides lots of commands through the bin/console script (e.g. the well-known bin/console cache:clear command). These commands are created with the Console component. You can also use it to create your own commands.\nEvent Dispatcher\nDuring the execution of a Symfony application, lots of event notifications are triggered. Your application can listen to these notifications and respond to them by executing any piece of code.\nBundle\nA bundle is similar to a plugin in other software, but even better. The core features of Symfony framework are implemented with bundles (FrameworkBundle, SecurityBundle, DebugBundle, etc.) They are also used to add new features in your application via third-party bundles.\nService\nYour application is full of useful objects: a \u0026ldquo;Mailer\u0026rdquo; object might help you send emails while another object might help you save things to the database. Almost everything that your app \u0026ldquo;does\u0026rdquo; is actually done by one of these objects. And each time you install a new bundle, you get access to even more!\nService Container\nThis is where your services live. Service container will centralize the way services are constructed.\n"
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-services/",
	"title": "Symfony 3: Services",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s get some terminology straight.\nService is an object which performs specific function - sending email, caching, etc. Service container will control how services are constructured and are used in Dependency Injection.\nBased on DI practices objects will be passed, which makes the code cleaner and writing tests easier.\n# Displays full list of defined services php bin/console debug:container services.yaml Contains service configuration. Here you will find paths defined to be used as services. In addition you can use services_[env].yaml for environment specific configuration.\nService is a class which represents the business logic. To utilize Dependency Injection, Interfaces classes are passed as arguments to the class constructors. In this aspect symfony will choose appropriate Service which implements an interface and inject this dependency.\n# Displays what service is associated with the interface php bin/console debug:autowiring LoggerInterface # Displays all services and corresponding interfaces php bin/console debug:autowiring # Displays information about the service php bin/console debug:container monolog.logger Auto-Wire Autowiring is responsible for injecting dependencies in your services. By default you will find autowiring enabled (services.yaml). If autowiring is disabled you will have to inject dependencies manually through defining them in services.yaml as:\n[Class name]: [Injected Dependency Name]\n# Injects Greeting service to BlogController App\\Controller\\BlogController: [\u0026#39;@App\\Service\\Greeting\u0026#39;] # Injects Logger service to Greeting service (constructor) App\\Service\\Greeting: [\u0026#39;@monolog.logger\u0026#39;] Auto-Configure When set to true symfony automatically adds tags to classes implementing certain interfaces. In addition symfony automatically registers your services as commands, event subscribers, etc.\nSample services.yaml file:\n# This file is the entry point to configure your own services. # Files in the packages/ subdirectory configure your dependencies. # Put parameters here that don\u0026#39;t need to change on each machine where the app is deployed # https://symfony.com/doc/current/best_practices/configuration.html#application-related-configuration parameters: locale: \u0026#39;en\u0026#39; services: # default configuration for services in *this* file _defaults: autowire: true # Automatically injects dependencies in your services. autoconfigure: true # Automatically registers your services as commands, event subscribers, etc. public: false # Allows optimizing the container by removing unused services; this also means # fetching services directly from the container via $container-\u0026gt;get() won\u0026#39;t work. # The best practice is to be explicit about your dependencies anyway. # makes classes in src/ available to be used as services # this creates a service per class whose id is the fully-qualified class name App\\: resource: \u0026#39;../src/*\u0026#39; exclude: \u0026#39;../src/{Entity,Migrations,Tests,Kernel.php}\u0026#39; # controllers are imported separately to make sure services can be injected # as action arguments even if you don\u0026#39;t extend any base controller class App\\Controller\\: resource: \u0026#39;../src/Controller\u0026#39; tags: [\u0026#39;controller.service_arguments\u0026#39;] # add more service definitions when explicit configuration is needed # please note that last definitions always *replace* previous ones Public / Private Services By default, services are private (public: false).\nIf you try to fetch the service using the container, service should be defined as public. This example is demonstrated as:\n/** * @required */ public function setContainer(ContainerInterface $container = null){ $container-\u0026gt;get(MyService::class); } In such example MyService should be a public service. The better way is just to use dependency injection which will make it possible to inject private services.\nTags Service tags are a way to tell Symfony or other third-party bundles that your service should be registered in specific way. For example, if tagged in with specific way we can fetch all services which are responsible for sending mail, based on the common tag used. Tags can be explicitly defined in services.yaml file under services block.\nservices: Swift_SmtpTransport: tags: [\u0026#39;app.mail_transport\u0026#39;] Swift_SendmailTransport: tags: [\u0026#39;app.mail_transport\u0026#39;] public function process(ContainerBuilder $container) { $taggedServices = $container-\u0026gt;findTaggedServiceIds(\u0026#39;app.mail_transport\u0026#39;); } Another way is when symfony defines the tags based on autoconfigure option being set to true. In such case, command extending from Console\\Command will automatically be tagged with console.command tag, tag makes it available under php bin/console command.\n\u0026lt;?php namespace App\\Command; \u0026lt;/pre\u0026gt; \u0026lt;pre\u0026gt;use Symfony\\Component\\Console\\Command\\Command; class UtilsCommand extends Command { protected function configure(){ $this-\u0026gt;setName(\u0026#39;app:utils\u0026#39;) -\u0026gt;setDescription(\u0026#39;Utils to be run in console\u0026#39;) -\u0026gt;addArgument(\u0026#39;name\u0026#39;, InputArgument::REQUIRED); } .. } # UtilsCommand is tagged with console.command php bin/console debug:container \u0026#39;App\\Command\\UtilsCommand\u0026#39; # app:utils is displayed based on share console.command tag php bin/console Manual service wiring, parameter binding Example below demonstrates wiring $message argument in the Greeting service.\n\u0026lt;?php namespace App\\Service; use Psr\\Log\\LoggerInterface; class Greeting { /** * @var LoggerInterface */ private $logger; /** * @var string */ private $message; public function __construct(LoggerInterface $logger, string $message) { $this-\u0026gt;logger = $logger; $this-\u0026gt;message = $message; } public function greeting(string $name): string { $this-\u0026gt;logger-\u0026gt;info(\u0026#34;{$this-\u0026gt;message}$name\u0026#34;); return \u0026#34;Hello $name\u0026#34;; } }\u0026lt;/pre\u0026gt; \u0026lt;p\u0026gt;Wiring service:\u0026lt;/p\u0026gt; \u0026lt;pre\u0026gt;# config/services.yaml services: App\\Service\\Greeting: arguments: $message: \u0026#39;hello from service\u0026#39; Inject our service using parameter:\n# config/services.yaml parameters: locale: \u0026#39;en\u0026#39; hello_message: \u0026#39;hello from service\u0026#39; services: App\\Service\\Greeting: arguments: $message: \u0026#39;%hello_message%\u0026#39; Binding (in this case all the parameters in any service will be wired automatically):\n# config/services.yaml parameters: locale: \u0026#39;en\u0026#39; hello_message: \u0026#39;hello from service\u0026#39; services: _defaults: autowire: true autoconfigure: true public: false bind: $message: \u0026#39;%hello_message%\u0026#39;\u0026lt;/pre\u0026gt; \u0026lt;p\u0026gt;Another great feature allow wiring based on type:\u0026lt;/p\u0026gt; \u0026lt;pre\u0026gt;# config/services.yaml services: _defaults: autowire: true autoconfigure: true public: false bind: App\\Service\\SomeInterface: \u0026#39;@some_service\u0026#39; "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfonycode-structure/",
	"title": "Symfony 3:[Code Structure]",
	"tags": [],
	"description": "",
	"content": "myproject/ config/ src/ Controller/ Entity/ Form/ Migrations/ Model/ Repository/ Service/ [components] templates/ Description:\nEntity/: Refers to the Persistence Layer\nExample:\nLet\u0026rsquo;s assume we have an entity Product, consisting of id and name.\n1) Start by creating Product directory\nA bundle is simply a structured set of files within a directory that implement a single feature. Each directory contains everything related to that feature, including PHP files, templates, stylesheets, JavaScripts, tests and anything else. Every aspect of a feature exists in a bundle and every feature lives in a bundle.\n"
},
{
	"uri": "https://majdarbash.github.io/symfony/symfonytwig-reference/",
	"title": "Symfony 3:[Twig]: Reference",
	"tags": ["twig", "symfony", "ready", "template", "filter", "twig inheritance", "for loop", "twig filter"],
	"description": "",
	"content": "Template file name\n[template_name].html.twig {# comment #} Template block\n{% block [block_name] %} [block_content] {% endblock %} Extending twig template\n{% extends [template_name] %} Output a variable\n{{ [variable_name] }} For loop\n{% for post in posts %} {{ post.title }}:\u0026amp;nbsp;{{ post.text }} {% endfor %} Generate URL from annotated route name\n{{ path([route_name]) }} Twig Filters\n# Uppercase conversion: {{ post.title|upper }} # Date conversion: {{ post.data|date('H:i:s') }} Environment variables\n{{ app.environment }} {{ app.user }} {{ app.request }} {{ app.session.isStarted() }} {{ app.debug }} Example: Extending Twig - Creating Filter \u0026lt;?php namespace App\\Twig; use Twig\\Extension\\AbstractExtension; use Twig\\TwigFilter; class AppExtension extends AbstractExtension { public function getFilters() { return [ new TwigFilter('price', [$this, 'priceFilter']) ]; } public function priceFilter($number) { return ' The price filter added above will be used as:\n{{ 1029|price }} Extension is automatically configured and \u0026lsquo;twig.extension\u0026rsquo; tag is added based on inheritance from AbstractExtension class (and autoconfigure: true in services.yaml)\nGlobal Variables in Twig\n# config/packages/twig.yaml twig: paths: ['%kernel.project_dir%/templates'] debug: '%kernel.debug%' strict_variables: '%kernel.debug%' globals: message: '%hello_message%' Here %hello_message% is defined as parameter in services.yaml and is not available as global variable \u0026ldquo;message\u0026rdquo; within twig templates.\n. number_format($number, 2, '.', ','); } } \nThe price filter added above will be used as:\nExtension is automatically configured and \u0026lsquo;twig.extension\u0026rsquo; tag is added based on inheritance from AbstractExtension class (and autoconfigure: true in services.yaml)\nGlobal Variables in Twig\nHere %hello_message% is defined as parameter in services.yaml and is not available as global variable \u0026ldquo;message\u0026rdquo; within twig templates.\n"
},
{
	"uri": "https://majdarbash.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/template/",
	"title": "template",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/template-inheritance/",
	"title": "template inheritance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/templates/",
	"title": "templates",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/terminal/",
	"title": "terminal",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/testing/",
	"title": "testing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/the-netflix-simian-army/",
	"title": "The Netflix Simian Army: reliability, security, resiliency and recoverability",
	"tags": [],
	"description": "",
	"content": " Along with price and scalability, redundancy and fault-tolerance are possibly the most important triggers driving cloud migration. The cloud architecture should allow failure without affecting the availability of the entire system. We want to be able to test the failure scenarios.  Chaos Monkey  Randomly disables production instances Testing ability to survive the failure without overall impact on the service Leads to building automatic recovery mechanism to deal with system failures  Latency Monkey  Induces artificial delays to RESTful client-server communication layer to simulate service degradation. Measures if upstream services respond appropriately. Simulate a node or an entire service downtime without physically bringing these instances down.  Conformity Monkey  Finds instances that don\u0026rsquo;t adhere to best-practices and shut them down.  Doctor Monkey  Detecting unhealthy instances using health checks and other external signs of health. Removes unhealthy instances from service.  Janitor Monkey  Searches for unused resources and disposes them.  Security Monkey  Finds security violations and vulnerabilities and terminates the offending instances.  10-18 Monkey (Localization / Internalization)  Detects configuration and run time problems in instances serving customers in different multiple geographic regions.  Chaos Gorilla  Simulates an outage of an entire Amazon availability zone. Services should re-balance to the functional AZs without user-visible impact or manual intervention.  The Simian Army project on Github has retired and the functionality has been moved to other Netflix projects. Check the Simian Army Github page to find more details about hte new projects.\n https://github.com/Netflix/SimianArmy https://github.com/netflix/chaosmonkey https://github.com/spinnaker/swabbie  "
},
{
	"uri": "https://majdarbash.github.io/tags/travis/",
	"title": "travis",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/travis-ci/",
	"title": "Travis CI Review",
	"tags": ["testing", "travis", "ci", "continuous integration", "software development", "review"],
	"description": "",
	"content": "Is Travis CI a good CI tool? Travis CI is a very popular and widely used tool. I was personally using Travis for quite a while and had a good experience.\nThere are few things you should consider before starting with Travis. It\u0026rsquo;s very easy to start with Travis. I cannot imagine a faster integration with a CI tool.\nAll you have to do in order to start is to add your github repo to travis. Hence, you can do this after creating Travis account by authenticating Travis app to access your github repo. From Travis CI settings you will choose if you want travis to run tests for every push and for which branches.\nAll the configurations of the test environment are specified in travis.yml file.\nThe key factor in the pricing model of Travis is the number of concurrent jobs. In terms of price Travis CI is not considered to be the most effective solution, as you will have to spend around 129 / month USD for 2 concurrent jobs (i.e. 2 builds running at the same time: https://travis-ci.com/plans).\nIf you are building an Open Source project, you can still use Travis CI for free, which will navigate you to travis-ci.org (free version of travis-ci.com).\nTravis.yml A sample travis.yml file, would contain information like below:\n# php language is used language: php\nthis will enable artifacts addon, which will be sent to you with every email addons: artifacts: true\nusing php 5.5 version php:\n 5.5  before_script:\nhere you list the commands to be run before the build usually you might want to install some packages and prepare your build for running notifications:\nin this section you would specify different methods how you want to be notified you can choose slack, email and other methods after_failure:\nin this section you might choose to do a certain action after_failure (e.g. upload artifacts, etc\u0026hellip;) As you can see the installation script will take around 30 minutes to setup. However you may not feel that you have a full control of the testing environment configuration, as you will be bound to certain underlying structure provided by Travis CI. Sometimes when the test fails you would like to login to the test instance and investigate the exact cause of the failure - this is another thing you would not be able to enjoy with Travis.\nResources Check out here to find out more information about Travis CI:\nhttp://travis-ci.com\nhttps://travis-ci.com/plans\nhttps://docs.travis-ci.com/user/customizing-the-build/\n"
},
{
	"uri": "https://majdarbash.github.io/tags/twig/",
	"title": "twig",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/twig-filter/",
	"title": "twig filter",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/twig-inheritance/",
	"title": "twig inheritance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/types-of-learning/",
	"title": "types of learning",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/ubuntu/",
	"title": "ubuntu",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/ubuntu-server/",
	"title": "ubuntu server",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/ubuntu-server-clock-synchronisation-using-ntpd/",
	"title": "Ubuntu Server clock synchronisation using NTP",
	"tags": ["ubuntu", "server", "clock"],
	"description": "",
	"content": "NTP is Network Time Protocol. This protocol is used for clock synchronisation between computer systems. NTP is one of the oldest protocols which are still used on the Internet.\nThe scenario below explains the use of NTP:\nWhile the server is running, the system clock will gradually slow or run faster which will lead to clock misalignment. Whether you are running multiple systems over a load balancer, or just multiple systems in parallel, it\u0026rsquo;s very important to make sure that they are all on the same clock, interpreting and processing the records with the same current time. As simple as it sounds before NTP this was a very challenging task.\nThe concept of NTP is to synchronise all computer clocks over some reference point. Having NTPd - being a Network Time Protocol daemon running on all the systems, you will assure that all your system clocks are synchronised against one reference point.\nNTPd installation on Ubuntu / Ubuntu Server 14.04 sudo ntpdate -s ntp.ubuntu.com; sudo apt-get install ntp; sudo service ntp reload; sudo service apache2 reload;\nIf you would like to execute this algorithm for several nodes over the network, you can do as follows:\nsudo ssh -i key.pem ubuntu@machine1 \u0026lsquo;sudo ntpdate -s ntp.ubuntu.com; sudo apt-get install ntp; sudo service ntp reload; sudo service apache2 reload;\u0026rsquo; ;\nsudo ssh -i key.pem ubuntu@machine2 \u0026lsquo;sudo ntpdate -s ntp.ubuntu.com; sudo apt-get install ntp; sudo service ntp reload; sudo service apache2 reload;\u0026rsquo; ;\nsudo ssh -i key.pem ubuntu@machine3 \u0026lsquo;sudo ntpdate -s ntp.ubuntu.com; sudo apt-get install ntp; sudo service ntp reload; sudo service apache2 reload;\u0026rsquo; ;\n"
},
{
	"uri": "https://majdarbash.github.io/random/create-user-using-terminal-ubuntu/",
	"title": "Ubuntu Terminal - adding user with SSH access",
	"tags": [],
	"description": "",
	"content": "Objective In the following example I\u0026rsquo;m doing the following\n create a new user using ubuntu terminal add my public key to authorized keys add user to sudoers (to be able to use sudo) enable ssh access for this user  Scripts # creating user adduser [username]\nadding user to sudo (administrative privelleges) visudo\nsearch for a line \u0026ldquo;root ALL=(ALL:ALL) ALL\u0026rdquo; and add a new line there [username] ALL=(ALL:ALL) ALL\ndelete a user deluser [username]\nwhen deleting the user, the line added in visudo should be also deleted delete home directory of the user along with the account deletion deluser \u0026ndash;remove-home [username]\n# adding user without password, with ssh access and sudo\n1. adding user without password adduser \u0026ndash;disable-password \u0026ndash;gecos \u0026quot;\u0026rdquo; [username]\n2. creating ssh user and supplying the public key mkdir /home/[username]/.ssh touch -f /home/[username]/.ssh/authorized_keys\necho \u0026ldquo;[public key contents]\u0026rdquo; \u0026raquo; /home/[username]/.ssh/authorized_keys\necho \u0026ldquo;AllowUsers [username]\u0026rdquo; \u0026raquo; /etc/ssh/sshd_config sudo service ssh reload\n3. giving sudo access echo \u0026ldquo;[username] ALL=(ALL) NOPASSWD:ALL\u0026rdquo; \u0026raquo; /etc/sudoers.d/[username]\n"
},
{
	"uri": "https://majdarbash.github.io/tags/unsupervised/",
	"title": "unsupervised",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/validation/",
	"title": "validation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/vcs/",
	"title": "vcs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/version-control-system/",
	"title": "version control system",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/vpcs/",
	"title": "VPCs",
	"tags": [],
	"description": "",
	"content": "VPC Components  Virtual Private Cloud Subnet Internet Gateway NAT Gateway Virtual Private Gateway Peering Connection VPC Endpoints Egress-only Internet Gateway  VPC (Virtual Private Cloud)  VPC (Virtual Private Cloud) is a virtual private network dedicated to your AWS account. Logically isolated from other virtual networks in AWS Cloud VPC is defined on Region level VPC Tenancy  Default Dedicated   By default, VPC is created with Route Table, Network ACL and Security Group Availability Zones a, b and c are randomized per account Default VPC  All Subnets in default VPC have a route out to the internet Each EC2 instance has both a public and private IP address    Subnet  Subnet is defined on AZ level IP Ranges which can be assigned to Subnets  10.0.0.0-10.255.255.255 (/8 prefix) - largest network 172.16.0.0-172.31.255.255 (/12 prefix) 192.168.0.0-192.168.255.255 (/16 prefix)   5 IP addresses are reserved\n(first four and the last one)  Network address (10.0.0.0) Reserved by AWS for VPC router (10.0.0.1) DNS server IP (10.0.0.2) For future use (10.0.0.3) Network broadcast address - is not supported in VPC, so reserved (10.0.0.255)   Allowed block size is between /16 and /28 netmask AWS Resources can be launched into Subnets Public IPs can be auto-assigned to resources in Subnet  IGW (Internet Gateway)  IGW is a virtual router providing VPC connectivity to the Internet IGW can be attached to VPC You can have only one IGW per VPC  RTB (Route Table)  Route Tables belong to a VPC Route Tables can be associated with Subnets Subnets which are not associated with any Route Table will be associated with the main Route Table It\u0026rsquo;s a better practice to keep the main Route Table as private and create the public Route Tables on demand Route\u0026rsquo;s status \u0026ldquo;blackhole\u0026rdquo; is a route that goes nowhere  VPC Peering  Allows you to connect one VPC with another via a direct network route using private IP addresses Instances behave as if they were on the same private network You can peer VPC\u0026rsquo;s with other AWS accounts as well as with other VPCs in the same account No Transitive Peering - VPC Peering should be defined between each of VPCs that should be connected  NAT Instances, NAT Gateways  NAT = Network Address Translations NAT Instances, NAT Gateways - can provide connectivity to the Internet for private resources, without the need of being public, i.e. having a public IP NAT Instance:  NAT Instance is a single point of failure NAT instance should be in a public subnet The amount of traffic that NAT instances can support depends on the instance size Launching NAT instance  Use EC2 instance with community AMI Disable Source / Destination check\nEach EC2 instance performs source/destination checks by default. This means that the instance must be the source or destination of any traffic it sends or receives. However, a NAT Instance must be able to send and receive traffic when the source or destination is not itself. Therefore, you must disable these checks on the NAT Instance. Add a route in Route Table to provide the Internet connectivity:\n0.0.0.0/0 -\u0026gt; ENI of the NAT Instance (Elastic Network Interface)     NAT Gateway  Created within the Subnet Redundant within the Availability Zone Elastic IP is assigned to the NAT Gateway The Route should be added to Route Table.\n0.0.0.0/0 -\u0026gt; NAT Gateway Starts with a throughput of 5Gbps and scales up to 45Gbps Not associated with Security Groups For Availability Zone-independent architecture, create a NAT gateway in each AZ and configure routing to ensure that resources use the NAT gateway in the same AZ    Network ACLs (Network Access Control Lists)  Network ACLs are created in VPCs Default NACL, created with VPC allows all inbound and outbound traffic By default each custom NACL denies all inbound and outbound traffiic, until you add rules Network ACLs are associated with several Subnets. When associating a network ACL with a Subnet, the previous Subnet\u0026rsquo;s association is removed. Subnet without NACL association will be automatically associated with the default NACL in VPC Recommended adding rules numbers as increments of 100 NACL Rules are applied starting from the lower numbered rule Each Subnet can be associated with only one NACL Inbound and Outbound Rules Ephemeral Ports are allocated automatically and typically used by TCP, UDP as port assignment for the client end of a client-server communication to a well-known port on a server Ephemeral ports should be Allowed in NACL\u0026rsquo;s Outbound Rules for NAT Gateway to work  NAT Gateway uses Ephemeral ports 1024-65535   Network Access Control Lists (ACLs) and Security Groups provide security on different levels Security Groups are stateful\n(Return traffic is automatically allowed, regardless of any rules) Network Access Control Lists are Stateless\n(Return traffic must be explicitly allowed by rules) You can block IP address in NACL - this cannot be done in SGs (Security Groups)  VPC Flow Logs  Enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data is stored using Amaazon CloudWatch Logs. Flow log is created from VPC list by selecting the VPC and choosing \u0026ldquo;Create flow log\u0026rdquo; from the actions You can filter traffic type in Flow Log: Accepted, Rejected or All Traffic Flow Log can send traffic to S3 bucket or CloudWatch Log  Destination Log Group has to be created from CloudWatch / Logs   You cannot enable flow logs for VPCs that are peered with your VPC unless the peer VPC is in your account You cannot tag a flow log After creating a flow log you cannot change its configuration Not all IP Traffic is monitored  Traffic to Amazon DNS server is not logged Traffic generated by a Windows Instance for Amazon Windows license activation Traffic to and from 169.254.169.254 for instance metadata DHCP traffic Traffic to the reserved IP addresses for the default VPC router    Bastion Host Special purpose computer on a network specifically designed and configured to withstand attacks.\n Community AMIs are available for Bastion Hosts A Bastion is used to securely administer EC2 instances (Using SSH or RDP). Bastions are sometimes called Jump Boxes.  Direct Connect AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS.\nIt\u0026rsquo;s a direct connection through dedicated lines from the Customer location to Direct Connect Location in AWS Data Centers.\n Direct Connect directly connects your data center to AWS Useful for high throughput workload Or if you need a stable and reliable secure connection  VPC Endpoints  VPC Endpoint enables you to privately connect your VPC supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT devices, VPN connection or AWS Direct Connect Connection. Traffic between your VPC and the other service does not leave the Amazon network. Two types of VPC Endpoints  Interface Endpoints Gateway Endpoints  S3 DynamoDB     VPN Endpoint can be added from VPC/Endpoints in the management console Endpoints are added in the VPC, with routes being added to the Route Tables.  FAQs  There are no additional charges for creating and using the VPC itself If you connect your VPC to your corporate data center using the optional hardware VPN connection, pricing is per VPN connection-hour Data transfer charges are not incurred when accessing AWS services like Amazon S3 via your VPC\u0026rsquo;s Internet gateway An Internet gateway is horizontally-scaled, redundant and highly available. It imposes no bandwidth constraints. Traffic between two EC2 instances communicating using public IP addresses in the same AWS Region stays within the AWS network. Traffic between two EC2 instances located in 2 different regions with VPC peering stays with the AWS network. Without VPC Peering connection between VPCs traffic is not guaranteed to stay within the AWS network. Site-to-Site VPN connection connects your VPC to your data center. This can be done using IPSec VPN connection. Internet Gateway is not required to establish a Site-to-Site VPN connection. Amazon VPCs support 5 IP ranges: one primary and four secondary IPv4. Each of these ranges can be between /28 and /16 size (in CIDR notation). For IPv6, the VPC is a fixed size of /56 (in CIDR notation). A VPC can have both IPv4 and IPv6 blocks attached to it. Size of the VPC can be changed by expanding your existing VPC and adding the secondary IPv4 ranges. You can create 200 subnets per VPC - the limit can be adjusted through a request. The minimum size of the subnet is /28 (14 IPs v4). For IPv6 the subnet size is fixed to be /64. You can assign one or more secondary private IP address to an Elastic Network Interface (ENI) or an EC2 instance in Amazon VPC. Multiple Elastic IPs (EIP) addresses can be assigned to VPC-based Amazon EC2 instance. Each EIP address will be associated with a unique private IP address on the instance. This is applicable only for IPv4. EIPs for IPv6 are not supported at this time. Multicast and broadcast are not supported by Amazon VPC. If the instances reside in subnets in different Availability Zones, you will be charged $0.01 per GB for data transfer. Elastic Network Interfaces (ENIs) can be attached or detached from an EC2 instance while it\u0026rsquo;s running. Total number of ENIs attached depends on the instance type. ENIs and instances should be in the same AZ and VPC. VPC peers can be done between VPCs in different regions as well as VPCs belonging to different AWS accounts. AWS Direct Connect cannot be used to access VPCs peered with. VPC peering traffic within the region is not encrypted - traffic remains private and isolated. Inter-Region VPC peering traffic is encrypted. Transitive peering relationships are not supported: A-B and B-C peering does not imply A-C peering. Network Load Balancers, AWS PrivateLink, and Elastic File System cannot be used over Inter-Region VPC Peering. AWS PrivateLink and VPC Endpoints is the same thing Bring Your Own IP (BYOIP) enables customers to move all or part of their existing publicly routable IPv4 address space to AWS. They will be able to create EIPs from IP space and associate them with AWS resources. Sometimes it\u0026rsquo;s done for IP reputation, regulation and compliance reasons. Limits  5 VPCs per AWS account per region 200 subnets 5 VPC EIP addresses per AWS account per region 1 Internet Gateway per VPC    "
},
{
	"uri": "https://majdarbash.github.io/tags/vsftpd/",
	"title": "vsftpd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/web-server/",
	"title": "web server",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/webpack/",
	"title": "webpack",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/how-captcha-works/",
	"title": "What is Captcha? How does it work?",
	"tags": [],
	"description": "",
	"content": "In this article I\u0026rsquo;m explaining the generic concept of captcha in technical and non-technical terms, what are the possible threats of insecure captcha and how to make sure the captcha is secure enough.\nCAPTCHA is defined as a Completely Automated Public Turing test. The main purpose of captcha is to distinguish automated requests from natural human behavior.\nBy using the visual ability of people to distinguish patterns in the images, we can distinguish humans from the computers and bad bots which are trying to flood our databases. Captcha should strong enough and should not be solvable by any OCR or image processing system. This will make us confident that even OCR or any other alternative image processing system will not be able to solve our captcha: i.e any automated bad bot won\u0026rsquo;t be able to \u0026ldquo;pretend\u0026rdquo; to be human.\nThe concept behind the captcha is that the server knows what is passed to the client, by saving the captcha in a session or database. The expected input is then embedded to a runtime generated image and passed to the client.\nWhile the user sees the captcha and submits what he sees to the server in his consequent request, the server compares the user input with the stored expected input. In case they don\u0026rsquo;t match the captcha verification fails and the request is rejected.\nBy using this simple mechanism, gaining advantage of human image processing capabilities over most sophisticated image processing softwares, you will be able to provide the level of security required to protect your database and web assets of being misused.\nIn addition to captcha it would be useful to limit the number of requests expected from the client at a period of time to make it more difficult and time consuming for potential attackers to play around with your website.\n"
},
{
	"uri": "https://majdarbash.github.io/tags/wordpress/",
	"title": "wordpress",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/x-forwarded-for/",
	"title": "X-Forwarded-For",
	"tags": [],
	"description": "",
	"content": ""
}]