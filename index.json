[
{
	"uri": "https://majdarbash.github.io/blockchain/blockchain-foundation/",
	"title": "1 - Blockchain Foundation",
	"tags": [],
	"description": "",
	"content": "Consensys developer program\nacademy.consensys.net/developer\nHistory of blockchain\n1990s Stuart Haber / Scott Stornetta\nHow to keep the past data secure and keep digital information safe and resistant to tampering\n 1991 - First paper outlined the use of a chain of cryptographically secured blocks to preserve the integrity of past information and protect it. 1993 - Spam countermeasures 2008 - Bitcoin is born\nSatoshi Nakamoto\nReleased whitepaper:\n“Bitcoin: A Peer-to-Peer Electornic Cash System” 2014 - Ethereum\nCurrency transactions and can run computations.\nDistributed world computer running on a blockchain.\n“Ethereum Virtual Machine” (EVM) 2015-2017 - Financial interest in Bitcoin / Blockchain  From Bitcoin white paper\n\u0026ldquo;A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution.”\nTriple Entry Accounting\nIn addition to standard entries in both books of transaction credit / transaction debit it includes additional entry of recording a transaction on distributed blockchain (similar to auditing).\nLedgers\nBook of transactions\nIf you want to confirm that transaction took place you have to look into books of 2 businesses - credit to one and debit to another. Here comes the concept of double entry accounting.\nDistributed Ledgers\nKeeping copies of ledgers across the internet with specific rules to manipulate them.\nConsensus: Consensus mechanisms ensure that the distributed ledgers of a blockchain stay synchronized.\nBlockchain\nIs a set of linearly connected information-containing blocks secured with cryptography.\nNodes: the computers that run the blockchain software.\nEthereum - free to download blockchain, can be programmed. These applications run across al the nodes of the blockchain - these applications are DApps - Decentralized Applications.\nTokenization - provides a set of instructions for creating digital representation of everything from physical objects to ideas.\nEther (ETH) is the token for Ethereum. Ether allows a user to pay for transactions on the Ethereum blockchain.\n"
},
{
	"uri": "https://majdarbash.github.io/aws-csap/data-stores/",
	"title": "Data Stores",
	"tags": [],
	"description": "",
	"content": " Amazon S3 Amazon EBS (Elastic Block Storage) Amazon EFS (Elastic File System)  EFS Performance Considerations   Amazon Storage Gateway Amazon WorkDocs Database on EC2 Amazon RDS Amazon DynamoDB Amazon Redshift Amazon Neptune Amazon Elasticache Other Database Options  Amazon Athena Amazon Quantum Ledger Database Amazon Managed Blockchain Amazon Timestream Database Amazon DocumentDB Amazon ElasticSearch   Database Comparison   Amazon S3 \u0026ldquo;Secure, durable, highly scalable object storage at a very low cost.1 You can store and retrieve any amount of data, at any time, from anywhere on the web through a simple web service interface. You can write, read, and delete objects containing from zero to 5 TB of data.\u0026rdquo;\nRead More\u0026hellip;\nAmazon EBS (Elastic Block Storage)  To be used with EC2 Bound to a single AZ Use snapshots for backup Snapshots can be shared across different accounts Change AZ by launching a volume in different AZ from snapshot Convert from unencrypted to encrypted volume through snapshot Snapshots consume storage incrementally  AWS reorganized necessary data on snapshot deletion, to guarantee restore-ability of all the snapshots Snapshot lifecycle policies help automate the creation and deletion of the snapshots    Amazon EFS (Elastic File System)  Based on NFS (Network File System) Multi-AZ storage Pay based on the usage Mount points in single or many AZs EFS in not an encrypted protocol, use in caution of mounting over the Internet EFS is as durable and availble as S3  Amazon DataSync - good alternative for syncrhonizing on-premise storage with EFS / S3   Beware of the cost!  EFS is about 3 times more expensive than EBS and about 20 times more expensive than S3    EFS Performance Considerations  Burst credits are allocated over time to control throughput  starts from 2.1TiB with baseline rate of 50MiB/s and a burst rate of 100MiB/s Defined by BurstCreditBalance metric in AWS CloudWatch   Supports different 2 performance modes:  General purpose - default mode for FS with workload up to 7000 IOPS Max I/O - workloads demanding higher than 7000 IOPS, optimized for applications where tens / hundreds / thousands of EC2 instances are accessing the file system  Systems scale to higher level of aggregate throughput Tradeoff on slightly higher latencies for file operations     If application can handle async writes, you can tradeoff consistency for speed, through enabling asynchronous writes.  Read More\u0026hellip;\nAmazon Storage Gateway  Virtual Machine that can run on premise or EC2 Provides local storage resources backed by AWS S3 and Glacier Contains logic to synchronize data back-and-forth to S3 Useful in cloud migrations  Running Modes\n File Gateway  Allow on-prem or EC2 instances to store objects in S3 via NFS or SMB mount point   Volume Gateway Stored Mode / Gateway-stored Volumes  Async replication of data from on-prem to S3, uses iSCSI interface   Volume Gateway Cached Mode / Gateway-cached volumes  Primary data stored in S3 with frequent access data cached locally on-prem, uses iSCSI interface   Tape Gateway / Gateway-Virtual Tape Library  Virtual media changer and tape library for use with existing backup software, uses iSCSI interface    Cost Model Following cost model components should be considered when using AWS Storage Gateway:\n gateway usage snapshot storage usage volume storage usage virtual tape shelf storage virtual tape library storage retrieval from virtual tape shelf data transfer out  Amazon WorkDocs  Amazon\u0026rsquo;s alternative to Dropbox / Google Drive Secure, fully managed file collaboration service Can integrate with AD for SSO Web, mobile and native clients (no Linux client yet) HIPAA, PCI DSS and ISO complant Available SDK for creation complementary apps  Database on EC2  Run any database with full control and ultimate flexibility Self-managed backups, redundancy, patching, scale Good option to run databases not supported by RDS yet  Amazon RDS  Managed database service Supports most-popular database engines Structured, relational databases Automated backups and patching in pre-defined maintenance windows Push-button scaling, replicaiton and redundancy Multi-AZ RDs  Standby instance replication is Synchronous Masters can be promoted at any point of time without data loss   Read replication is asynchronous Read-replicas service regional users MariaDB is open-source fork of MySQL   Note: Non-transactional storage enginers like MyISAM don\u0026rsquo;t support replication; you must use InnoDB (XtraDB on MariaDB)  RDS Anti-Patterns\n Large BLOBS - use S3 Automated scalability - use DynamoDB Name/Value data structure - use DynamoDB Data is not well structure or unpredictable - use DynamoDB Unsupported by RDS database - use EC2 Complete control over the database - use EC2  Amazon DynamoDB  Key-value store Managed multi-AZ NoSQL data store Cross-Region Replication option Defaults to eventual consistency reads SDK supports strong read consistncy via a parameter  May slow down read in case of outages in the write AZ   Priced on throughput  Read/Write Capacity Units   Autoscale capacity adjusts per configured min/max levels  DynamoDB won\u0026rsquo;t scale down   On-Demand Capacity provides flexible capacity at a small premium cost Achieve ACID compliance with DynamoDB Transactions  Relational vs NoSQL\n Relational - structured data NoSQL - self-contained records  NoSQL Indexes\n Primary Key is used to create internal hash Composite Primary Key key consists of partition key and sort key  Can have duplicate of partition keys as long as the sort key is different   Global Secondary Index  If you want a fast query of attributes outside the primary key   Local Secondary Index  You know the partition key adn want to quickly query on some other attibute   There is a limit to the number of indexes and attributes per index Indexes take up storage space  Amazon Redshift  Fully managed, clustered peta-byte scale data warehouse Extremely cost-effective as compared to some other on-premises data warehouse platforms PostgreSQL compatible with JDBC and ODBC drivers available; comptiable with most BI tools out of the box Features parallel processing and columnar data stores which are optimized for complex queries Option to query directly form data files on S3 via Redshift Spectrum  Data Lake\n Large repository for a variety of data Query raw data without extension pre-processing Lessen time from data collection to data value Identify correlations between disparate data sets Data can be located on AWS S3 and queried from BI tools using Amazon Redshift Spectrum  Amazon Neptune  Fully-managed Graph database Optimized to deal with relationships between objects  Allows to store interrelationships and query them in very effective manner   Supports open graph APIs for both Gremlin and SPARQL  Amazon Elasticache  Fully managed implementations of 2 popular in-memory data stores - Redis and Memcached Push-button scalability for memory, writes and reads In Memory key/value store - not persisten in the traditional sense\u0026hellip; Billed by node size and hours of use  Use Cases\n Web Session Store  Stateless application   Database Caching  Offload load from database servers, return results faster to users   Leaderboards  Provide live leaderboard for millions of users in your mobile app   Streaming Data Dashboards  Provide a landing spot for streaming sensor data on the factory floor, providing live real-time dashboard displays.    Redis vs Memcached\nMemcached\n Simple, no-frills, straight-forward You need to scale out and in as demand changes You need to run multiple CPU corers and threads You need to cache objects (i.e. database queries)  Redis\n You need encryption You need HIPAA compliance Support for clustering You need complex datatypes You need high-availability (replication) Pub/Sub capability Geospacial Indexing Backup and Restore  Other Database Options Amazon Athena  SQL Engine overlaid on S3 base on Presto Query raw data objects as they sit in an S3 bucket Use or convert your data to Parquet format if possible for a big performance jump Similar in concept to Redshift Spectrum  Amazon Athena vs Amazon Redshift Spectrum\n Athena: Data lives mostly on S3 without the need to perform joins with other data sources Redshift Spectrum: Want to join S3 data with existing RedShift tables or create union products Supports Apache Parquet, JSON and Apache ORC formats  Amazon Quantum Ledger Database  Based on blockchain concepts Provides immutable and transparent journal as a service without having to setup and maintain an entire blockchain framework Centralized design (as opposed to decentralized consensus-based design for common blockchain frameworks) allows for higher performance and scalability Append-only concept where each record contributes to the integrity of the chain  Amazon Managed Blockchain  Fully managed blockchain framework supporting open source frameworks of Hyperledger Fabric and Ethereum Distributed consensus-based concept consisting of a network members (other AWS accounts), nodes (instances) and potentially applications  Amazon Timestream Database  Fully managed database service specifically built for storing and analyzing time-series data Alternative to DynamoDB or Redshift and includes some built-in analytics like interpolation and smoothing  Use Cases\n Industrial Machinery Sensor Networks Equipment Telemetry  Amazon DocumentDB  with MongoDB compatibility AWS\u0026rsquo;s investion that emulates the MongoDB API so it acts like MongoDB to existing clients and drivers Fully managed with all the good stuff (multi-AZ, HA, scalability, integrated with KMS, S3 backups) An option if you currently use MongoDB and want to get out of the server management  Amazon ElasticSearch  Stores and indexes documents (JSON) Usually referred to as ELK stack:  ElasticSearch - search and storage Kibana - analytics LogStash - intake    Other intake solutions:\n CloudWatch Firehose IoT  Database Comparison  Database on EC2  Ultimate control over database Preferred DB not available under RDS   Amazon RDS  Need traditional database for OLTP Your data is well-formed and structured   Amazon DynamoDB  Name/value pair data or unpredictable data structure In-memory performance with persistence   Amazon Redshift  Massive amounts of data Primary OLAP workloads   Amazon Neptune  Relationships between objects a major portion of data value   Amazon Elasticache  Fast temporary storage for small amounts of data Highly volatile data    "
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/2019-11-7-elements-of-data-science/",
	"title": "Elements of Data Science",
	"tags": [],
	"description": "",
	"content": " What is Data Science?  Types of Learning Key Issues in ML Supervised Methods  Linear Methods Logistic Regression and Linear Separability Linear Separability     Problem Formulation and Exploratory Data Analysis  Data collection  Data Sampling Data Labeling   Exploratory Data Analysis  Domain Knowledge Data Schema Data Statistics Correlations Data Issues     Data Processing and Feature Engineering  Encoding Categorical Variables  Encoding Ordinals Encoding Nominals   Handling Missing Values Feature Engineering  Filtering and Scaling Transformation Text-Based Features     Model Training, Tuning, and Debugging  Supervised Learning: Neural Networks Supervised Learning: K-Nearest Neighbors Supervised Learning: Linear and Non-Linear Support Vector Machines Supervised Learning: Decision Trees and Random Forests Model Training: Validation Set  Splitting Data: Training, Testing, Validation**   Model Training: Bias Variance Tradeoff Model Debugging: Error Analysis Model Tuning: Regularization  Regularization Techniques   Model Tuning: Hyperparameter Tuning Model Tuning  Training Data Tuning Feature Set Tuning   Model Tuning: Feature Extraction Model Tuning: Bagging/Boosting   Model Evaluation and Model Productionizing  Using ML Models in Production Model Evaluation Metrics  Confusion Matrix Metrics Cross Validation K-Fold Cross Validation Leave-one Out Cross Validation Stratified K-fold Cross Validation   Metrics for Linear Regression Using ML Models in Production: Storage  Model and Pipeline Persistence Model Deployment   Using ML Models in Production: Monitoring and Maintenance Using ML Models in Production: Using AWS Common Mistakes    Non-Linear support vector machines ](#ul-linon-linear-support-vector-machinesli-ul) - [Model Evaluation and Model Productionizing](#model-evaluation-and-model-productionizing)  ](#ul-lili-ul) - [Model Evaluation and Model Productionizing](#model-evaluation-and-model-productionizing)  ](#ul-lili-ul) - [Model Evaluation and Model Productionizing](#model-evaluation-and-model-productionizing)  ](#ul-lili-ul) - [Model Evaluation and Model Productionizing](#model-evaluation-and-model-productionizing)  ](#ul-lili-ul) - [Model Evaluation and Model Productionizing](#model-evaluation-and-model-productionizing)  ](#ul-lili-ul) - [Model Evaluation and Model Productionizing](#model-evaluation-and-model-productionizing)  ](#ul-lili-ul) - [Model Evaluation and Model Productionizing](#model-evaluation-and-model-productionizing)  ](#ul-lili-ul) - [Model Evaluation and Model Productionizing](#model-evaluation-and-model-productionizing)  ](#ul-lili-ul) - [Model Evaluation and Model Productionizing](#model-evaluation-and-model-productionizing) What is Data Science? Data Science Definition Processes and systems to extract knowledge or insights from data, either structured on unstructured. (Wikipedia)\nMachine Learning Artificial Intelligence machines that improve their predictions by learning from large amounts of input data.\nLearning Is the process of estimating underflying function $f$ by mapping data attributes to some target value.\nTraining Set Is a set of labeled examples $(x, f(x))$ where $x$ is the input variarbles and $f(x)$ is the observed target truth.\nGoal Given a training set, find approximation $f^`$ of $f$ that best generalizes, or predicts, labels for new measures. Results are measured by quality, e.g. error rate, sum squared error.\nFeatures Can be defined as Features, Attributes, Independent Variables, Predictors.\nLabel Can be defined as Label, Target, Outcome, Class, Dependent Variable, Response.\nDimensionality Refers to the number of Features.\nTypes of Learning  Supervised Learning  Labeled data Types:  Regression - target type is a continous numerical value Classification - categorical type     Unusupervised Learning  Unlabeled data Grouping / clustering the data   Semi-Supervised Learning  Some data is labeled, some is not   Reinforcement Learning  Apply reward and penalty for each step of ML Algorithm Example, teach ML algorithm to play video games    Key Issues in ML Data Quality\n Consistency of data  Consider the business problem Select subset of data and do some adjustments if required   Accuracy of data Noisy data Missing data  Missing values should be recovered for ML algorithms to work with   Outliers in data  Errors, typos should be refined Outliers should be replaced or removed   Bias Variance, etc.  Model Quality\n Underfitting  Failure to capture the important patterns Model is to simple or there are too few explanatory variables Not flexible enough to model real patterns Correponds to high bias (results show systematic lack of fit in certain regions)   Overfitting  Failure to generalize Indicates that the Model is too flexible Overreacting to the noise Correponds to high variance (small change in training data correponds to big change in the results)    Computation Speed and Scalability AWS SageMaker\n Increase speed Solve prediction time complexity Solve space complexity  Supervised Methods Linear Methods  $f(x) = \\Phi(W^T X)$ where $\\Phi()$\tis some activation function. Weights are optimized by applying (stochastic) gradient descent to minimize the Loss Function $\\sum_{}\\left\\lvert{\\hat{Y}_i - Y_i}\\right\\rvert ^ 2 $ Methods  Linear regression for numeric target outcome Logistic regiression for categorical target outcome    Univariate Linear Regression\n Simplest model One explanatory variable $X$ One target variable $Y$ Goal is to find a line that minimizes the Sum of Squared Errors (SSE) Finding a line is basically finding an intercept and slope, represented by $w_0$ and $w_1$  Multivariate Linear Regression\n Expansion of Univariate Linear Regression Includes $N$ explanatory variables Sensitive to correlation between features, resulting in high variance in coefficients  Logistic Regression and Linear Separability  Logistic Regression is a ML algorithm with the binary result (0/1) Estimates the probability of the input belonging to one or two classes: positive and negative Logistic Regression is vulnerable to outliers Sigmoid curve is a representation of probability $\\sigma(x) = \\frac{1}{1+e^-x}$  Value can be any value, but the output is always between 0 and 1 The threshold should be defined, e.g. 0.5 for predicting the label to the new observation    Intermediary variable $z$ will be a linear combination of features and used with the sigmoid function\n$$z = W^T = w_0 + w_1 x_1 + w_2 x_2 + w_n x_n $$\n$$\\sigma(z) = \\frac{1}{1 + e^-z}$$\nLogistic regression finds the best weight vector by fitting the training data\n$$ logit(p(y = 1 \\vert x)) = z $$\nwhere\n$$ logit = log ( \\frac{p}{1 - p} ) $$\nLinear Separability  Data set is considered as linearly separable if the features can be separated in a linear fashion Logistic regression can be easily applied in linearly separable data set   Problem Formulation and Exploratory Data Analysis  What is the problem we need to solve? What is the business metric?  Measure quality Measure impact of the solution   Is ML the appropriate approach?  Can the problem be solved with rules or standard coding? Are the patterns too difficult to capture algorithmically? Is there a lot of data available from which to induce patterns?   What data is available?  What are the data sources? What is the gap between desired data and actual data that is available? How to add more data?   What type of ML problem is it?  Characterize the ML model according to the dimensions Decompose the business model into a few models   What are your goals?  Technical ML goals Define the criteria for successful outcome of the project    Data collection AWS provides a comprehensive tool kit for sharing and analyzing data at any scale\n Open Data on AWS  you can discover and share data sets   Data Sampling  Selecting a subset of instances, examples, data points for training and testing   Data Labeling  Obtaining a gold-standard answers for supervised learning    Data Sampling  Representatively  Sample needs to be representative of the expected production population Sample be unbiased    Sampling Methods\n Random Sampling  Each data point has an equal probability of being selected Disadvantage: rare populations may miss, and lead to being under-represented   Stratified Sampling  Apply Random Sampling to each subpopulation separately    Issues with Sampling\n Seasonality (time of the day, day of the week, time of the year, etc..)  Stratified Sampling can minimize bias Visualizations may help   Trends (patterns can shift over time, new patterns can emerge)  To detect, try comparing models trained over different time periods Visualizations may help   Leakage: Train/Test Bleed  Inadvertent overlap of training and test data when sampling to create data sets Running tests on test set will end up showing better results then if the test set was independent   Leakage: Using information during training or validation that is not available in production  Consider using validation data that was gathered after your training data was gathered.\nData Labeling Labeling Components\n Labeling Guidelines  Instructions to Labelers Critical to get right Minimize ambiguity   Labeling Tools  Technology  Excel Spreadsheets Amazon Mechanical Turk Custom-built tools     Questions  Human Intelligence Tasks (HITs) should be:  Simple Unambiguous      Amazon Mechanical Turk\n Provides Human Intelligence on Demand Global, on-demand, 24x7 workforce Use for labeling  Managing Labelers\n Motivation Plurality  Assign each HIT to multiple labelers to identify difficult or ambigous cases, or problematic labelers (lazy, confused, biases)   Gold Standard HITs  HITs with known labels mixed in to identify problematic labelers   Auditors Labeler Incentives  Compensation Rewards Voluntary Gamification   Quality and productivity metrics  Sampling and Treatment Assignment\n Random Sampling  Random Assignment  Ideal experiments: Causal conclusion can be generalized   No Random Assignment  Typical survey or observation studies: Cannot establish causation but can establish correlation and can be generalized     No Random Sampling  Random Assignment  Most Experiments: Causal conclusion for the sample only   No Random Assignment  Badly-designed survey or pooled studies: Cannot establish neither causation nor correlation, cannot generalize to larger population      Exploratory Data Analysis Domain Knowledge  Understand how domains works, important dynamics and relationship constraints, how data is generated, etc. Better understanding of domain leads to better features, better debugging, better metrics, etc.  Amazon ML Solutions Lab\n Developing Machine Learning skills through collaboration and education  Brainstorming Custom Modeling Training On-Site with Amazon experts    Data Schema  Types of Data:  Categorical Ordinal Numerical Date Vector Text Image Un-structured    Pandas DataFrame Merge/Join\nimport pandas as pd df = pd.DataFrame({\u0026quot;Name\u0026quot;: [\u0026quot;John\u0026quot;, \u0026quot;Bob\u0026quot;, \u0026quot;Jim\u0026quot;, \u0026quot;Kate\u0026quot;], \u0026quot;Job\u0026quot;: [\u0026quot;Accountant\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Marketing\u0026quot;]}) df_1 = pd.DataFrame({\u0026quot;VP\u0026quot;: [\u0026quot;Tom\u0026quot;, \u0026quot;Andy\u0026quot;, \u0026quot;Kate\u0026quot;], \u0026quot;Job\u0026quot;: [\u0026quot;Accountant\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Marketing\u0026quot;]}) df_merged = df.merge(df_1, on=\u0026quot;Job\u0026quot;, how=\u0026quot;inner\u0026quot;) print(df_merged) Data Statistics  Look into each Feature one at a time Assess Interactions between the Features (relationships)  Descriptive Statistics\n Overall statistics  Number of instances (rows) Number of attributes (columns)   Attribute statistics (univariate)  Statistics for numeric attributes (mean, variance, etc.) - df.describe() Statistics for categorical attributes (histogram, mode, most/least frequent values, percentage, number of unique values)  Histogram of values: df[].value_counts() or seaborn\u0026rsquo;s distplot()   Target Statistics  Class distribution: E.g. df[].value_counts() or np.bincount(y)     Multivariate Statistics  Correlation Contingency Tables/Cross Tabulation    import pandas as pd import seaborn as sb import matplotlib.pyplot as plt df = pd.DataFrame({\u0026quot;Name\u0026quot;: [\u0026quot;John\u0026quot;, \u0026quot;Bob\u0026quot;, \u0026quot;Jim\u0026quot;, \u0026quot;Kate\u0026quot;], \u0026quot;Job\u0026quot;: [\u0026quot;Accountant\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Marketing\u0026quot;], \u0026quot;Salary\u0026quot;: [1000, 2500, 2750, 1800]}) df_1 = pd.DataFrame({\u0026quot;VP\u0026quot;: [\u0026quot;Tom\u0026quot;, \u0026quot;Andy\u0026quot;, \u0026quot;Kate\u0026quot;], \u0026quot;Job\u0026quot;: [\u0026quot;Accountant\u0026quot;, \u0026quot;Programmer\u0026quot;, \u0026quot;Marketing\u0026quot;]}) df_merged = df.merge(df_1, on=\u0026quot;Job\u0026quot;, how=\u0026quot;inner\u0026quot;) print(df_merged[\u0026quot;Job\u0026quot;].value_counts()) sb.distplot(df_merged[\u0026quot;Job\u0026quot;].value_counts()) plt.show() Basic Plots\n Density Plot Histogram Plot Scatter Plot Scatter Matrix Plot  import pandas as pd import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer dataset = load_breast_cancer() cols = [ 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39' ] df = pd.DataFrame(dataset['data'], columns=cols) df['target'] = dataset.target # show first a few rows print(df.head()) # show data type for each column print(df.info()) # show summary statistics for each columns print(df.describe()) # check the target variable properties print(df['target'].value_counts()) # Density Plot df['V11'].plot.kde() plt.show() # Histogram df['V11'].plot.hist() plt.show() # Box Plot df.boxplot(['V11']) plt.show() # Scatter Plots (detecting relationship between variables) df.plot.scatter(x='V11', y='V12') plt.show() # Scatter Matrix Plot pd.plotting.scatter_matrix(df[['V11', 'V21', 'V31']], figsize=(15,15)) plt.show() Correlations Correlation values are between -1 and 1.\n 0 means there\u0026rsquo;s no linear relationship -1 means negative correlation 1 means positive correlation  Correlation Matrices Measure the linear dependence between features; can be visualized with heat maps\nCorrelation Matrix Heatmap Apply color coding to the correlation matrix for easy detection of correlation among the attributes\nGenerating Heatmap\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer dataset = load_breast_cancer() cols = [ 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39' ] df = pd.DataFrame(dataset['data'], columns=cols) col = ['V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19'] heatmap = np.corrcoef(df[col].values.T) fig, ax = plt.subplots(figsize=(15, 15)) im = ax.imshow(heatmap, cmap='PiYG', vmin=1) fig.colorbar(im) ax.grid(False) [[ax.text(j, i, round(heatmap[i, j], 2), ha=\u0026quot;center\u0026quot;, va=\u0026quot;center\u0026quot;, color=\u0026quot;w\u0026quot;) for j in range(len(heatmap))] for i in range(len(heatmap))] ax.set_xticks(np.arange(len(col))) ax.set_yticks(np.arange(len(col))) ax.set_xticklabels(col) ax.set_yticklabels(col) plt.show() Generating Heatmap Using Seaboarn\nimport pandas as pd import numpy as np import seaborn import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer dataset = load_breast_cancer() cols = [ 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39' ] df = pd.DataFrame(dataset['data'], columns=cols) col = ['V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19'] heatmap = np.corrcoef(df[col].values.T) seaborn.heatmap(heatmap, yticklabels=col, xticklabels=col, cmap='PiYG', annot=True) plt.show() Data Issues  Messy Data Noisy Data Biased Data  The data itself is biased and non-representative of the truth For example: randomly populated data by the user in the survey   Imbalanced Data  Response variation in sample dataset does not represent the production data For example: the model returns 0 only in 1% of the cases in the sample dataset   Outliers Correlated Data  Highly correlated features can cause colinearity problems and numerical instability     Data Processing and Feature Engineering Encoding Categorical Variables  Categorical Variables are also called Discrete Categories are often represented by text Many algorithms required numerical input Special encoding is required to convert categories into numerical representations  Encoding Ordinals Types of Categorical Variables\n Ordinal  Ordered by certain numerical measurements E.g. clothes size, shoe size: large, medium, small   Nominal  No information about the order E.g. color: red, green, blue \u0026hellip;    Pandas support special dtype=\u0026quot;category\u0026quot;\nEncoding Categorical Variables Example\n Encoding ordinals using the map function, e.g. garden_size Encoding using sklearn\u0026rsquo;s LabelEncoder for labels  Cannot be used if there\u0026rsquo;s no relationship between categories with more than two categories    import pandas as pd from sklearn.preprocessing import LabelEncoder df = pd.DataFrame([ ['house', 3, 2572, 'S', 1372000, 'Y'], ['apartment', 2, 1386, 'N', 699000, 'N'], ['house', 3, 1932, 'L', 800000, 'N'], ['house', 1, 851, 'M', 451000, 'Y'], ['apartment', 1, 600, 'N', 324000, 'N'] ]) df.columns = ['type', 'bedrooms', 'area', 'garden_size', 'price', 'loan_approved'] print(df) # Converting garden_size using mapping mapping = dict({'N': 0, 'S': 5, 'M': 10, 'L': 20}) df['num_garden_size'] = df['garden_size'].map(mapping) # Converting label loan_approved using LabelEncoder loan_enc = LabelEncoder() df['num_loan_approved'] = loan_enc.fit_transform(df['loan_approved']) print(df) Encoding Nominals One-Hot Encoding Explode nominal attributes into many binary attributes, one for each discrete value\nfrom sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder import pandas as pd df = pd.DataFrame({\u0026quot;Fruits\u0026quot;: ['Apple', 'Banana', 'Banana', 'Mango', 'Banana']}) type_labelenc = LabelEncoder() num_type = type_labelenc.fit_transform(df[\u0026quot;Fruits\u0026quot;]) print(num_type) # output: # [0 1 1 2 1] print(num_type.reshape(-1, 1)); # output: # [[0] # [1] # [1] # [2] # [1]] type_enc = OneHotEncoder() type_trans = type_enc.fit_transform(num_type.reshape(-1, 1)).toarray() print(type_trans) # output: # [[1. 0. 0.] # [0. 1. 0.] # [0. 1. 0.] # [0. 0. 1.] # [0. 1. 0.]] Using Pandas\u0026rsquo; function:\nimport pandas as pd df = pd.DataFrame({\u0026quot;Fruits\u0026quot;: ['Apple', 'Banana', 'Banana', 'Mango', 'Banana']}) dummies = pd.get_dummies(df) print(dummies) Encoding with Many Classes\n To avoid drastically increasing dataset size, define a hierarchy structure For example: for a ZIP code column, use regions -\u0026gt; states -\u0026gt; city as the hierarchy and choose specific level to encode the ZIP code column Try to group the levels by similarity to reduce the overall number of groups  Handling Missing Values Most ML Algorithms cannot deal with missing values automatically\nCheck the Missing Values using Pandas\nimport pandas as pd df = pd.DataFrame({ \u0026quot;Fruits\u0026quot;: [\u0026quot;Banana\u0026quot;, \u0026quot;Apple\u0026quot;, \u0026quot;Mango\u0026quot;, \u0026quot;Mango\u0026quot;, \u0026quot;Apple\u0026quot;], \u0026quot;Number\u0026quot;: [5, None, 3, None, 1] }) # Display the total number of missing values for each column print(df.isnull().sum()) # Display the total number of missing values for each row print(df.isnull().sum(axis=1)) Important to Consider:\n What were the mechanisms that caused the missing values? Are these missing values missing at random? Are there rows or columns missing that you are not aware of?  Treating Missing Values\n Dropping rows  May loose to much data, (overfitting) May bias the sample   Dropping columns  May loose information in features (underfitting)   Imputing the values  Dropping The Missing Values\nimport pandas as pd df = pd.DataFrame({ \u0026quot;Fruits\u0026quot;: [\u0026quot;Banana\u0026quot;, \u0026quot;Apple\u0026quot;, \u0026quot;Mango\u0026quot;, \u0026quot;Mango\u0026quot;, \u0026quot;Apple\u0026quot;], \u0026quot;Number\u0026quot;: [5, None, 3, None, 1] }) # Drop the rows with null values print(df.dropna()) # Drop the columns with null values print(df.dropna(axis=1)) Imputing (Replacing) the Missing Values\nImputing using the mean strategy:\nfrom sklearn.impute import SimpleImputer import numpy as np arr = np.array([ [5, 3, 2, 2], [3, None, 1, 9], [5, 2, 7, None] ]) imputer = SimpleImputer(strategy='mean') imp = imputer.fit_transform(arr) print(imp) Feature Engineering  Creating new features to use as inputs to ML model using domain and data knowledge Scikit-learn\u0026rsquo;s library sklearn.feature_extraction can help in this stage Some rules of thumb  Use intuition Try generating features first and then apply dimensionality reduction if needed Consider transformations of attributes, e.g. squaring, multiplication of attributes Try not to overthink Try not to include too much manual logic    Filtering and Scaling Filter Examples:\n Remove color channels from the image if it\u0026rsquo;s not important Remove frequencies from audio if power is less than threshold  Scaling:\n Many algorithms are sensitive to each feature being on different scale  Gradient Descent KNN   Some algorithms are not sensitive to different scales  Decision trees Random forests    Scaling Transformation in Sklearn:\n Mean/variance standardization  Many algorithms behave better with smaller values Keeps outlier information, but reduces the impact sklearn.preprocessing.StandardScaler   MinMax scaling  Robust to small standard deviations sklearn.preprocessing.MinMaxScaler   MaxAbs scaling  Divides by the maximum value of the feature sklearn.preprocessing.MaxAbsScaler   Robust scaling  Uses 75th and 25th quartile metrics to do the scaling Robust to outliers as they have minimal impact sklearn.preprocessing.RobustScaler   Normalizer  Applied to the row, and not to columns as others sklearn.preprocessing.RobustScaler    Standard Scaler\nfrom sklearn.preprocessing import StandardScaler import numpy as np arr = np.array([ [5, 3, 2, 2], [2, 3, 1, 9], [5, 2, 7, 6] ], dtype=float) scale = StandardScaler() print(scale.fit_transform(arr)) print(scale.scale_) MinMaxScaler (produces values between 0 and 1)\nfrom sklearn.preprocessing import MinMaxScaler import numpy as np arr = np.array([ [5, 3, 2, 2], [2, 3, 1, 9], [5, 2, 7, 6] ], dtype=float) scale = MinMaxScaler() print(scale.fit_transform(arr)) print(scale.scale_) Transformation Polynomial Transformation\n Polynomial transformation can be applied to numeric features of the model Can use sklearn.preprocessing.PolynomialFeatures The newly created features can be then fed to the Model Beware of overfitting if the degree is too high Risk of extrapolation beyond the range of the data Consider non-polynomial transformations as well:  Log transforms Sigmoid transforms    from sklearn.preprocessing import PolynomialFeatures import numpy as np import pandas as pd df = pd.DataFrame({'a': np.random.rand(5), 'b': np.random.rand(5)}) cube = PolynomialFeatures(degree=3) cube_features = cube.fit_transform(df) cube_df = pd.DataFrame(cube_features, columns=[ '1', 'a', 'b', 'a^2', 'ab', 'b^2', 'a^3', 'ba^2', 'ab^2', 'b^3' ]) print(cube_df) Radial Basis Function\n Widely used in Support Vector Machines as a kernel and in Radial Basis Neural Networks (RBNN) Gaussian RBF is the most common RBF used  Text-Based Features Bag-of-words Model\n Represent documents as vector of numbers, one for each word (tokenize, count and normalize) Sparse matrix implementation is typically used Count Vectorizer is available in scikit-learn library: sklearn.feature_extraction.text.CountVectorizer  Includes lowercasing and tokenization on white space and punctuation   TfidVectorizer - Inverse Document-Frequency Vectorizer is available in sklearn.feature_extraction.text.TfidVectorizer  Hashing Vectorizer - stateless mapper from text to term index available in sklearn.feature_extraction.text.HashingVectorizser     Model Training, Tuning, and Debugging Supervised Learning: Neural Networks  Perceptron  Perceptron is a simplest form of a neural network It is a single-layer neural network   Neural Network  Contains several layers skelearn.neural_network.MLPClassifier Deep Learning Frameworks  MXNet TensorFlow Caffe PyTorch Keras     Convolutional Neural Network  Very efficient for Image processing Kernel is applied to extract features from the image Pooling layer: reducing dimension to reduce the size of the data   Recurrent Neural Network  Input data involves time-series or sequential features e.g forecasting, translation    Supervised Learning: K-Nearest Neighbors  Define a distance metric  Euclidean distance Manhattan distance Any vector norm   Choose the number of $k$ neighbors Find the $k$ nearest neighbors of the new observation that we want to classify\\ Assign the class label by majority vote Imortant to find the right $k$  Commonly use $k = \\frac{\\sqrt{N}}{2}$, where $N$ = number of samples    Characteristics of K-Nearest Neighbor Algorithm\n Non-parametric, instance-based, lazy Model requires keeping the original data set Space complexiy and prediction-time complexity grow with the size of training data Suffers from curse of dimensionality: points become increasingly isolated with more dimensions, for a fixed-size training dataset scikit-learn: sklearn.neighbors.KNeighborsClassifier  Supervised Learning: Linear and Non-Linear Support Vector Machines  Suited for extreme cases Unoptimized decision boundary can result in greater miscallsifications on new data  Types of SVM\n Linear support vector machines  For linearly separable sets The goal to define a hyperplane that classifies all training vectors into classes The best choice is to find a hyperplane that leaves the maximum margin from both classes   Non-Linear support vector machines  Applying kernels will support tranform a non-linear input space into a linear feature space Kernel is a function that takes as an input vectors in the original space and returns the dot product of the vectors in the feature space  Polynomical Kernel Radial Basis Function RBF Kernel Sigmoid Kernel, etc\u0026hellip;      Supervised Learning: Decision Trees and Random Forests  Algorithm decides which features are used for splitting in each layer Entropy - relative measure of disorder in the data source Simple to understand, interpret and visualize Less need for feature transformation Susceptible to overfitting scikit-learn: sklearn.tree.DecisionTreeClassifier  Building a Decision Tree\n Which features to choose? What are conditions for splitting? Where to stop? Pruning  Types of Decision Trees\n Regression Trees  Used when dependent variable is continuous Uses mean/average   Classification Trees  Used when dependent variable is categorical Uses mode/class    Common Decision Tree Algorith\n GIRI Index Chi-Squared Information Gain Reduction Invariance  Model Training: Validation Set  Model Training  Improve the model by optimizing parameters or data   Model Tuning  Analyze the model for generalization quality and sources of underperformance such as overfitting    Splitting Data: Training, Testing, Validation**  Training Set  Labeled dataset Dataset used for training the model   Validation Set  Usually 80/10/10 split, where 10% is for validation set and 10% for test set Labeled dataset Dataset used for assessing the Model\u0026rsquo;s performance on each trained epoch The split between training and validation dataset is required to avoid model overfitting Overfitting happens when the Model is giving good results on the training data, however is not able to generalize on the data it has not been trained on. Giving good results on both training and validation dataset gives us greater confidence in the model and its ability to generalize Validation dataset should be representative   Testing Set  Measures the final predictive power of the Model Can be used to compare the performance of different models    Model Training: Bias Variance Tradeoff Bias\n Indicates that the model is underfitting Caused by wrong assumptions, lack of complexity in the model  Variance\n Indicates that the model is overfitting Caused by over-complex models Model can be performing great on the testing set, but is not performing well on the validation set  $Total Error(x) = Bias^2 + Variance + Irreducible Error$\n As the model grows in complexity, it tends to move from Low Variance, High Bias to High Variance and Low Bias Best model is the model which keeps the Total Error minimal by making the right Bias-Variance tradeoff Examples of Improving the Model  Adjusting the Model complexity Adjusting the training size Adjusting the Hyperparameters    Learning Curve\n Plots the model performance Training dataset and validation dataset error or accuracy against training set size scikit-learn provides sklearn.learning_curve.learning_curve  Uses stratified k-fold cross validation by default    Model Debugging: Error Analysis  Filter on failed predictions and manually look for patterns Pivot on target, key attributes, and failure type, and build histograms of error counts Common patterns  Data problems (many variants for the same word) Labeling errors (data mislabeled) Under/over-represented subclasses (too many examples of one type) Discriminating information is not captured in features   It often helps to look on what model is predicting correctly  Model Tuning: Regularization  Regularization helps reduce variance / overfitting of the model Regularization penalizes for certain model complexity Higher complexity models are more likely to be unable to generalize well Regularization is achieved by adding a term to the loss function, that penalizes for the large weights: $loss + penalty$ Regularization is another hyperparameter that should be found out based on k-fold cross validation  Regularization Techniques Regularization in Linear Models\n L1 regularization, Lasso  $ penalty = \\sum_{j=1}^{n} \\lvert w_j \\rvert $\n L2 regularization, Ridge  $ penalty = \\sum_{j=1}^{n} \\lvert w_j^2 \\rvert $\nL2 Regularization In Neural Network\n$ penalty = (\\sum_{j=1}^n \\lvert w^{[j]} \\rvert^2 ) \\frac{\\lambda}{2 m} $\n$n$ - the number of layers $w^{[j]}$ - the weight matrix for the $j^th$ layer $m$ - the number of inputs $\\lambda$ - the regularization parameter\nScikit Learn Support\n Ridge regression model: Linear regression with L2  sklearn.linear_model.Ridge   Lasso regression model: Linear regression with L1  sklearn.linear_model.Lasso   Elastic net regression model: Linear regression with both  sklearn.linear_model.ElasticNet   Strength of regularization $c = \\frac{1}{\\alpha} $  Model Tuning: Hyperparameter Tuning Parameter vs Hyperparameter\n Parameter: an internal configuration whose value can be estimated from the data Hyperparameter: an external configuration whose value cannot be estimated from the data  Tuning Hyperparameters\n Grid Search  Will try all combinations for hyperparameters and evaluated the best Computationally intensive   Random Search  Finding the best set of hyperparameters based on random permutations of possible values of hyperparameters    Model Tuning Training Data Tuning Possible Issues and Solutions\n Small training data  More data can be sampled and labeled if possible   Training set biased against missing or more important scenarios  Sample and label more data for those scenarios if possible   Can\u0026rsquo;t easily sample or label more?  Consider creating synthetic data (duplication or techniques like SMOTE)   Training data doesn\u0026rsquo;t need to be exactly representative. Testing dataset needs to be exactly representative.  Feature Set Tuning  Add features that help capture pattern for classes of errors Try different transformations of the same feature Apply dimensionality reduction to reduce impact of weak features  Dimensionality Reduction\n Common cause of overfitting: too many features for the amount of data Dimensaionality Reduction: Reduce the (effective) dimension of the data with minimal loss of information The curse of dimensionality: certain models may not be able to give good results in the current dimension due to sparsity of data  Model Tuning: Feature Extraction  Mapping data into smaller feature space that captures the bulk of the information in the data  aka Data compression   Improves computational efficiency Reduces the curse of dimensionality Techniques  Principal Component Analysis (PCA)  Unsupervised Approach for feature extraction   Linear Discriminant Analysis (LDA)  Supervised Approach for feature extraction   Kernel versions of these for fundamentally non-linear data    Feature Selection vs Feature Extraction\n Both Feature selection and Feature Extraction reduce the dimensionality of the feature space Feature Selection  Uses algorithms to remove some of the features from the model Selected features will enable the model to have better performance There is no change (such as transform, linear combination or non-linear combination) in the selected features   Feature Extraction  Using algorithms to combine original features to generate a new set of features Number of features to be used in the model is generally less than the original number of features    Model Tuning: Bagging/Boosting Bagging\n Good for high variance but low bias Generates group of weak learners that when combined together generate higher accuracy  Create a x dataset of size m by randomly sampling original dataset with replacement (duplicates allowed)   Train weak learners on the new datasets to generate predictions Choose the output by combining the individual predictions (average in regression problem) or voting (classification) Reduces variance Keeps bias the same sklearn  sklearn.ensemble.BaggingClassifier sklearn.ensemble.BaggingRegressor    Boosting\n Good for high bias and accepts weights on individual samples Assign strength to each week learner Iteratively train learners using misclassified examples by the previous weak learners Training a sequence of samples to get a strong model sklearn:  sklearn.ensemble.AdaBoostClassifier sklearn.ensemble.AdaBoostRegressor sklearn.ensemble.GradientBoostingClassifier   XGBoost library   Model Evaluation and Model Productionizing Using ML Models in Production  Integrating an ML solution with existing software Keeping it running successfully over time  Aspects to Consider\n Model hosting Model deployment Pipelines to provide feature vectors Code to provide low-latency and/or high-volume predictions Model and data update and versioning Quality monitoring and alarming Data and model security and encryption Customer privacy, fairness and trust Data provider contractual constraints  Types of Production Environments\n Batch Predictions  All inputs are know upfront Predictions can be served real-time from pre-computed values   Online Predictions  Low latency requirements   Online Training  Data patterns change frequently, needs online training    Model Evaluation Metrics Confusion Matrix  Confusion Matrix values will be obtained when running the test data on the ML Model  Metrics $ Accuracy = \\frac{ TP + TN }{ TP + TN + FP + FN }$\n$ Precision = \\frac{ TP }{ TP + FP }$\n$ Recall = \\frac{ TP }{ TP + FN }$\n$ F1-Score = \\frac{ 2 x Precision x Recall }{ Precision + Recall }$\nCross Validation Cross validation is a model validation technique, for assessing the prediction performance of the model. Based on this, certain chunk of data, referred to as testset, will be excluded from the training cycle and utilized for testing stage.\nK-Fold Cross Validation  K-fold gives you an opportunity to train across all the data Is especially useful for smaller data sets Typically 5-10 folds are used  Steps\n Randomly partition the data into k-segments For each segment / fold, train the model on all other segments exclusing the selectged one  Use the fold excluded from training to evaluate the model   Train on all the data Average metric across K-folds estimates test metric for trained model  Leave-one Out Cross Validation  K = number of data points Used for very small sets K specifies the number of rows to used for training, and then leave one out  Stratified K-fold Cross Validation  Preserve class proportions in the folds Used for imbalanced data There are seasonality or subgroups  Metrics for Linear Regression $ Mean Squared Error $ $$ Mean Squared Error (MSE) = \\frac{1}{N} \\sum_{i=1}_{N} ( \\hat{y_i} - y_i )^2 $$\n$ R^2 $ $$ R^2 = 1 - \\frac{Sum of Squared Error (SSE)}{Var(y)} $$\n $R^2$ Coefficient of Determination Values between 1 and 0 1 indcates that regression perfectly fits the data  $$ Adjusted R^2 = 1 - (1 - R^2) \\frac{no. of data pts. - 1}{no. of data pts. - no. of variables - 1} $$\nUsing ML Models in Production: Storage Considerations\n Read/Write speed, latency requirements Data Storage Format Platform-dependency Ability for schema to evolve Schema/data separability Type richness Scalability  Model and Pipeline Persistence  Predictive Model Markup Language (PMML):  Vendor-independent XML-based language for storing ML models Support varies in different libraries  KNIME (analytics / ML Library): Full support Scikit-learn: extensive support Spark MLlib: Limited support     Custom Methods:  Scikit-learn: uses Python pickle method to serialize/deserialize Python objects Spark MLlib: transformers and estimators implement MLWritable TensorFlow: Allows saving of MetaGraph MxNet: Saves into JSON    Model Deployment  A/B Testing  May help detect production issues at early stage   Shadow Testing  Model is running behind the scenes Allows estimating Model\u0026rsquo;s performance while not serving production systems    Using ML Models in Production: Monitoring and Maintenance Monitoring Considerations:\n Qualiy Metrics and Buinsess Impacts Dashboards Alarms User Feedback Continuous model performance reviews  Expected Changes\n The real-world domain may change over time The software environment may change High profile special cases may fail There may be a change in business goals Performance deterioration may require new tuning Changing goals may require new metrics Changing domain may require changes to validation set Your validation set may be replaced over time to avoid overfitting  Using ML Models in Production: Using AWS  AWS SageMaker  Pre-built notebooks Built-in, high performance algorithms One-click training Hyperparameter optimization One-click deployment Fully managed hosting with auto-scaling   Amazon Rekognition Image Amazon Rekognition Video Amazon Lex  Service for building conversational interfaces into any application using voice and text ASR (Automatic Speech Recognition) NLU (Natural Language Understanding)    Amazon Transcribe  Amazon Polly  Text to speech service   Amazon Comprehend  NLP (Natural Language Processing) service  Discover insights and relationships in text Identify language based on the text Extract key phrases, places, people, brands or events Understand how positive or negative the text is Automatically organizes a collection of text files by topic     Amazon Translate  Automatic speech recognition (ASR) service   AWS DeepLens  Custom-designed deep learning inference engine HD video camera with on-board compute optimized for deep learning Integrates with AmazonSageMaker and AWS Lambda From inboxing to first inference in \u0026lt; 10 minutes Tutorials, examples, demos and pre-built models   AWS Glue  Data integration service for managing ETL jobs (Extract, Transform, Load)   Deep Scalable Sparse Tensor Network Engine (DSSTNE): Neural network engine  Common Mistakes  You solved the wrong problem The data was flawed The solution didn\u0026rsquo;t scale Final result doesn\u0026rsquo;t match with the prototype\u0026rsquo;s results It takes too long to fail The solution was too complicated There weren\u0026rsquo;t enough allocated engineering resources to try out long-term science ideas There was a lack of a true collaboration  "
},
{
	"uri": "https://majdarbash.github.io/python-2.7/1-python-introduction/",
	"title": "Python 2.7: Introduction",
	"tags": [],
	"description": "",
	"content": "Python installation:\nYou can install python by using this link:\nhttps://www.python.org\nDocumentation can be downloaded from:\nhttps://www.python.org/doc\nAs I will be using Mac, i will install python from:\nhttps://www.python.org/download/mac/\nYou can run python using command:\n$ python Following flags can be provided\n-v - verbose output You can use PyCharm - a python IDE provided by JetBrain - has some really cool features. Just google PyCharm and download the community / professional edition.\n"
},
{
	"uri": "https://majdarbash.github.io/ml-neural-networks/neural-networks-overview/",
	"title": "What are Neural Networks? Why do we use them?",
	"tags": ["artificial intelligence", "neural networks", "machine learning"],
	"description": "",
	"content": "Why do we use neural networks? Neural networks help us solve a lot of complicated problems when there\u0026rsquo;s no clear way to solve it. Sometimes problems can be complicated enough where we don\u0026rsquo;t know how to solve it, or solving them in conventional way will lead to very complicated algorithms which cannot adapt to changes.\nNeural networks play essential role in acting like human brain - helping us solving problems which we are good at: speech recognition, object recognition, pattern recognition, classification, etc\u0026hellip;\nNeural networks consists of layers on neurons, starting from input layer - which accepts the signal from receptors - one or multiple hidden layers (quite a big amount of hidden layers is one of main characteristics of deep learning networks) and output layer. Neurons of each layer are interconnected with the neurons next layer through synapses - which act like connectors transmitting signals. These synapses have positive or negative weights which will inhibit (negative weight) or excite (positive weight) neurons. Incoming signals from synapses are combined within the neuron and are passed through outgoing synapses to the next layer neural connections. Adjusting synaptical weights gives the ability to the neural network to respond to experience.\nMNIST - is a database of handwritten digits which is publicly available and can be used to train our networks.\nImageNet task - is a competition of classification of 1.3 million high resolution images from the web into 1000 different object classes. Measurements are done on top-1 and top-5 error rates. Top-1 error rate - error rate of classification of image matching the first answer of classification of machine. Top-5 error rate - error rate of classification of image being one of the top 5 choices selected by machine.\nSpeech Recognition - improved drastically with introduction of deep neural networks. Google achieved 12.3% error rate in android 4.1 after 5,870 hours of training data.\nNeuron Models Linear Neurons This is the simplest model for a neuron.\n\\begin{equation} y = b + \\sum_{i}{x_i w_i} \\end{equation}\n$b$ - bias for the neuron\n$i$ - index of input connection to neuron i\n$x_i$ - $i^{th}$ input\n$w_i$ - weight on $i^{th}$ input\nBinary Threshold Neurons \\begin{equation} z = \\sum_{i}{x_i w_i} \\end{equation}\n\\begin{equation} y = \\begin{cases} 1, if z \\geq \\Theta \\\\ 0, otherwise \\end{cases} \\end{equation}\nz - total input calculation\ny - output of the neuron\nRectified Linear Neurons (Linear Threshold Neurons) \\begin{equation} z = \\sum_{i}{x_i w_i} \\end{equation}\n\\begin{equation} y = \\begin{cases} z, if z \\geq 0 \\\\ 0, otherwise \\end{cases} \\end{equation}\nz - total input calculation\ny - output of the neuron\nSigmoid Neurons \\begin{equation} z = b + \\sum_{i}{x_i w_i} \\end{equation}\n\\begin{equation} y = \\dfrac{1}{1 + e^{-z}} \\end{equation}\nz - total input calculation\ny - output of the neuron\nThe beauty about the neuron is that it will generate an output betwen 0 and 1. with higher $z$ you will get $y$ approaching 1, while the lower, negative $z$ will lead to $y$ approaching 0.\nStochastic Binary Neurons Use the same equations of logistic units. The output of logistic is treated as the probability of producing a spike.\n\\begin{equation} z = b + \\sum_{i}{x_i w_i} \\end{equation}\n\\begin{equation} p(s = 1) = \\dfrac{1}{1 + e^{-z}} \\end{equation}\nz - total input calculation\ns - spike\np - probability of producing 1\nFor example, if p(s=1) = 0.9, neuron will be giving us 1 for 90% of the times. This gives some intrinsically random nature to the neuron.\n"
},
{
	"uri": "https://majdarbash.github.io/blockchain/blockchain-the-technical-side/",
	"title": "2 - Blockchain - The Technical Side",
	"tags": [],
	"description": "",
	"content": "Public an private keys are large integer numbers and they are represented using a separate Wallet Import Format (WIF), consisting of letters and numbers.\n\nPrivate key is used to generate a signature for each blockchain transaction a user sends out. It is used to confirm that the transaction has come from the user and also prevents the transaction from being altered by anyone once it has been issued.\nYou sign the cryptocurrencies you send to others using a private key.\nPrivate key derives public key. With a has function public key is transformed to address visible to everyone. You can receive cryptocurrencies to your address.\nCryptographic Hash Functions\nCollision when two inputs to a hash function produce the same output is extremely unlikely to happen.\nHash functions are deterministic - the same input will always yield the same output.\nBitcoin uses a SHA-256. Ethereum blockchain uses a hashing algorithm called Ethash. Hash created with Ethash looks like this:\n0xb846300e188829d1b819389b31cef3b9cfaf335082ee66f830a875f1c1beb396\nWhen data is written to the block it’s hashed. This principle allows the nodes participating in the blockchain to detect any changes to data.\nHashed data is used to create a link between each specific block. Hash of each previous block is written into the block in the chain and then hashed - producing the current block’s hash. This means that the current block hash will contain the prior hash.\nData written for the blockchain is considered permanent, making blockchain immutable.\nBlockchain relies on public key cryptography to acknowledge the idea of ownership on the blockchain.\nProcess of miners verifying actions on the blockchain is known as a proof of work.\nMessage Encryption\nBob wants to send message to Alice\nBob uses Alice’s public key to encrypt the message\nAlice decrypts the message using her private key\nMessage Signing\nBob uses his private key to create digital signature of the message\nAlice uses Bob’s public key to verify the content of the message and that the message is from Bob\nMerkle Tree\nEvery 2 nodes in this binary tree will be hashed together and merged into a single hash. As the result hash root is obtained.\nVerifying that a data chunk is part of the bunch represented by a merkle root will require less computation.The Bitcoin blockchain uses Merkle proofs in order to store the transactions in every block.\nAnatomy of the Block\n Transactions (limited to 1mb in size, 1500 - 2000 transactions) Block height: unique, auto-incremental number, identifying block’s position Timetstamp Nonce Hash of each previous block is written into the block in theHash of the previous block in the chain  Validity: not all blocks are valid. Finding a valid block = mining. Blockchains have an arbitrary “difficulty” setting which changes how hard it is to find a block. The work required to create a valid block is where value comes from. Miners are rewarded financially for finding a block. This is the work in “proof of work”.\nBased on the set difficulty you have to find a nonce which will result in the block hash value to be below the threshold. As a result of this the block will become valid and you will receive a reward for your work - currently it’s 12.5 bitcoins.\nTampering a single transaction will invalidate all the subsequent transactions and will require someone to recalculate the valid nonces for them.\nBlockchain protocols work on peer-to-peer network model. (e.g. Ethereum network, Bitcoin and Bittorent). There are 3 types of nodes operating in blockchain network: Full Node, Light Node, Miners.\n"
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/2019-11-01-ml-building-blocks-services-and-terminology/",
	"title": "ML Building Blocks: Services and Terminology",
	"tags": [],
	"description": "",
	"content": " Terminology Stages Data Process / ML Workflow  ML Problem Framing Data Collection / Integration Data Preparation  Data Cleaning Shuffling Training Data Test-Validation-Train Split Cross Validation   Data Visualization \u0026amp; Analysis Feature Engineering Model Training  Parameter Turning   Model Evaluation Business Goal Evaluation  Feature and Data Augmentation   Prediction    Terminology Stages  Training  Refers to how machine uses historical data sets to build its prediction algorithms.   Model  Model is what your machine creates after it\u0026rsquo;s been trained and refines over time as it learns.   Prediction  Prediction is machine\u0026rsquo;s best estimate of what the outcome of specific input or set of inputs would be. It\u0026rsquo;s sometimes called the Inference of a Model.    Data In Training Process, Data is split into:\n Training Dataset  Used by machine to create first model. Constitutes the majority of data.   Test Dataset  Is used to test the model for accuracy.    Process / ML Workflow  ML Problem Framing Data Collection / Integration Data Preparation Data Visualization \u0026amp; Analysis Feature Engineering Model Training Model Evaluation Business Goal Evaluation Prediction  Goal of Machine Learning model is to provide solution to a Business Problem. This happens through prediction. Prediction is not accurate and improves over time through provided feedback.\n ML Problem Framing  Forming Machine Learning Problem from the Business Problem What to use and how to use it? Do we have all the data needed? What algorithm do we use to answer the business question?  Supervised Learning  Learning from historical data set with a known answer.   Unsupervised Learning  Outcome is not known, ML algorithm will choose how to quantify the data and then give us the result.   Reinforcement Learning  The algorithm is rewarded based on the choices it makes while learning.      Classification Problems\n Binary Classification  2 classes   Multiclass Classification  3 + classes    Problem Definition\n Defining:  Observations Labels (Variables we are trying to predict) Features (Feature Engineering Process)     Data Collection / Integration  Structured data Semi-structured data Unstructured   Data Preparation Data Cleaning  Handling outliers Handing missing feature values  Introduce new indicator variable to represent missing value Removes rows Imputation  Replacing missing value with a value from dataset - may be a calculated guess. For example, for numerical we can use: mean, median.      Shuffling Training Data Makes data order not important and improves the results in certain algorithms.\nTest-Validation-Train Split  Test: 20% Validation: 10% Train: 70%   Cross Validation  Validation Leave-one-out (LOOCV) K-Fold   Data Visualization \u0026amp; Analysis Helps us understand the data better, refine the data, clean the outliers. This will result in better features leading to better models.\n Statistics Scatter-polts  Could help detect feature correlations   Histograms  Will help us detect outliers and skews in data     Feature Engineering Process of converting raw data into more useful features.\n Numeric Value Binning  Helps introduce non-linearity into linear models, breaking up continuous values Continuous values can be partioned into Bins based on ranges   Quadratic Features  Deriving new non-linear features by combining feature pairs   Non-Linear Feature Transformations Tree Path Features  Uses leaves of decision tree as features   Domain-Specific Transformations  Text: stop words removal / stemming, lowercasing, puctuation, cutting off very high / low percentiles Web-page Features: multiple fields of text, URL, anchor text, relative style and positioning     Model Training Parameters are the knobs used to tune our Machine Learning Algorithm.\nParameter Turning  Loss Function  Predicts how far your predictions are from the ground truth values.  Mean Square Loss Hinge Loss Logistic Loss     Regularization  Prevent overfitting by constraining weights to be small.   Learning Parameters  How fast or slow will your algorithm learn. Learning too fast may mean the algorithm will never reach the optimum value. Learning too slow means algorithm may take too long and never converge to the optimum.     Model Evaluation  Overfitting \u0026amp; Underfitting Bias-Variance Tradeoff Evaluation Metrics (Will be checked on test dataset.)  Regression  Root Mean Square Error (RMSE) MAPE (Mean Absolute Percent Error) $R^2$   Classification  Confusion Matrix ROC Curve Precision-Recall       Business Goal Evaluation  How well the model is performing related to business goals Make the decision to deploy or not  Accuracy Model generalization on unseen/unknown data Business success criteria     Feature and Data Augmentation Increases the complexity of the training data set by deriving features from internal / external data.\nPrediction  Model deployment is continuous process Monitoring distribution of production data vs. traning data is required Model should be re-trained with fresh learning data which reflect the current production distribution Model can be trained periodically  "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/networking/",
	"title": "Networking",
	"tags": [],
	"description": "",
	"content": " Ephermal Ports Reserved IP Addresses in VPC Network to VPC Connectivity  AWS Managed VPN AWS Direct Connect AWS Direct Connect Plus VPN AWS CloudHub VPN Software VPN Transit VPC   VPC to VPC Connectivity  VPC Peering AWS PrivateLink VPC Endpoints   Internet Gateways  Internet Gateway Egress-Only Internet Gateway NAT Instance NAT Gateway NAT Instances vs. Nat Gateways   Routing  Routing Tables Border Gateway Protocol (BGP)   Enhanced Networking  Placement Groups   Route 53  Route 53 Routing Policies   CloudFront Elastic Load Balancer  Read more on VPCs from AWS CSAA\n  AWS does not allow multicast (unicast vs multicast) TCP - stateful, connection based with acknowledging receipt UDP - stateless, no retransmission delays ICMP - used by network devices to exchange info (e.g. traceroute, ping)  Ephermal Ports  Short-lived transport protocol ports used in IP communication Above the well-know ports (\u0026gt; 1024) Refered to as \u0026ldquo;Dynamic Ports\u0026rdquo; Suggested range 49152 to 65535  Linux kernels use 32568 to 61000 Windows platform default from 1025   NACL and Security Group implications  Client defines the Ephermal port to use for response    Reserved IP Addresses in VPC  10.0.0.0 - Network address 10.0.0.1 - Reserved by AWS for VPC Router 10.0.0.2 - Reserved by AWS for Amazon DNS 10.0.0.3 - Reserved by AWS for future use 10.0.0.255 - VPCs don\u0026rsquo;t support broadcast so AWS reserves this address  Network to VPC Connectivity AWS Managed VPN  Managed IPSec VPN connection Quick and simple way to establish secure tunneled connection to a VPC; Redundant link for Direct Connect or other VPC VPN Support static routes or BGP peering and routing Dependent on your internet connection  AWS Direct Connect  Direct connection to AWS backbone May require additional telecom and hosting provider relationships and/or new network circuits Work with your existing Data Networking Provider; Create Virtual Interfaces (VIF) to connect to VPCs or other AWS service like S3 or Glacier (public VIF) Direct Connect connections consist of a single connection between your network and AWS with no inherent redundancy (by default it\u0026rsquo;s not HA) Traffic coming from on-prem via a Direct Connect connect is restricted from internet access  AWS Direct Connect Plus VPN  IPSec VPN connection over private lines Extra layer of security through encrypted tunnel over AWS Direct Connect  AWS CloudHub VPN  Connect locations in a \u0026ldquo;Hub and Spoke\u0026rdquo; manner using AWS\u0026rsquo;s Virtual Private Gateway Link remote offices for backup or primary WAN access to AWS resources and each other  Software VPN  Provide your own VPN endpoint and software (e.g. OpenVPN) Useful when you want to manage both ends of the VPN connection for compliance reasons or you want to use VPN option not supported by AWS You must build and design the VPN architecture for redundancy and availability  Transit VPC  Common strategy for geographically disperse VPCs in order to create a global network transit center Locations and VPC-deployed assets across multiple regions that need to communicate with one another Cisco, Juniper Networks and Riverbed have offerings which work with their equipment and AWS VPC  VPC to VPC Connectivity VPC Peering  AWS-provided network connectivity between two VPCs Uses AWS Backbone without touching the Internet Transitive peering is not supported  A connected to B and B connected to C, doesn\u0026rsquo;t mean that A is connected to C via B   VPC peering request is made from Originator to Accepter  AWS PrivateLink  AWS-provided network connectivity between VPCs and AWS Services Use AWS backbone to reach other services, without going through the Internet  VPC Endpoints  Interface Endpoint  Elastic Network Interface with Private IP Uses DNS entries to redirect traffic API Gateway, CloudFormation, CloudWatch, etc\u0026hellip; Secured by Security Groups   Gateway Endpoint  The gateway which is a target for a specific route Uses prefix lists in the route table to redirect traffic Amazon S3, DynamoDB VPC Endpoint Policies    Internet Gateways Internet Gateway  Horizontally scaled, redundant and highly available component No AZ risk or bandwidth constraints Provides route table target for Internet-bound traffic Perform NAT for instances with public IP addresses  Egress-Only Internet Gateway The purpose of an \u0026ldquo;Egress-Only Internet Gateway\u0026rdquo; is to:\n  Allow IPv6 based traffic within a VPC to access the Internet\n  Denying any Internet based resources the possibility of initiating a connection back into the VPC\n  IPv6 addresses are globally unique and are therefore public by default\n  Provides outbound intenret access for IPv6 addressed instances\n  Prevents inbound access to those IPv6 instances\n  Stateful - forwards traffic from instance to internet and then sends back the response\n  Must create a custom route for ::/0 to the Egress-Only Internet Gateway\n  Use Egress-Only Internet Gateway instead of NAT for IPv6\n  NAT Instance  EC2 instance from a special AWS-provided AMI Translates traffic from many private IP instance to a single public IP and back Doesn\u0026rsquo;t allow public internet initiated connections into private instances Not supported for IPv6 (use Egress-Only Gateway) NAT instance must live on a public subnet with route to Internet Gateway Private instances in private subnet must have route to the NAT instance, usually the default route destination of 0.0.0.0/0  NAT Gateway  Fully-managed NAT service that replaces need for NAT instances on EC2 Must be created in a Public subnet Uses an Elastic IP for public IP for the life of hte gateway Private instances must have route to the NAT gateway, usually to destination 0.0.0.0/0 Created in a specified AZ with redundancy in that zone For multi-AZ deployment, create NAT Gateways in each AZ with routes for private subnets to use the local Gateway Up to 5Gbps bandwidth that can scale up to 45 Gbps  NAT Instances vs. Nat Gateways  NAT Instances allow you to detach and attach EIPs while NAT Gateways do not allow you to detach NAT Instances can use security groups as they are just EC2 instances NAT Instances also can be configured to support port forwarding while NAT Gateways do not support this  Routing Routing Tables  VPCs have an impicity router and main routing table You can modify the main routing table or create new tables Each route table contains a local route for the CIDR block Most specific route for an address wins Taffic to VPC Endpoints is routed through the routing tables  Border Gateway Protocol (BGP)  Popular protocol for the Internet BGP is the only protocol supported by AWS Propagates information about the network to allow dynamic routing Required for Direct Connect and optional for VPN Alternative of not using VGP with AWS VPC is static routes AWS Supports BGP community tagging as a way to control traffic scope and route preference Required TCP port 179 + ephemeral ports Autonomous System Number (ASN) = Unique point identifier Weighting is local to the router and higher weight is preferred path for outbound traffic  BGP is a protocol designed to connect Autonomous Systems (CloudFlare):\n Autonomous system is large network or group of networks that has a unified routing policy. Every computer or device that connects to the Internet is connected to an AS. BGP is the protocol that makes the Internet work BGP enables data routing on the Internet BGP is responsible for looking at all of the available paths that data could travel and picking the best route, which usually means hopping between autonomous systems   Enhanced Networking  Generally used for High Performance Computing use-cases Uses single root I/O vritualization (SR-IOV) to deliver higher performance than traditional virtualized network interfaces Might have to install driver if other than Amazon Linux HVM AMI Interface Options  Intel 82599 VF Interface (10 Gbps) Elastic Network Adapter (25 Gbps)    Placement Groups  Cluster placement group  Packs instances close together inside an Availability Zone Required for low network latency and/or high network throughput Pros: Get the most out of Enhanced Networking Instances Cons: Finite capacity: recommend launching all you might need up front   Spread placement group  Instances spread across underlying hardware Required to reduce risk of simultaneous failure if underlying hardware fails Pros: Can span multiple AZ\u0026rsquo;s Cons: Max of 7 instances running per group per AZ   Partition placement group  Instances are grouped into partitions and each partition is spread across racks Partitions do not share underlying hardware Required to reduce risk of correlated hardware failure for multi-instance workloads Pros: Better for large distributed or replicated workloads than Spread Cons: Not supposed for Dedicated Hosts    Route 53  Register domain names Check the health of your domain resources Route internet traffic for your domain Route 53 currently supports 13 different DNS record types including; AAAA, CNAME and SPF Route 53 does not currently support any of DNSSEC related records, such as DNSKEY  Route 53 Routing Policies  Simple Failover  Health check of the primary route will lead to failover to secondary route   Geolocation Geoproximity  Closer proximity to the region is specified. Bias value ranging between -99 and +99 is used to control the proximity radius   Latency  Traffic will be routed to the region that provides the best latency   Mutlivalue Answer  respond to DNS queries with up to eight healthy records selected at random not good for distributing the traffic - DNS client may cache the IP for some time which may lead to unexpected results   Weighted  Traffic is distributed respecting the weights of the routes.    CloudFront SSL and TLS\n 1995: SSLv2 first public release 1996: SSLv3 released to fix security design flaws 1999: TLSv1.0 with SSL backwards 2005: TLSv1.2 Present: TLSv1.3  CF Settings\n CF Security Policy  Defines the protocols and ciphers that CloudFront uses   Server Name Indication (SNI)  Allows CloudFront distribution instances to server content over HTTPS to clients that support SNI. Older browsers will not have access to content over HTTPS.    Elastic Load Balancer  Application Load Balancer (Layer 7)  Supports VPC HTTPS / HTTP Path, Method, Query string or Host-based Routing Web Sockets SNI Sticky Sessions Static IP, Elastic IP (through AWS Global Accelerator) User Authentication   Network Load Balancer (Layer 4)  Supports VPC TCP / UDP / TLS Web Sockets SNI, as of 2019 Static IP, Elastic IP   Classic Load Balancer (Layer 4 /7)  Supports EC2-Classic or VPC Sticky Sessions    "
},
{
	"uri": "https://majdarbash.github.io/ml-neural-networks/neural-networks-types-of-learning/",
	"title": "Neural Networks: Supervised, Unsupervised &amp; Reinforcement Learning Types of Learning",
	"tags": ["artificial intelligence", "neural networks", "machine learning", "types of learning", "supervised", "unsupervised", "reinforced"],
	"description": "",
	"content": "There are 3 types of learning\n Supervised learning Reinforcement learning Unsupervised learning  Supervised Learning In supervised learning we try to predict an output when given an input vector. Input and target are clear in supervised learning.\nRegression - target ouptut is a real numbers or a vector of real numbers\nClassification - target output is a class label\nModel-class is a function:\n\\begin{equation} y = f(x; W) \\end{equation}\n$y$ - output of the model-class\n$x$ - input vector\n$W$ - other parameters provided to the function $f$\nLearning is the process of adjusting the parameters $W$, which will reduce the discrepancy between the target output $t$ and the actual output $y$.\nFor regression, discrepancy can be measured as:\n\\begin{equation} \\dfrac{1}{2} (y - t) ^2 \\end{equation}\nExamples of unsupervised learning can be:\n linear regression for regression problems random forest for classification and regression problems support vector machines for classification problems  Unsupervised Learning Here you have only input data (X) and no output variables. The goal is to model the structure in the data order to learn more about the data. After your data is normalized and certain metrics are correlated between different data groups, you may find out some interesting facts about the data you are dealing with.\nThere are mainly 2 groups of unsupervised learning problems:\nClustering - discovering how data can be grouped (i.e. grouping customers by a certain behavior)\nAssociation - association learning problem, discovering rules that describe large portions of data\nExamples can include:\n k-means for clustering problems Apriori algorithm for association rule learning problems  Reinforcement Learning The objective of reinforcement learning is to select each action to maximize the rewards. Usually it is not easy to conclude which action was right or wrong as the reward is happening in the future and the time lag will add some uncertainity on the learning / adjustment process.\nBasically the learner decides What to do in order to maximize the reward. The exact output is not clear like in supervised learning, and we know that we are expecting an output (in contrast to unsupervised learning).\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/2-python-basics-variables-strings-date-and-time/",
	"title": "Python 2.7: Basics: Variables, Strings, Date and Time",
	"tags": [],
	"description": "",
	"content": "Variables x = 10 This statement will create a variable called x which will store a number 10. By assigned value python understands that this variable will store numeric data. We can also store other data types, by assigning different types of values:\nx_int = 1 x_float = 1.23 x_bool = True Note that booleans should have an uppercase in the beginning. (True and not true, False and not false)\nTo print anything on the screen you can use the print function:\nprint x_float Whitespaces In python whitespaces have a significant meaning - they are used to structure code beyond the scope of indentation and formatting. Badly formatted code will result in IndentiationError.\nComments Single-line comments\n# my comment Multi-line comments\n\u0026quot;\u0026quot;\u0026quot; my comment - mutline line comment \u0026quot;\u0026quot;\u0026quot; Math Python will go beyond the standard math operators provided by the programming languages. In addition to operations like multiplication, addition, subtraction and division, python will provide you with some more interesting operators.\nLet\u0026rsquo;s assume you want to raise 2 to a power of 3. This is how we can do it in python:\nresult = 2 ** 3 Exponentiation is performed in python by using ** operator.\nStrings Similar to any other language:\nx_string = \u0026quot;John\u0026quot; # espace character x_string = 'This is John\\'s birthday' String is actually an array of characters. You can directly access characters by using indexes:\nx_string = \u0026quot;Python\u0026quot; # will display the character t print x_string[2] String operators: len() - returns the number of characters in a string\n# will print number 5 as a number of characters in the word hello print len(\u0026quot;hello\u0026quot;) string.lower() - this method is used to convert all characters to lowercase\n# output: hello print \u0026quot;Hello\u0026quot;.lower() string.upper() - converts all characters to uppercase\n# output: HELLO print \u0026quot;Hello\u0026quot;.upper() str() - converts non-strings to strings\n# creating a float variable pi pi = 3.14 # converting pi to string print str(pi) String Concatenation Strings can be concatenated using the summation operator +:\n# output: Times of our life print \u0026quot;Times \u0026quot; + \u0026quot; of \u0026quot; + \u0026quot; our \u0026quot; + \u0026quot; life \u0026quot; # output: My score is 50 x_int = 50 print \u0026quot;My score is \u0026quot; + str(x_int) Another way to print strings by doing string replacement is to format them with %\n# output: Jane and Mike were best friends since a long time string_1 = \u0026quot;Jane\u0026quot; string_2 = \u0026quot;Mike\u0026quot; print \u0026quot;%s and %s were best friends since a long time\u0026quot; % (string_1, string_2) This can be a more interesting exercise:\n# input: for first name: [Tom], for last name: [Hanks] # output: Dear Tom Hanks, please proceed to the registration counter first_name = raw_input(\u0026quot;What's your first name?\u0026quot;) last_name = raw_input(\u0026quot;What's your last name?\u0026quot;) print \u0026quot;Dear %s %s, please proceed to the registration counter\u0026quot; (first_name, last_name) Date and Time The code below displays the current time\n# output - will print current time in format of : 2016-11-10 18:18:31.379762 from datetime import datetime now = datetime.now() print now # output: current year, e.g. 2016 print now.year # output: current month, e.g. 11 print now.month # output: current day, e.g. 10 print now.day just like above, you can also use now.hour, now.minute and now.second\n"
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/2019-11-6-crisp-dm-on-aws/",
	"title": "CRISP-DM on AWS",
	"tags": [],
	"description": "",
	"content": "CRISP-DM  Stands for Cross Industry Standard Process - Data Mining Is a framework to Data Science Project  Phases of CRISP-DM  Business Understanding Data Understanding Data Preparation Modeling Evaluation Deployment   Business Understanding 1. Understanding business requirements\n Questions from the business perspective which need answering Highlight project\u0026rsquo;s critical features People and resources required  2. Analyzing supporting information\n List required resources and assumptions Analyze associated risks Plan for contingencies Compare costs and benefits  3. Converting to a Data Mining or Machine Learning problem\n Review machine learning question Create technical data mining objective Define the criteria for successful outcome of the project  4. Preparing a preliminary plan\n Number and duration of stages Dependencies Risks Goals Evaluation methods Tools and techniques   Data Understanding  Data collection  Detail Various sources and steps to extract data Analyze data for additional requirements Consider other data sources   Data properties  Describe the data, amount of data used, and metadata properties Fidn key features and relationshps in the data Use tools and techniques to explore data properties   Quality  Verifying attributes Identifying missing data Reveal inconsitencies Report solution    AWS tools for Data Understanding\n Amazon Athena  Run interactive SQL queries on Amazon S3 data Schema-on-read Serverless   Amazon QuickSight  Fast, cloud-powered business intelligence and data visualization service   AWS Glue  Managed Extract-Transform-Load (ETL) service     Data Preparation Tasks \u0026amp; Modeling Data Prepation Tasks 1. Final Dataset Selection  Total size Included and Excluded columns Record selection Data type  2. Data Preparation  Cleaning  Missing data  Dropping rows Add default value or mean value Use statistical methods to calculate the value   Address noise values   Transformed  Derive additional attributes from the original Normalization Attribute transformation   Merging  Merging data into the final data set Null values introduced may require a cleaning iteration   Formatting  Rearrange attributes Randomly shuffle data Remove constraints of the modeling tool (Unicode characters \u0026hellip;)    Data Modeling 1. Model selection and creation  Identify modeling technique (Regression for numeric values, Recurrent NN for sequence prediction\u0026hellip;) Constraints of mdoeling technique and tool  2. Model testing plan  Test/train dataset split (30% test, 70% train) Model evaluation criterion  3. Parameter tuning/testing  Build multiple models with different parameter settings Describe the trained models and report on the findings  AWS Tools for Data Preparation and Modeling\n AWS EMR + Spark  IPython notebooks, Zeppelin notebooks, R studio Scala, Python, R, Java, SQL Cost savings: Leverage Spot instances for task nodes   AWS EC2 + Deep Learning AMI  GPU CUDA support for training Preinstalled deep learning frameworks  MXNet, TensorFlow, Caffe2, Torch, Keras, Theano\u0026hellip;   Includes Python Anaconda Data Science Platform with popular libraries like Numpy, Sikit-learn You can install R Studio on EC2 Deep Learning AMI     Evaluation 1. Evaluate how the model performed related to the business goals  Accuracy of the model Model generalization on unseen/unknown data Evaluation of the model using existing business criteria  Reviewing the project\n Assess the steps taken in each phase Perform quality assurance checks  2. Make the final decision to deploy or not Based on complete evaluation and business goals acceptance criteria we will take a decision wether a model will be deployed or not. This requires careful analysis of the false positives and true negatives.\nRunning Jupyter Notebook on EC2 Instance  Create instance using Deep Learning AMI Connect to the instance using SSH Run: screen (read more \u0026hellip;) Run Jupyter Notebook: jupyter notebook --no-browser OR jupyter notebook --no-browser --ip=0.0.0.0 --port=[choose your port] Copy-paste the URL containing the token to the browser and access the example   Deployment 1. Planning deployment  Runtime  AWS EC2 Instances AWS EC2 Container Service AWS Lambda  Trained model could be saved to S3 and then loaded in Lambda function     Application Deployment  AWS CodeDeploy AWS OpsWorks AWS Elastic Beanstalk   Infrastructure Deployment  AWS CloudFormation AWS OpsWorks AWS Elastic Beanstalk   Code Management  AWS CodeCommit AWS CodePipeline AWS Elastic Beanstalk    2. Maintenace and monitoring Monitoring\n AWS CloudWatch AWS CloudTrail AWS Elastic Beanstalk  3. Final report  Highlight processes used in the project Analyze if all the goals for the project were met Detail the findings of the project Identify and explain the model used and reason behind using the model Identify the customer groups to target using this model  4. Project review  Assess the outcomes of the project Summarize the results and write thorough documentation Common pitfalls Choosing the right ML solution Generalize the whole process to make it useful for the next iteration  "
},
{
	"uri": "https://majdarbash.github.io/python-2.7/3-python-built-in-functions-examples/",
	"title": "Python 2.7: Built-in Functions, Examples",
	"tags": [],
	"description": "",
	"content": "Python provides you with a great variety of built-in functions and modules. We wil display a bit of interesting examples here, using some built-in functions.\ndef biggest_number(*args): return max(*args) As you noticed, we can pass multiple inputs to our function using *args syntax.\n# output: 6 print max(4,2,6) # output: 3 print min(3,6,7) # output: 98 print abs(-98) Determining the type of variabel and using it: # output: displays the types of variables: int, str, float x_int = 52 x_string = \u0026quot;hello\u0026quot; x_float = 2.436 # output: print type(x_string) # output: print type(x_float) # output: print type(x_int) # verify the input and increment it if numeric def increment(number): if type(number) == int or type(number) == float: return number + 1 else: return False def increment_input(input): result = increment(input) if result: print \u0026quot;The result is : %s\u0026quot; % result else: print \u0026quot;Sorry, the input you provided is not a number\u0026quot; # output: Sorry, the input you provided is not a number input1 = raw_input(\u0026quot;Enter a string: \u0026quot;) increment_input(input1) # output: the number provided + 1 input2 = raw_input(\u0026quot;Enter a number: \u0026quot;) input2 = int(input2) increment_input(input2) Creating average function # calculating average with the provided list of numbers # using pre-built function sum def average(numbers): total = float(sum(numbers)) total = total / len(numbers) return total print average([2,4,9]) Generating random numbers from random import randint random_number = randint(0, 12) # output: a random number between 0 and 12 print rand_number ### Sum of digits in a number def digit_sum(n): total = 0 while n != 0: number = n % 10 n = int(n / 10) total = total + number return total # output: 10 = 1+2+3+4 print digit_sum(1234) Factorial function def factorial(x): total = 1 for number in range(1, x + 1): total = total * number return total # output: 24 = 4! = 4 * 3 * 2 * 1 print factorial(4) Is Prime number function def is_prime(x): x = abs(x) for number in range(0, x): # skipping the numbers 0 and 1 if abs(number) == 0 or abs(number) == 1: continue # if number can be divided by any other number then it's not a prime number if x % number == 0: return False return True # output: True print is_prime(13) # output: False print is_prime(12) # output: True print is_prime(-11) String reverse function def reverse(word): reversed_word = \u0026quot;\u0026quot; for character in word: reversed_word = character + reversed_word return reversed_word # output: emesrever print reverse('reverseme') Anti-vowel function This function will remove the vowel in a string and return a function without vowels.\ndef anti_vowel(word): word_without_vowels = \u0026quot;\u0026quot; for character in word: if character.lower() in ['a', 'e', 'i', 'o', 'u']: continue word_without_vowels = word_without_vowels + character return word_without_vowels # output: \u0026quot;Hr y g, dr!\u0026quot; print anti_vowel(\u0026quot;Here you go, dear!\u0026quot;) ### Counting number of item occurences in the list def count(sequence, item): total = 0 for search_item in sequence: if item == search_item: total = total + 1 return total # output: 3 (number 2 occurs 3 times in the list) print count([1,2,3,2,3,2], 2) ### Removing odd numbers from the list def purify(list): new_list = [] for item in list: if item % 2 == 0: new_list.append(item) return new_list # output: [2] - odd numbers will be removed print purify([1,2,3]) ### Multiplying numbers in the list def product(numbers): total = 1 for number in numbers: total = total * number return total # output: 6 = 1 * 2 * 3 print product([1,2,3]) ### Removing duplicates from the list def remove_duplicates(list): clean_list = [] for item in list: if item in clean_list: continue clean_list.append(item) return clean_list # output: [1,3,4] print remove_duplicates([1,3,3,4]) ### Calculating median in the list of numbers # even number of elements in the list # taking an average of the middle elements # otherwise, taking the middle element def median(numbers): numbers = sorted(numbers) if len(numbers) == 0: return 0 if len(numbers) % 2 == 0: middle = len(numbers) / 2 - 1; return float(numbers[middle] + numbers[middle + 1]) / 2 else: middle = (len(numbers) - 1) / 2; return numbers[middle] # output: 0 - median of empty list print median([]) # output: 5.5 print median([1,2,5,6,7,8]) # output: 3 print median([1,2,3,4,5]) Grades Example: calculating average, variance and standard deviation: grades = [100, 100, 90, 40, 80, 100, 85, 70, 90, 65, 90, 85, 50.5] def print_grades(grades): for grade in grades: print grade def grades_sum(grades): total = 0 for grade in grades: total = total + grade return total def grades_average(grades): return grades_sum(grades) / float(len(grades)) def grades_variance(grades): average = grades_average(grades) variance = 0 for grade in grades: variance = variance + (average - grade) ** 2 variance = variance / len(grades) return variance def grades_std_deviation(variance): return variance ** 0.5 # output: the grades list, each grade on a single line print_grades(grades) # output: 80.4230769231 print 'Average Grade: %s' % grades_average(grades) # output: 334.071005917 variance = grades_variance(grades) print 'Variance: %s' % variance # output: 18.2776094147 print 'Standard Deviation: %s' % grades_std_deviation(variance) "
},
{
	"uri": "https://majdarbash.github.io/ml-neural-networks/machine-learning-python/",
	"title": "Python 2.7: Setting up Neural Network with PyBrain",
	"tags": ["artificial intelligence", "python", "pybrain", "neural networks"],
	"description": "",
	"content": "Today, I\u0026rsquo;m experimenting machine learning concepts in python. For this purchase I\u0026rsquo;m using PyBrain. If you would like to have a better idea about Python, I suggest having a quick glance at posts 1-10 in Python category.\nPyBrain is a Machine Learning library for Python. PyBrain stands for Python-Based Reinforcemnet Learning, Artificial Intelligence and Neural Network Library.\nFor complete guide on installation you can get complete details from:\nhttp://pybrain.org/docs/\nIn the example below, based on pybrain.org tutorial I\u0026rsquo;m creating a network, dataset and training my network on the dataset.\nInstalling PyBrain:\n$ git clone git://github.com/pybrain/pybrain.git $ python setup.py install\nBuilding a network Let\u0026rsquo;s build a simple network. Assume our network accepts 2 inputs, and is expected to generate 2 outputs. Let\u0026rsquo;s experiment with the following network structure:\n one input layer (2 neurons) one hidden layer (3 neurons) one output layer (1 neuron)  from pybrain.tools.shortcuts import buildNetwork net = buildNetwork(2, 3, 1); Activating Network Network gets populated with random values. We can test the output of the network by activating it.\nLet\u0026rsquo;s pass the inputs 2 and 3 to our network:\nprint net.activate(\\[2, 3\\]) Customizing your network structure You can check out the network structure, using the following print commands:\nfrom pybrain.tools.shortcuts import buildNetwork\n# building network net = buildNetwork(2, 3, 1); # activating network on input 2, 3 print net.activate(\\[2, 3\\]); # will display the network structure print net \u0026quot;\u0026quot;\u0026quot; output: FeedForwardNetwork-8 Modules: \\[, , , \\] Connections: \\[ 'out'\u0026gt;, 'hidden0'\u0026gt;, 'out'\u0026gt;, 'hidden0'\u0026gt;\\] \u0026quot;\u0026quot;\u0026quot; # output: print net\\['in'\\] # output: print net\\['out'\\] # output: print net\\['hidden0'\\] When using buildNetwork the hidden layer is constructed with a sigmoid squashing function. Let\u0026rsquo;s assume you would like to change the hidden layer to a different type of layer, i.e. Hyperbolic Tangent function. You can do so, by supplying the hidden layer class as an argument to buildNetwork constructor:\nfrom pybrain.tools.shortcuts import buildNetwork from pybrain.structure import TanhLayer from pybrain.structure import SoftmaxLayer # the hidden layer of network 1 is constructed # with Hyperbolic Tangent activation function net1 = buildNetwork(2, 3, 1, hiddenclass = TanhLayer) # the hidden layer of network 2 is constructed # with Softmax activation function net2 = buildNetwork(2, 3, 1, hiddenclass = TanhLayer, outclass = SoftmaxLayer) # network is using bias net3 = buildNetwork(2, 3, 1, bias = True) ### Building a DataSet SupervisedDataSet - this class is used for standard supervised learning. Supports input and target values whose size is defined. from pybrain.datasets import SupervisedDataSet # dataset supports 2-d input and 1-d target ds = SupervisedDataSet(2, 1) # data set for the XOR function ds.addSample((0, 0), (0)) ds.addSample((0, 1), (1)) ds.addSample((1, 0), (1)) ds.addSample((1, 1), (0)) # will print dataset length # output: 4 print len(ds) # iterating through dataset like a dictionary for input, target in ds: print input, target # will print inputs of data set print ds\\['input'\\] # will print targets of dataset print ds\\['target'\\] if you want to clear the dataset, you can use: \\# clear the data set ds.clear(); Training the Neural Network on the Dataset Now that we\u0026rsquo;ve got the network ready it\u0026rsquo;s time to train our network. We will do so by using back propagation algorithm - BackpropTrainer class.\nAll we have to do is to provide our network instance and dataset instance to the trainer - instantiated from BackpropTrainer class, and then run the train method.\nfrom pybrain.supervised.trainers import BackpropTrainer from pybrain.tools.shortcuts import buildNetwork from pybrain.datasets import SupervisedDataSet from pybrain.structure import TanhLayer # building network to be trained on XOR output net = buildNetwork(2, 3, 1, bias = True, hiddenclass = TanhLayer) # dataset supports 2-d input and 1-d target ds = SupervisedDataSet(2, 1) # data set for the XOR function ds.addSample((0, 0), (0)) ds.addSample((0, 1), (1)) ds.addSample((1, 0), (1)) ds.addSample((1, 1), (0)) trainer = BackpropTrainer(net, ds) # this will train the network for full epoch and return # double which is proportional to the error. print trainer.train() # will continue training until results converge # returns a list of tuples containing the errors for every training epoch print trainer.trainUntilConvergence() The example above is used just to show different functions and usages of these functions and won\u0026rsquo;t lead to effective results. The problem with the above example is that trainUntilConvergence by default requires validationProportion. The default value is 0.25, meaning that 25% of dataset will be used for the validation dataset. These two datasets are split and don\u0026rsquo;t intersect. The problem is that omitting 25% of dataset samples will lead to a badly trained network.\nYou can solve XOR problem using different approach:\n\\# learn XOR with a nerual network with saving of the learned paramaters import pybrain from pybrain.datasets import \\* from pybrain.tools.shortcuts import buildNetwork from pybrain.supervised.trainers import BackpropTrainer ds = SupervisedDataSet(2, 1) ds.addSample((0, 0), (0,)) ds.addSample((0, 1), (1,)) ds.addSample((1, 0), (1,)) ds.addSample((1, 1), (0,)) net = buildNetwork(2, 4, 1, bias=True) trainer = BackpropTrainer(net, learningrate=0.01, momentum=0.99) trainer.trainOnDataset(ds, 1000) trainer.testOnData() print net.activate((1, 1)) As you notice we are trainig our network explicitly for 1000 epochs and not until the results converge with expected output. You can validate the results of the network by rounding the output: \\# output: 0.0 # output: 1.0 # output: 1.0 # output: 0.0 print round(net.activate((0, 0))) print round(net.activate((0, 1))) print round(net.activate((1, 0))) print round(net.activate((1, 1))) Saving the neural network to file and loading it After searching the internet for XOR pybrain example I found some reliable code, which actually doesn\u0026rsquo;t ommit the solution space as mentioned in the example above. As there\u0026rsquo;s no validation set, the results cannot be tested for convergence, and we have to train for certain number of epochs. Eventually the results did converge, based on evidence.\nhttps://github.com/thedanschmidt/PyBrain-Examples/blob/master/xor.py\n\\# learn XOR with a nerual network with saving of the learned paramaters import pybrain from pybrain.datasets import \\* from pybrain.tools.shortcuts import buildNetwork from pybrain.supervised.trainers import BackpropTrainer import pickle if \\_\\_name\\_\\_ == \u0026quot;\\_\\_main\\_\\_\u0026quot;: ds = SupervisedDataSet(2, 1) ds.addSample((0, 0), (0,)) ds.addSample((0, 1), (1,)) ds.addSample((1, 0), (1,)) ds.addSample((1, 1), (0,)) net = buildNetwork(2, 4, 1, bias=True) try: f = open('\\_learned', 'r') net = pickle.load(f) f.close() except: trainer = BackpropTrainer(net, learningrate=0.01, momentum=0.99) trainer.trainOnDataset(ds, 1000) trainer.testOnData() f = open('\\_learned', 'w') pickle.dump(net, f) f.close() print net.activate((1, 1)) The beauty of the code above is that the first time you run it, the file _learned does not exists and the code will jump to except clause, which will train the network and popualate the _learned file. Afterwards we will activate the network with certain input in the main scope.\nIn any subsequent execution of the code, _learned file will be opened successfully and the training part will be skipped. We will be able to reuse our network directry from the file!\nNote the usage of pickle library which supports dumping / loading objects from / to the file.\n"
},
{
	"uri": "https://majdarbash.github.io/aws-csap/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": " Security Concepts  Shared Responsibility Model Principle of Least Privillege** Security Facets Typical Components SAML 2.0, OAuth 2.0, OpenID Connect AWS Artifact   Multi-Account Management Network Controls and Security Groups AWS Directory Services Credentials and Access Management Encryption  Key Management Service (KMS) CloudHSM (Hardware Security Module) AWS Certificate Manager   Distributed Denial of Services Attacks IDS and IPS AWS Service Catalog  AWS Service Catalog Constraints     Security Concepts Shared Responsibility Model  AWS - Reponsbility for Security of the Cloud Customer - Responsibility for Security in the Cloud  Principle of Least Privillege**  Give users (or services) nothing more than those privileges required to perform their intended function. (and only when they need them)  Security Facets  Identity  Who are you?   Authentication  Prove that you\u0026rsquo;re who you say   Authorization  Are you allowed to do this?   Trust  Do other entities that I trust say they trust you    Typical Components  Identities  Users who want to grant access to certains services   Identity Providers  e.g. Facebook, Google, etc..   Identity Broker  Facilitates communication with identity to grant access to certain services   Identity Store  Contains information about identities privileges   Services  SAML 2.0, OAuth 2.0, OpenID Connect SAML 2.0\n Can handle both authorization and authentication XML-based protocol Can contain user, group, membership and other useful information Assertions in the XML for authentication, attributes or authorization Best suited for Single Sign-On for enterprise users  OAuth 2.0\n Allow sharing of protected assets without having to send login credentials Handles authorization only, not authentication Issues token to client Application validates token with Authorization Server Delegate access, allowing the client applications to access information on behalf of user Best suited for API authorization between apps  OpenID Connect\n Identity layer built on top of OAuth 2.0, adding authentication Uses REST/JSON message flows Supports web clients, mobile clients, Javascript clients Extensible Best suited for Single Sign-on for consumer  AWS Artifact  AWS resource for compliance-related information Provides on-demand access to AWS\u0026rsquo; security and compliance reports and select online agreements  Multi-Account Management  AWS Organizations Service Control Policies Tagging Resource Groups Consolidated Billing  Identity Account Structure\n Manage all user accounts in one location Users trust relationship from IAM roles in sub-accounts to identify Account to grant temporary access Variations include by Business Unit, Deployment, Environment, Geography  Logging Account Structure\n Centralized logging repository Can be secured so as to be immutable Can use Service Control Policies (SCP) to prevent sub-accounts from changing logging settings  Publishing Account Structure\n Common repository for AMI\u0026rsquo;s, Containers, Code Permits sub-accounts to use pre-approved standardized services or assets  Central IT Account Structure\n IT can manage IAM users and groups while assigning to sub-account roles IT can provide shared services and standardized assets (AMI\u0026rsquo;s, databases, EBS, etc.) that adhere to corporate policy  Network Controls and Security Groups Security Groups\n Virtual firewalls for individual assets (EC2, RDS, AWS Workspaces, etc..) Controls inbound and outbound traffic Port or port ranges Inbound rules are by SourceIP, Subnet or other Security Groups Outbound rules are by Destination IP, Subnet, or other SG  Network Access Control Lists (NACLs)\n Additional layer of security for VPC that acts as a firewall Apply to entire subnets rather than individual assets Default NACL allows all inbound and outbound traffic NACLs are stateless - outbound traffic simply obeys outbound rules - no connection is maintained Can duplicate or futher restrict access with SG Remmeber ephermal ports for OUtbound if you need them  Why NACLs?\n NACLs provide a backup method of security if you accidentally change yoiur SG to be too permissive Covers the entire subnet so users to create new instances and fail to assign a proper SG are still protected Part of a multi-layer Least Privilege concept to explicitly allow and deny  AWS Directory Services  AWS Cloud Directory  Cloud-native directory to share and control access to hierarchical data between applications Cloud appliocations that need hierarchical data with complex relationships   Amazon Cognito  Sign-up and sign-in functionality that scales to millions of users and federated to public social media services Develop consumer apps or SaaS   AWS Directory Service for Microsoft Active Directory  AWS-managed full Microsoft AD (standard or enterprise) running on Windows Server 2012 R2 Enterprises that want hosted Microsoft AD or you need LDAP for Linux Apps   AD Connector  Allow on-premises users to log into AWS services with their existing AD credentials. Also allows EC2 instances to join AD domain. Single sign-on for on-prem employees and for adding EC2 instances to the domain   Simple AD  Low scale, low cost AD implementation based on Samba Simple user directory, or you need LDAP compatibility    Credentials and Access Management AWS Security Token Service (STS)\nToken Vending Machine Concept\n Common way to issue temporary credentials for mobile app development Anonymous TVM (Token Vending Machine) - Used as a way to provide access to AWS services only, does not store user identity Identity TVM - used for registration and login, and authorizations AWS now recommends that mobile developers use Cognito and related SDK  AWS Secret Manager\n Store passwords, encryption keys, API keys, SSH keys, PGP keys, etc\u0026hellip; Alternative to storing passwords or keys in a \u0026ldquo;vault\u0026rdquo; Can access secrets via API with fine-grained access control provided by IAM Automatically rotate RDS database credentials for MySQL, PostgreSQL and Aurora Better than hard-coding credentials in scripts or application  Encryption  Encryption at Rest  Data is encrypted such as on EBS, on S3, in an RDS database or in an SQS queue waiting to be processed   Encryption in Transit  Data is encrypted as it flows through a network or process, such as SSL/TLS for HTTPS, or with IPSec for VPN connections    Key Management Service (KMS)  Key storage, management and auditing Tightly integrated into many AWS services, like Lambda, S3, EBS, EFS, DynamoDB, SQS, etc. You can import your own keys or have KMS generate them Control who manages and accesses keys via IAM users and roles Audit use of keys via CloudTrail Differs from Secret Manager as its purpose-build for encryption key management Validated by many cmpliance schemes  CloudHSM (Hardware Security Module)  Dedicated hardware device, Single Tenanted Must be within a VPC and can access via VPC Peering Does not natively integrated with many AWS services like KMS, but rather requires custom application scripting Offload SSL from web servers, act as an issuing CA, enable TDE for Oracle databases  Cloud HSM vs KMS\n CloudHSM  Single-Tenant HSM Customer-managed durability and available Customer managed root of trust Broad 3rd Party Support   AWS KSM  Multi-Tenant AWS Service Highly available and durable key storage and management AWS managed root of trust Broad AWS Service Support    AWS Certificate Manager  Managed service that lets you provision, manage and deploy public or private SSL/TLS certificates Directly integrated into many AWS services like CloudFront, ELB and API Gateway Free public certificates to use with AWS services; no need to register via a 3rd party certificate authority You can import 3rd party certificates for use on AWS Supports wildcard domains Managed certificate renewal Can create a managed Private Certificate Authority as well for internal and proprietary apps, services or devices  Distributed Denial of Services Attacks  Phishing is one of the common way to compomise the systems Amplification/Reflection Attacks  Send small request to NTP server by manipulating data packets (NTP Monlist) NTP server replies to target device with a big response (MONLIST command output) Target device is flooded   Application Attacks (Layer 7)  HTTP GET Flood    Mitigating DDoS\n Minimize attack surface  NACLs, SGs, VPC Design   Scale to absorb attack  Auto-Scaling Groups, AWS CloudFront, Static Web Content via S3   Safeguard exposed resources  Route 53, AWS WAF, AWS Shield   Learn normal behavior  AWS GuardDuty, CloudWatch    IDS and IPS  Intruder Detection System  watches network to identify someone trying to compromise the system   Intruder Prevention System  Prevents exploits, sits behind firewall and scans and analyzes suspicious content   Systems usually consist of a Collection/Monitoring system and monitoring agents on each system Logs collected in CloudWatch, S3 or other tools are sometimes called Security Information and Event Management (SIEM) system AWS marketplace has different third-party choices of IDS and IPS appliances  CloudWatch\n Log events across AWS services Higher-level comprehensive monitoring and eventing Log from multiple accounts Logs stored indefinetely Alarms history for 14 days  CloudTrail\n Log API activity across AWS services More low-level granular Log from multiple accounts Logs stored to S3 or CloudWatch indefinetely No native alarming; Can use CloudWatch alarms  AWS Service Catalog  Framework allowing administrators to create pre-defined products and landscapes for their users Granular control over which users have access to which offerings Make use of adopter IAM roles so users don\u0026rsquo;t need underlying service access Allows end users to be self-sufficient while uploading enterprise standards and deployments Based on CloudFormation templates Administrators can version and remove products. Existing running product versions will not be affected.  AWS Service Catalog Constraints  Launch Constraint  IAM role that Service Catalog assumes when an end-user launches a product.   Notificaiton Constraint  SNS topic to receive notifications about stack events.   Template Constraint  One or more rules that narrow allowable values an end-user can select    "
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/2019-11-21-isolation-forest-algorithm/",
	"title": "Anomaly Detection: Isolation Forest Algorithm",
	"tags": [],
	"description": "",
	"content": " Defining the Anomaly Detection Problem  Methods to Resolve Anomaly Detection Problem Well Defined Anomaly Distribution Assumption   Isolation Forest Algorithm  Algorithm Steps Example     Defining the Anomaly Detection Problem  Data is set of vectors in d-dimensional space  $x_1$, \u0026hellip; $x_n$ each $x_i$ $\\epsilon$ $R^d$   Mixture of nominal points and anomaly points Anomaly points are generated by different generative process than the normal points  Solutions\n Supervised  Training data labeled with \u0026ldquo;nominal\u0026rdquo; or \u0026ldquo;anomaly\u0026rdquo;   Clean  Training data are all \u0026ldquo;nominal\u0026rdquo;, test data contaminated with \u0026ldquo;anomaly\u0026rdquo; points   Unsupervised  Training data consists of mixture of \u0026ldquo;nominal\u0026rdquo; and \u0026ldquo;anomaly\u0026rdquo; points    Methods to Resolve Anomaly Detection Problem  Density Based  DBSCAN LOF   Distance Based  KNN K-MEANS Regression hyperplane distance   Parametric  GMM Single Class SVMs Extreme value theory    Well Defined Anomaly Distribution Assumption   WDAD: the anomalies are drawn from a well-defined probability distribution\n  The WDAD assumption is often risky\n adversarial situations (fraud, insider threats, cyber security) diverse set of potential causes user\u0026rsquo;s notion of anomaly changes with time     Isolation Forest Algorithm  Builds an ensemble of random trees for a given data set Anomalies are points with the shortest average path length  Assumes that outliers takes less steps to isolate compared to normal point in any data set   Anomaly score is calculated for each point based on the formula: $2^{-E(h(x))/c(n)}$  $h(x)$ is the number of edges in a tree for a point $x$ $c(n)$ is a normalization constant for a data size of size $n$   Isolation score can be used in both supervised and unsupervised setting  Algorithm Steps  Sampling for Training  Choose a sampling proportion from the original data set   Generate a Binary Decision Tree  Split based on 2 random elements  Randomly choose an attribute Randomly choose a value of an attribute in its range of values   Perform a split to branch the tree   Repeat the process iteratively for the sub-data set After generating a Tree, repeat  Create a Forest, collection of Trees Stop when maximum number of Trees is reached   Feed data set an calculate anomaly score for each data point  Calculate anomaly score for a data point across Tree, using the equation:  $2^{-E(h(x))/c(n)}$   Average out the calculated anomaly scores   Score Interpretation  Anomalies will get a score closer to 1 Scores much smaller than 0.5 indicates normal observations If all scores are close to 0.5 then the entire sample doesn\u0026rsquo;t seem to have clearly distinct anomalies    Example In the following example we are using python\u0026rsquo;s sklearn library to experiment with the isolation forest algorithm. In the example below we are generating random data sets:\n Training Data Set  Required to fit an estimator   Test Data Set  Testing Accuracy of the Isolation Forest Estimator   Outlier Data Set  Testing Accuracy in detecting outliers    Generated Data:\n# importing libaries ---- import numpy as np import pandas as pd import matplotlib.pyplot as plt from pylab import savefig from sklearn.ensemble import IsolationForest # Instantiating container for Mersenne Twister pseudo-random number generator with a seed of int 42 rng = np.random.RandomState(42) # Generating 2 clusters of data using \u0026quot;Standard Normal\u0026quot; distribution # In Standard Normal distribution 95% of data lies within +-2 # Multiplying by 0.2 leaves 95% within +-0.4 X_train = 0.2 * rng.randn(1000, 2) print((abs(X_train[:, 0]) \u0026lt;= 0.4).sum() / len(X_train[:, 0]) * 100, \u0026quot; percent of data lies within 2 devitations from the mean\u0026quot;) # Generating second cluster of data (5,5) points away from the center of the first cluster in both axes X_train = np.r_[X_train + 5, X_train] # Generating the Dest Data using the same distribution of the training data X_test = 0.2 * rng.randn(100, 2) # Second cluster of the Test Data as well X_test = np.r_[X_test + 5, X_test] # Generating outliers spread throughout the plot using uniform distribution X_outlier = rng.uniform(low=-1, high=6, size=(50, 2)) # Visualizing the generated data sets: Training - blue, Test - green, Outliers - red plt.title(\u0026quot;Generated Data Sets\u0026quot;) plt.scatter(X_train[:, 0], X_train[:, 1], c='blue') plt.scatter(X_test[:, 0], X_test[:, 1], c='green') plt.scatter(X_outlier[:, 0], X_outlier[:, 1], c='red') plt.show() # Fitting the Isolation Forest Estimator with Training Data # Contamination factors indicates the percentage of data we believe to be outliers clf = IsolationForest(behaviour='new', max_samples=100, random_state=rng, contamination=0.1) clf.fit(X_train) # Running predictions on the Test and Outliers Data Sets using the estimator pred_test = clf.predict(X_test) pred_outlier = clf.predict(X_outlier) print(\u0026quot;Accuracy with Test Data: \u0026quot;, (pred_test == 1).sum() / len(pred_test)) print(\u0026quot;Accuracy In Outlier Detection: \u0026quot;, (pred_outlier == -1).sum() / len(pred_outlier)) "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/migrations/",
	"title": "Migrations",
	"tags": [],
	"description": "",
	"content": " Migration Strategies Cloud Adoption Framework  TOGAF Cloud Adoption Phases   Hybrid Architectures Migration Tools Network Migrations and Cutovers Amazon Snow Family  Migration Strategies  Re-Host: Simply move assets without change, e.g. on-prem MySQL to EC2. (Lift and Shift) Re-Platform: Move assets but change underlying platform, e.g. Migrate MySQL DB to RDS MySQL. (Lift and Reshape) Re-Purchase: Abandon existing and purchase new. (Drop and Shop) Rearchitect: Redesign application in a cloud-native manner, e.g. create Serverless version of legacy application. Retire: Get rid of applications which are not needed. Retain: Do nothing - decide to reevaluate at a future date.  Cloud Adoption Framework TOGAF The Open Group Architectural Framework\n Approach for designing, planning, implementing and governing enterprise IT architectures Started development in 1995 De-facto standard in Enterprise Architecture practice Favored EA framework for most Fortune 500 companies  Cloud Adoption Phases  Project Running projects to get familiar and experience the benefits from the cloud. Foundation Build foundation to scale the cloud adoption. Creating landing zone (pre-configured, secure, multi-account AWS environment), Cloud Center of Excellence (CCoE), operations model, as well as assuring security and compliance readiness. Migration Migrate existing applications including mission-critical applications or entire data centers to the cloud as you scale your adoption across a growing portion of your IT portfolio. Reinvention Focus on reinvention by taking advantage of the flexibility and capabilities of AWS to transform your business by speeding time to market and increasing the attention on innovation.  Holistic Approach to Cloud Adoption Framework:\n Business  Creation of a strong business case for Cloud Adoption Business goals are in harmony with cloud objectives Ability to measure benefits (ROI - Return on Investment, TCO - Total cost of Ownership)   People  Evaluate organizational roles and structures, new skills and process needs and identify gaps Incentives and Career Management aligned with evolving roles Training options appropriate for learning styles   Governance  Portfolio Management geared for determining cloud eligibility and priority Program and Project management more agile projects Align KPI\u0026rsquo;s with newly enabled business capabilities   Platform  Resource provisioning can happen with standardization Architecture patterns adjusted to leverage cloud-native New application development skills and processes enable more agility   Security  Identiy and Access Management modes change Logging and Audit capabilities evolve Shared Responsibility Model removes and adds some facets   Operations  Service monitoring has potential to be highly automated Performance management can scale as needed Business continuity and disaster recovery takes on new methods in the cloud    Hybrid Architectures  Using cloud resources along with on-prem resources First step as a pilot for cloud migrations VMWare - infrastructure can be extended to the cloud Integrations should be loosely coupled  Examples:\n Storage Gateway  Creates a bridge between on-prem and AWS Seamless to end users Common first step due to low risk and appealing economics   Middleware  Great way to leverage cloud services Loosely coupled, canonical-based Example: Can publish messages from Corporate Landscape to SQS to be consumed on the other side by a worker   VMWare vCenter Plugin  Allows transparent migration of VMs to and from AWS VMWare Cloud furthers this concept with more public-native features    Migration Tools  Storage Migration  AWS Storage Gateway AWS Snowball   Server Migration Service  Automates migration of on-prem VMWare vSphere or Microsoft Hyper-V/SCVMM virtual machines to AWS Replaces VMs to AWS, sync volumes and create periodic AMIs Minimizes cutover downtime by syncing VMs incrementally Supports Windows and Linux VMs only (like AWS) The Server Migration Connector is downloaded as a virtual appliance into your on-prem vSphere or Hyper-V setup   Database Migration Service (DMS)  Along with Schema Conversion Tool (SCT) helps customers migrate databases to AWS RDS or EC2-based databases SCT can copy database for homogenous migrations and covert schemas for heterogenous migrations DMS is used for smaller, simpler conversions and also supports MongoDB and DynamoDB SCT used for larger, more complex datasets like data warehouses DMS has replication function for on-prem to AWS or to Snowball or S3   Application Discovery Service  Gathers information about on-prem data centers to help in cloud migration planning Collects configs, usage and behavior data from your servers to help in estimating TCO of running on AWS Can run agent-less (VMWare Environment) or agent-based (non-VMWare Environment) Only supports those OSes that AWS Supports   AWS Migration Hub  Network Migrations and Cutovers  Ensure there\u0026rsquo;s no overlap between VPC and on-prem Most migrations start with VPN to AWS With higher usage, you may choose AWS Direct Connect and keep the VPN as backup Transition from VPN to Direct Connect can be done seamlessly using BGP (BGP weighing or static routes)  Amazon Snow Family  Evolution of AWS Import/Export process Move massive amount of data to and from AWS Data is encrypted at rest Data is encrypted at transit  Solutions:\n AWS Import/Export Ship an external hard drive to AWS. AWS plugs it in and copies the data to S3. AWS Snowball NAS in a box AWS ships to you. You can copy over 80TB of data and ship it back. Data will be available on S3. AWS Snowball Edge Similar to Snowball with some computing power, like Lambda and Clustering. AWS Snowmobile Shipping container full of storage (up to 100PB) and a truck to transport it.  "
},
{
	"uri": "https://majdarbash.github.io/python-2.7/4-python-conditional-control-flows/",
	"title": "Python 2.7: Conditional &amp; Control Flows",
	"tags": [],
	"description": "",
	"content": "Python supports:\n Standard comparison operators: \u0026lt;, \u0026gt;, ==, !=, \u0026lt;=, \u0026gt;=. Logical operators: and, or, not (not is evaluated first, then and and then or)  Conditional Clause if [condition]: [statements ...] else: [statements ...] More sofisticated example, would be:\nif [condition 1]: [statements ...] elif [condition 2]: [statements ...] else: [statements ...] Beware of the indentation as it will indicate how many statements to be executed within the condition success block. Example:\nx_int = 12 if not x_int \u0026gt; 20: print \u0026quot;The value of our variable is %s\u0026quot; % x_int print \u0026quot;We used an if condition here\u0026quot; Function Definition Function can be defined using the following syntax:\ndef [function name](): [statements ... ] Example of function definition:\n# function defintion - notice the indentation defining the scope def get_user_input(x_int): if x_int % 2 == 0: print \u0026quot;You've got an even number\u0026quot; else: print \u0026quot;You've got an odd number\u0026quot; # obtaining raw input represented as string x_string = raw_input(\u0026quot;Type your number: \u0026quot;) # converting the string to integer x_int = int(x_string) # calling the above defined function get_user_input(x_int) "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/architecting-to-scale/",
	"title": "Architecting to Scale",
	"tags": [],
	"description": "",
	"content": " Architectural Patterns  Loosely Coupled Architecture Horizontal Scaling vs. Vertical Scaling   Auto-Scaling  Type of Auto-Scaling Amazon EC2 Auto-Scaling Options Auto-Scaling Policy Scaling Cooldown Period   AWS Kinesis DynamoDB Scaling  DDB Terminology DDB Partitions and Scaling DynamoDB Accelerator - DAX   CloudFront SNS (Simple Notification Service) SQS  Queue Types   Amazon MQ AWS Lambda, Serverless Application Manager and EventBridge  AWS Serverless Application Model (AWS SAM)   Amazon EventBridge Simple Workflow Service (SWF) AWS Step Functions AWS Batch Elastic MapReduce  Components of Elastic MapReduce    Architectural Patterns Loosely Coupled Architecture Components can stand independently and require little or no knowledge of the inner workings of the other components.\nBenefits:\n Layers of Abstraction Permits more flexibility Interchangable components More atomic functional units  Horizontal Scaling vs. Vertical Scaling  Vertical scaling requires downtime Horizontal scaling is theoretically unlimited In horizontal scaling instances can be added on demand which may be a more cost effective solution Horizontal scaling can be automated while vertical scaling would require scripting Operations  Scale Out (horizontal) Scale In (horizontal) Scale Up (vertical) Scale Down (vertical)    Auto-Scaling Type of Auto-Scaling  Amazon EC2 Auto-Scaling Application Auto-Scaling  API used to to control scaling for resources other than EC2, like DynamoDB, ECS, EMR Provides a common way to interact with the scalability of resources   AWS Auto Scaling  Provides centralized way to manage scalability for whole stacks; Predictive scaling feature Console that can manage both of the above from a unified standpoint    Amazon EC2 Auto-Scaling Options  Maintain - Specific minimum number of instances running Manual - Use maximum, minimum or specific number of instances Schedule - Scale in/out based on schedule Dynamic - based on real-time metrics of the system  Auto-Scaling Policy  Target Tracking Policy Simple Scaling Policy Step Scaling Policy (More Sophisticated Logic)  Scaling Cooldown Period  Gives resources time to stabilize before automatically triggering another scaling event Different from health check period 300 seconds by default Automatically applies to dynamic scaling and optionally to manual scaling but not supported for schedule scaling  AWS Kinesis   Collection of services for processing streams of various data\n  Data is processed in \u0026ldquo;shards\u0026rdquo; - each shard can ingest 1000 records per second\n  Default limit of 500 shards\n  Record consists of Partition Key (128 bit MD5 hash), Sequence Number and Data Blob (up to 1MB)\n  Sequence numbers can be duplicated across Shards\n  Transient Data Store - default retention period of 24 hours, can be configured to up to 7 days\n  Kinesis Data Streams - Ingest and stores data streams for processing\n  Kinesis Firehose - Prepares and loads the data continously to the destinations you choose\n  Kinesis Data Analytics - Run standard SQL queries against data streams\n  DynamoDB Scaling  Throughput: Read/Write capacity units Max item size is 400KB There\u0026rsquo;s no limit on number of items  DDB Terminology  Parition - physical space where DDB data is stored Partition Key - Unique identifier for each record, also called Hash Key Sort Key - Optional second part of a composite key that defines storage order - sometimes called a Range Key  DDB Partitions and Scaling  Partitions have limitation of Capacity Units and Storage Number of Partitions required are determined by both factors  Capacity - RCU / 3000 + WCU / 1000 Storage - Total Size / 10GB Total Partitions = Round Up Max(Capacity, Storage)   RCU and WCU will be equality allocated across partitions Partition Key should be designed to have high avariability across paritions to distribute the WCUs and RCUs load across the partitions DynamoDB allows Auto-Scaling based on Target Utilization and Limits  Supports Global Secondary Indexes Uses Target Tracking method Doesn\u0026rsquo;t scale down if consumptions drops to zero Workaround1: send requests to table at minimal level Workaround2: manually reduce max capacity to be the same as minimum   DynamoDB supports On-Demand scaling  Costs more than traditional provisioning and auto-scaling    DynamoDB Accelerator - DAX  Sits in from of DDB and provides in-memory caching Micro-second level reads Good for read-intensive applications  CloudFront  Supports static / dynamic content at edge locations Supports Adobe Flash Media Server\u0026rsquo;s RTMP protocol Web Distributions support streaming through HTTP / HTTPS Origins can be S3, EC2, ELB and another Web Server Cache invalidation requests can delete the file from the edge location or you have to wait for TTL to expire Support Zone Apex (domain without subdomain infront of it) Supports Geo-Restriction  SNS (Simple Notification Service)  Enables Publish/Subscribe design pattern Topics - Channels for publishing notifications Subscriptions - configuring an endpoint to receive messages published to a topic  Endpoint options: HTTP/HTTPS, Email, SMS, SQS, Amazon Device Messaging (push notifications), Lambda   Supports Fan-out Architecture - helps achieve a loosely coupled architecture  SQS  Highly scalable hostead messaging queue Available integration with KMS for encrypting messages Transient Storage - default 4 days, max 14 days Supports first-in / first-out queueing Maximum message size of 256KB - Java SDK allows up to 2GB by utilizing S3 Allows Loosely Coupled Architecture  Queue Types  Standard Queue - no guarantee about the order of the messages FIFO Queue- maintains receiving order - holds all messages until a message is processed  Amazon MQ  Managed, HA Implementation of Apache ActiveMQ Similar to SQS, but a different implementation Supports different protocols Designed as a drop-in replacement for on-premise message brokers (Lift and Shift to the Cloud) Recommended to use SQS if you are building a new application from scratch  AWS Lambda, Serverless Application Manager and EventBridge  Run code on-demand without the need for infrastructure Supports Node.js, Python, Java, Go and C# Code is stateless - executed on an event basis (SNS, SQS, S3, DynamoDB Streams, etc.) Very useful for event driven architectures No limits to scaling a function since AWS dynamically allocates capacity in relation to events  AWS Serverless Application Model (AWS SAM)  Open source framework for building serverless apps on AWS Uses YAML as configuration language Includes CLI functionality to create, deploy and update serverless apps using AWS services such as Lambda, DynamoDB and API Gateway Enables local testing and debugging of apps using a Lambda-like emulator via Docker Extension of CloudFormation so you can use everything CloudFormation can provide by way of resources and functions AWS Serverless Application Repository - contains sample apps Serverless Framework is different from AWS SAM - supports other provides besides AWS  Amazon EventBridge  Ingest events from your own apps, SaaS and AWS Services Setup rules to filter and send events to targets  Simple Workflow Service (SWF)  Create distributed asynchronous systems as workflows Supports both sequential and parallel processing Best suited for human-enabled workflows, e.g. order fulfillment or procedural requests AWS recommends Step Functions over SWF for new applications Main Components: Activity Worker, Decider (Activity Workers are doing long-polling) AWS Simple Workflow is used when we need to support external processes processes or specialized execution logic (maybe beyond the scope of AWS)  AWS Step Functions  Managed Workflow and Orchestration platform Scalable and Highly Available Defined your app as a state machine Create tasks, sequential steps, parallel steps, branching paths or timers Amazon State Language declarative JSON Apps can interact and update the stream via Step Function API Visual Interface describes flow and realtime status Detailed logs for all the steps Out-of-the box coordination of AWS components (e.g. Order processing flow) Recommended by AWS over Simple Workflow Service for new applications  AWS Batch  Management tool for creating, managing and executing batch-oriented tasks using EC2 Instances   Create a Computer Environment: Managed/Unmanaged, Spot, On-Demand, vCPUs Create a Job Queue with priority and assigned to a Comput Environment Create Job Definition: Script/JSON, ENV vars, mount points, IAM role, container image, etc. Schedule the Job  Elastic MapReduce  Managed Hadoop framework for processing huge amounts of data Also supports Apache Spark, HBase, Presto and Flink Most common used for log analysis, financial analysis, or ETL (extract, transform and load) activities A Step is a programming task for performing some process on the data (i.e. count words) A Cluster is a collection of EC2 instances provisioned by EMR to run your steps Master Node, Core Node (HDFS), Task Node  Components of Elastic MapReduce  Hadoop HDFS - Distributed File System Hadoop MapReduce - Distributed Processing Flume - Log Collection ZooKeeper - Resource Coordination Sqoop - Data Transfer Oozie - Workflow Apache Pig - Scripting Hive SQL Mahout - Machine Learning HBase - Columnar Datastore Ambari - Management and Monitoring  "
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/2019-11-11-developing-ml-apps/",
	"title": "Developing Machine Learning Applications",
	"tags": [],
	"description": "",
	"content": " AWS SageMaker  Notebook Instance   AWS SageMaker Neo Machine Learning Algorithms  Linear Supervised Algorithms Non-Linear Supervised Algorithms Unsupervised Learning Algorithms Deep Learning  Convolutional Neural Networks (CNNs) Recurrent Neural Networks (RNNs)     Automatic Model Tuning  Hyperparameters Tuning Hyperparameters  Manual Brute Force Meta model SageMaker\u0026rsquo;s method     Advanced Analytics with Amazon SageMaker  Building and Training Machine Learning Models with Amazon SageMaker and Apache Spark  Building Machine Learning Pipelines using Spark and SageMaker      Anomaly Detection on AWS  Random Cut Forest Algorithm Architecture   Building Recommendation Systems with MXNet and GluOn  Collaborative Filtering  Matrix Factorization How to Choose a Model      AWS SageMaker AWS SageMaker is a fully managed service that allows the users to build, train and deploy machine learning models.\nUnderlying steps are fully managed by AWS through each of the stages:\n Build  Easily and quickly setup Jupyter Notebook instance   Train  Distributed training environment Spinning up a cluster of instances Loading Docker container which has an algorithm Bring in data from S3 Train the algorithms Output data back to S3 Tear-Down the cluster Allows you to use custom-build algorithm and bring in your own Docker container   Tune Deploy  Notebook Instance  Jupyter Notebooks  Open-source web application that allows users to author and execute code interactively Widely used by the data scientists community Can use any of AWS built-in models Docker image can be provided for endpoint deployment     AWS SageMaker Neo Challenges Faced\n Framework  Choose a framework best-suited for the task at hand   Models  Build the models using the chosen framework   Train  Train a mdoel using sample data to make accurate predictions on bigger data sets   Integrate  Integrate the model with the application   Deploy  Deploy the application, the model and the framework on a platform    AWS SageMaker Neo\n Helps developers take models trained in any framework and port them to any platform Converts model from Framework-specific format to portable code Optimizes the framework to run up to 2 times faster and 100x memory footprint reduction Supports populat deep learning and decision tree models Apache MXNet, TensorFlow, PyTorch and XGBoost Various EC2 instances and edge devices  AWS SageMaker Neo Components\n Neo Compiler Container  Takes the framework-specific model as an input Converts to framework-agnostic representation Optimzes the model performance Reduces the model\u0026rsquo;s runtime footprint   Shared Object Library   Machine Learning Algorithms Machine Intelligence\n Knowledge Acquisition  Inductive Reasoning Coming up with rules which would explain the observations   Inference  Ability to use acquired knowledge to derive truths Deductive Reasoning Predictions    Predicting Numbers\n Regression Problem  Linear Supervised Algorithms  Linear decision boundary  Hyperplane that best separates samples belonging to different classes   Linearly separable classes  If there exists a linear surface that separates 2 classses Error margin is expected in real-life situation   Support Vector Machine (SVM) Perceptron AWS SageMaker Linear Learner  Linear + Logstic Regression    Non-Linear Supervised Algorithms  Decision Tree RandomForest, XGBoost are based on these approaches Factorization Machines  Good for high dimensional sparse datasets Click prediction \u0026amp; item recommendation   Polynomial Neural Networks AWS SageMaker supports XGBoost - Gradient Boosted Trees  Unsupervised Learning Algorithms  Clustering  Given a collection of data points trying to divide the samples into clusters Assume that points belonging to the same cluster are somehow similar Number of clusters should be specified Different Algorithms and Hyperparameters will lead to different clusters What do clusters represent? (will need to be defined after running the algorithm)   Anomaly Detection  Random Cut Forest   Topic Modelling  Given collection of documents and number of topics to discover Algorithm produces top words appearing to define a topic   AWS  SageMaker Supports K-Means Clustering Principal Component Analysis (PCA)  Reduces dimensionality within a dataset Precursor to Supervised Learning   Latent Dirichlet Allocation (LDA)  Used for Topic Modelling by AWS Comprehend   Anomaly Detection  Kinesis Data Analytics Amazon SageMaker   Hot Spot Detection  Helps identify relatively dense regions in your data Supported Kinesis Data Analytics      Deep Learning  Neural Networks Neuron Back-propagation Algorithm  Factors in Advent of Deep Learning\n Algorithms Data Programming Models GPUs \u0026amp; Acceleration  Deep Neural Networks\n Image understanding Natural Language Processing Speech Recognition Autonomy  Networks with over thousand layers have been experimented with\n Training can be distributed across several instances AWS provides GPU powered instances on-demand  AWS SageMaker\n DeepAR Forecasting  Time Series Prediction    Convolutional Neural Networks (CNNs)  Breakthrough in deep learning Especially useful for image processing Able to corelated nearby pixels in an image instead of treating as completely independent input Convolution operation is applied to subsection of the image Use Cases  Object recognition, Image classification Semantic segmenetation Artistic style transfer Meow generator   AWS SageMaker  Image Classification (ResNet CNN)    Recurrent Neural Networks (RNNs)  Output of a Neuron is feeded to the Neuron itself Long Short-Term Memory (LSTM Network) AWS SageMaker  Sequence to Sequence (seq2seq)  RNN for text-summarization, translation, TTS       Automatic Model Tuning  Wraps up tuning jobs Works with custom algorithms or pre-built learning algorithms Helps find the best hyperparameters Improves performance of the machine learning model  Hyperparameters  Help tuning Machine Learning Model to get the best performance Have large influence on performance of ML Model Grows exponentially Non-linear / interact Expensive Evaluations  Neural Networks\n Learning Rate Layers Regularization Drop-out  Trees\n Number Depth Boosting step size  Clustering\n Number Initialization Pre-processing  Tuning Hyperparameters Manual  Defaults, guess and check Experience, intuition, and heuristics  Brute Force  Grid  We try out each possibility of hyperparameter values and compare based on this   Random  Randomly pick values for each of the hyperparameters   Sobol  Meta model  Builds another ML model on top of your ML model Objective to predict which hyperparameters which yield the best potential accuracy  SageMaker\u0026rsquo;s method  Gaussian process regression models objective metric as a function of hyperparameters  Assumes smoothness Low data Confidence estimates   Bayesian optimization decides where to search next  Explore and exploit Gradient free    SageMaker Integration\n Accepts SageMaker algorithms Frameworks Your own algorithm in a docker container  Flat Hyperparameters\n Continuous Integer Categorical  Advanced Analytics with Amazon SageMaker Building and Training Machine Learning Models with Amazon SageMaker and Apache Spark Apache Spark\n Powerful data processing tool Rich ecosystem Distributed processing  Spark and SageMaker Integration\n Spark runs locally on SageMaker notebooks The SageMaker-Spark SDK  Scala and Python SDK Amazon SageMaker algorithms are compatible with Spark MLLib There are Spark and Amazon SageMaker hybrid pipelines   Connect a SageMaker notebook to a Spark Cluster (e.g. Amazon EMR)  Spark / SageMaker Integration Components\n Spark DataFrame  Is a distributed data combined from different data sources   Estimator  Algorithm to use Parameters associated with the algorithm Types / Number of instances to host your model   Model  Training data is used to create a Model Model created    Building Machine Learning Pipelines using Spark and SageMaker Problem: Recognizing handwritten numbers 0-9 using MNIST data set.\nIn this example Apache Spark will pre-process data to do feature reduction using PCA (Principal Component Analysis). We instantiate PCA object proving the input: set of Features, and a target number of features k. PCA algorithm will choose the most significant features and return \u0026ldquo;Projected Features\u0026rdquo;. Those features will act as an input to the second stage in the pipeline.\nDefining the pipeline in Jupyter Notebook using both Spark SDK and SageMaker SDK will allow us to automate the process of pre-processing, training and deploying the model.\nAs a result we expect to have 2 Endpoints running on AWS SageMaker Infrastructure:\n Endpoint (PCA) Endpoint (K-Means)  This allows to fully de-couple the pre-processing task from prediction. On calling transform() function the Pipeline will first contact PCA Endpoint to reduce the features of the provided input and then will call K-Means Endpoint to get the prediction based on the Projected Features as an input.\nOur pipeline will consist of the following steps:\n Performing feature analysis/reduction on the input data set using PCA algorithm running on Apache Spark cluster  SageMaker Job will be created for running PCA feature reduction   Training on reduced feature data using K-Means algorith on AWS SageMaker  SageMaker Job will be created and will run automatically on completion of Step 1   Running Test-Data using the created AWS SageMaker Endpoint   Anomaly Detection on AWS Random Cut Forest Algorithm  Algorithm developed by AWS for Anomaly detection Improvement of an existing algorithm [\u0026ldquo;Isolation Forest\u0026rdquo;](/aws certified mls/2014/09/10/isolation-forest-algorithm.html) Published in ICML 2016 Incorporated into AWS Kinesis Data Analytics and AWS SageMaker  How it works\n Choose a bounding box randomly Choose the bigger dimension Perform the random cut Keep doing until each point is isolated Building a tree of random cuts Repeat the steps above to result in several trees, i.e. the Forest  Dealing with Stream of Data\n Reservoir Sampling  Maintain a random sample of 5 points in a stream Put first 5 observed data points in our sample buffer of size 5   For each new point, decide whether to keep or discard it  Flip a coin for each data point For the first data point, the probability of retaining the point is 50% For each new data point, it becomes less and less likely that the point will be retained   Produce a Random Cut Forest, with each tree looking into its own subsample of the stream For each new point we have to calculate the insertion point in the tree  Based on the insertion point in the tree we will have to perform displacements to accommodate the new point Average Displacement in Forest will represent Anomaly Score    Shingle\n Results can be improved using Shingling technique A single data point is replaced by a window of datas Shingle size will define the window size (e.g. 48 hours in the time axis)  Architecture Kinesis Streams\n Data from the source can be streamed to Kinesis Streams Kinesis Analytics can process the data from Kinesis Streams and label the Anomaly Score Using Amazon Kinesis Firehose the data can be redirected to S3 for further processing  SageMakers\n Data from the source can be passed to SageMaker through S3 After Training, Model will be deployed to the Endpoint Any new data can be checked for Anomaly using AWS Lambda function   Building Recommendation Systems with MXNet and GluOn Collaborative Filtering  User based recommenders  Identifies a short list of other users who are \u0026ldquo;similar\u0026rdquo; to you and rated an item. Assumes the average of their rating as your rating.   Item based recommenders  Identifies a short list of other items who are \u0026ldquo;similar\u0026rdquo; to the item in question and take a weighted average of the ratings for those top few items which you provided and predict that number as your likely rating for that item.    Matrix Factorization Factorizes a matrix to separate matrices, that when multiplied approximate to the completed matrix.\n Using GluOn Library MXNet with Linear Model MXNet with Non-Linear Model using Neural Network SageMaker  Points to Consider\n Matrix Factorization is ideal for small amounts of data Memory becomes a challenge with large data sets Factorization Machines and Distribution of the workload can help address the limitation and distribute the workload  Cold-Start Problem\n New users have no previous rating history New items have never been rated before Detecting similarity in items or users will help solve the Cold-Start Problem Content-based models  Lot of information can be extracted from item/user Search criterias, click behavior, etc.. provide more data supply whenever the rating history is not available    Hybrid Models\n Combine item/user-based models with content based  Semantic Models\n Finding similarities based on Semantic of the data (movie titles, description, untapped data like images) Using Deep Structure Semantic Models (DSSM)  How to Choose a Model "
},
{
	"uri": "https://majdarbash.github.io/python-2.7/5-python-classes/",
	"title": "Python 2.7: Classes",
	"tags": [],
	"description": "",
	"content": "In the example below of class syntax, the class in parantheses indicate the clsaas from which the new class inherits - in the current case it\u0026rsquo;s object.\nclass Person(object): \u0026quot;\u0026quot;\u0026quot;Person class\u0026quot;\u0026quot;\u0026quot; def __init__(self, name, phone, gender): self.name = name self.phone = phone self.gender = gender def description(self): print \u0026quot;Name: %s, phone: %s, gender %s\u0026quot; % (self.name, self.phone, self.gender) def is_male(self): if self.gender == 'male': return True else: return False def is_female(self): if self.gender == 'female': return True else: return False person = Person(\u0026quot;John\u0026quot;, \u0026quot;+011111xxxx\u0026quot;, \u0026quot;male\u0026quot;); # output: Name: John, phone: +011111xxxx, gender: male person.description(); # output: True print person.is_male(); ### Class scope example Notice how variable is_alive will be available to all the members of the Animal class: class Animal(object): is_alive = True def __init__(self, name): self.name = name zebra = Animal(\u0026quot;Jeffrey\u0026quot;) giraffe = Animal(\u0026quot;Bruce\u0026quot;) panda = Animal(\u0026quot;Tach\u0026quot;) print zebra.name, zebra.is_alive print giraffe.name, giraffe.is_alive print panda.name, panda.is_alive # output: Jeffrey True # output: Bruce True # output: Tach True Inheritance In the example below we have defined a class called Person which inherits from the object, and a class called Student, which inherits the Person.\nclass Person(object): def __init__(self, name, age): self.name = name self.age = age def description(self): print self.name, self.age class Student(Person): def __init__(self, name, age, studentId): self.studentId = studentId return Person.__init__(self, name, age) def info(self): print \u0026quot;Student id number is: %s\u0026quot; % (self.studentId) student = Student(\u0026quot;Tom Jones\u0026quot;, 26, 8928372) # output: Tom Jones 26 student.description() # output: Student id number is: 8928372 student.info() Accessing parent class using super() At any point in time you can access the parent / superclass of the current class by using super function:\nclass A(object): def test(self): print \u0026quot;This is the parent function\u0026quot; class B(A): def test(self): print \u0026quot;This is the overridden function in the child\u0026quot; def parent_test(self): super(B, self).test() # instantiating from the child class B instance = B() # accessing the test function # output: This is the overridden function in the child instance.test(); # accessing the parent function using super() # output: This is the parent function super(B, instance).test() # accessing the parent function using parent_test, which is using super() # output: This is the parent function instance.parent_test() Another example: using __repr__ and print methods: class Point3D(object): def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): return \u0026quot;(%d, %d, %d)\u0026quot; % (self.x, self.y, self.z) my_point = Point3D(1, 2, 3) # will use __repr__ function of the class # output: (1, 2, 3) print my_point. "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/business-continuity/",
	"title": "Business Continuity",
	"tags": [],
	"description": "",
	"content": " Concepts Disaster Recovery Architectures Storage Options Compute Options HA Approaches for Databases Network Options Failure mode and Effects Analysis (FMEA)  Concepts  Business Continuity (BC)  Seeks to minimize business activity disruption when something unexpected happens   Disaster Recovery (DR)  Act of responding to these events that threaten business continuity   High Availability  Designing in redundancies to reduce the chance of impacting service levels   Fault Tolerance  Design in the ability to tolerate faults   Service Level Argreement (SLA)  An agreed goal or target for a given service on its performance or availability   Recovery Time Objective (RTO)  Time taken after disruption to restore business processes to their levels   Recovery Point Objective (RPO)  Acceptable amount of data loss measured in time.    Business Continuity Plan specify RTO and RPO. RTO and RPO metrics then define amouint of investment on High Availability to be made. They also define what is the process for Disaster Recovery.\nTypes of Disaster:\n Hardware Failure Deployment Failure Load Induced (e.g. DDOS attacks) Data Induced Credential Expiration Dependency Infrastructure Identifier Exhaustion  Disaster Recovery Architectures  Backup and Restore   Requires Minimum entry point into AWS Minimal effort to configure Least flexible, off-site backup storage  Pilot Light   Minimal environment on standby on AWS for failover Switching to AWS will require a manual intervention It may take several minutes or hours to spin an environment AMIs should be up-to-date with on-prem counterparts  Warm Standby   Services are already up and running Could be considered as a shadown environment or production staging Resources could scale up to meet the incoming demand Process can be automated  Multi-Site   Ready at all time to take full production load Fails over in seconds or less No or little intervention required to fail over Most expensive DR option: can be considered as wasteful option Can be automatically configured through Route53 health checks  Storage Options Amazon EBS\n Annual Failure rate less than 0.2% Availability target of 99.999% (replicated within a single AZ) Vulnerable to AZ failure Easy to snapshot which is stored on S3 and multi-AZ durable You can copy snapshots to other regions Supports RAID configurations  Due to the facts that EBS operates over network, it\u0026rsquo;s not recommended to operate higher than RAID1    EBS RAID Configurations\n RAID0 (Striping)  No Redundancy Highest Speed Reads and Writes Highest Capacity   RAID1 (Mirroring)  1 drive can safely fail Slight decrease in reads and writes Capacity reduced by 50% due to mirroring   RAID5  Redundancy: 1 drive can safely fail 2 drives will store the data 1 drive stores the parity bit to be able to recreate the data good reads (similar to RAID0), but low writes capacity of (n-1)/n   RAID6  2 drives can safely fail minimum of 4 drives needed (2 parity) Same reads as RAID0, but worst writes capacity of (n-2)/n    S3 Storage\n Standard: 99.99% availability = 52 minutes/year Standard Infrequent Access: 99.9% = 9 hours/year One-zone Infrequent Access: 99.5% availability = 2 days/year Eleven 9s of durability: 99.99999999999% Standard \u0026amp; Standard-IA have multi-AZ durability. One-zone only as single AZ durability S3 is a backing service for many AWS services  Amazon EFS\n Implementation of the NFS file system True file system as opposed to block EBS or object storage (S3) File locking, strong consistency, concurrently accessible Each file object and metadata is stored across multiple AZs Can be accessed from all AZs concurrently Mount targets are highly available  Other Options\n Amazon Storage Gateway Snowball Glacier  Compute Options  Up-to-Date AMIs are critical for rapid fail-over AMIs can be copied to other regions for safety or DR staging Horizontally scalable architectures are preferred Reserved instances is the only way to guarantee that resources will be available when needed Auto Scaling and Elastic Load Balancing work together to provide automatic recovery by maintaining minimum instances Route 53 Health Checks also provide \u0026ldquo;self-healing\u0026rdquo; redirection of traffic  HA Approaches for Databases  If possible, choose DDB over RDS because of inherent faul tolerance Choose Aurora because of redundancy and automatic recovery features If aurora can\u0026rsquo;t be used choose multi-AZ RDS Frequent RDS snapshots can protect against data corruption or failure - and they wont\u0026rsquo; impact performance of multi-AZ deployment Regional replication is also an option, but will not be strongly consistent If hosting database on EC2, you have to develop your own HA plan  Redshift\n Currently Redshift doesn\u0026rsquo;t support multi-AZ deployments Best HA option is to use multi-node cluster which supports replication and node recovery Single node Redshift cluster does not support data replication - in case of failure will have to restore an S3 snapshot  Memcached does not support replication\n Use multiple nodes in each shard to minimize data loss on node failure Launch multiple nodes across available AZs to minimize data loss on AZ failure  Redis\n Use multiple nodes in each shard and distribute the nodes across multiple AZs Enable multi-AZ on the replication group to permit automatic failover if the primary node fails Schedule regular backups of your Redis cluster  Network Options  Subnets should be created in different AZs, resources should be allocated in multiple AZs Create at least 2 VPN tunnels to your Virtual Private Gateway Direct connect is not HA by default, you need to establish a secondary connection via another Direct Connect (ideally use another provider) or use a VPN Route 53\u0026rsquo;s Health Checks provide basic level of redirecting DNS resolutions Elastic IPs allow you flexibility to change backing assets without impacting name resolution For mutli-AZ redundancy of NAT Gateway, create gateways in each AZ with routes for private subnets to use the local Gateway  Failure mode and Effects Analysis (FMEA) FMEA is a systematic process to examine:\n What could go wrong What impact it might have What is the likelyhood of it occurring What is our ability to detect and react  Severity * Proability * Detection = Risk Priorty Number (RPN)\nSteps\n Round up Possible Failures Assign scores for each failure mode: customer impact, likelihood, detect and react -\u0026gt; calculate RPN Prioritize on Risk - implement mitigation plan, additional redundancy  "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/deployment-and-operations/",
	"title": "Deployment and Operations Management",
	"tags": [],
	"description": "",
	"content": " Software Deployments CI, CD, CD Elastic Beanstalk CloudFormation Elastic Container Service API Gateway Management Tools Enterprise Applications AWS Machine Learning Landscape  AI Services ML Services ML Frameworks \u0026amp; Infrastructure    Software Deployments Types of Deployment:\n Big Bang Phased Rollout Parallel Adoption  Deployment Strategies:\n Rolling Deployment  Changing launch configuration to specific version will rollout the changes   A/B Testing  Using Route53 we can specific the ALB which gets the traffic   Canary Release  Deploy new version on production - if no errors are detected, deploy the rest   Blue-Green Deployment  Create new ALB and ASG with new version Switch to the new version using Route53 Switch back to the old version using Route53 The goal of blue/green deployment is to achieve immutable infrastructure - you don\u0026rsquo;t make changes to your application after it\u0026rsquo;s deployed but redeploy altogether How to achieve Blue-Green Deployment on AWS:  Update DNS with Route53 to point to new ELB or instance Swap ASG of new instances behind the ELB Change ASG LC to use new AMI and terminate old instances Swap environment URL on Elastic Beanstalk Clone stack using AWS Opswork and update DNS      Blue-Green Deployments are not recommended:\n If the code is very tightly coupled with database schema  Schema should be forward and backward compatible in best case   The upgrade requires special upgrade routines to be run during deployment Off-the-shelf products might not be blue-green friendly  CI, CD, CD  Continuous Integration  Merge code changes to main branch as frequently as possible with automated testing as you go   Continous Delivery  You have automated your release process to the point you can deploy at the click of a button   Continuous Deployment  Each code change that passes all stages of the release process is released to production with no human intervention required    Continuous Integration Pipeline:\n Get latest from Repo Make changes Unit Testing Commit to Repo Integration Testing Acceptance Testing Deploy to Production Smoke Testing  CI/CD Consideration:\n Objective is to create smaller, incremental compartmentalized improvements and features Lowers deployment risk and tries to limit negative impact Test Automation must be strong Feature toggle patterns useful for dealing with in-progress features not read for release (versus more traditional branching strategies) Microservice architectures lend themselves well to CI/CD practices  AWS Development Lifecycle Tools\n AWS CodeCommit AWS CodeBuild AWS CodeDeploy AWS CodePipeline AWS X-Ray AWS CodeStar  Elastic Beanstalk  Orchestration service to make it easy to deploy scalable web packages Wide range of supported platforms - Docker, PHP, Java, NodeJS Multiple Environments within Application (DEV, QA, PRD\u0026hellip;) Great for ease of deployment, but not great if you need a lot of control and flexibility  Deployment Options\n All At Once  minimal deployment time, downtime, manual rollback process   Rolling  one by one - terminates old version instances and replaces the new instances no downtime expected, manual rollback process   Rolling with Additional Batch  launch new version instances, and then take old instances out of service no downtime, manual rollback process   Immutable  launch a full set of instances in a separate ASG and cut over when health checks are passed no downtime, terminate new instances to rollback   Blue/Green  CNAME DNS entry is changes when new version is up, old version is preserved no downtime, rollback is achieved through URL SWAP    CloudFormation  Infrastructure as Code JSON/YAML to model and provision entire landscapes Repeatable, automatic deployments and rollbacks Nest components for reusability Supports over 300 Resource Types Supports custom resources via SNS / Lambda  Concepts\n Templates Stacks Change Sets  Stack Policies\n Protect certain resources from being unintentionally deleted or updated Once created, stack policy cannot be deleted, but can be modified via the CLI  CloudFormation Best Practices\n AWS Provides Python \u0026ldquo;helper scripts\u0026rdquo; which can help you install software and start services on EC2 CloudFormation should be used to make changes - don\u0026rsquo;t apply changes directly to the resources Make use of Change Sets to identity potential trouble spots Use Stack Policies to protect against accidential changes for sensitive resources Use version control systems like CodeCommit or Github to track changes to templates  Elastic Container Service There are 2 main services:\n ECS: Elastic Container Service  Managed, highly available and highly scalable platform AWS-specific platform that supports Docker containers Leverages AWS services: Route53, ALB, CloudWatch, etc. Collection of containers are called tasks, tasks provide a service Limited extensibility   EKS: Elastic Kubernetes Service  Managed, highly available and highly scalable platform Comptaible with upstream K8s so its easy to lift and shift from other K8s A hosted K8s platform that handles many things internally Collection of containers is called \u0026ldquo;Pods\u0026rdquo;. They can share resources and access to each other Extensible via a wide variety of third-party community ad-ons    Launch Types:\n Amazon EC2 Launch Type  You provision EC2 instances You are responsible for upgarding, patching and taking care of EC2 pool You must handle cluster optimization Allows more granular control over infrastructure   Amazon Fargate Launch Type  Fargate automatically provisions underlying resources Fargate provisions compute as needed Fargate handles cluster optimization Limited control, infra is automated    API Gateway  Managed, high availability service to front-end REST APIs Backed with custom code via Lambda, as a proxy for another AWS Service or any other HTTP API on AWS or elsewhere Regionally based, private or edge optimized (deployed via CloudFront) Supports API Keys, Usage Plans for user identitifcation, throttling and quota management Can be published as product and monetized via AWS marketplace API Gateway can cache responses  Management Tools AWS Config\n Allows to assess, audit and evaluate configuration of your AWS resources Very useful for Configuration Management as part of ITIL Program Creates baseline for various configuration settings and files - tracks variations against tha baseline AWS Config rules can check resources for certain desired conditions and if violations are found, the resources is flagged as \u0026ldquo;noncompliant\u0026rdquo;  AWS OpsWorks\n Managed instance of Chef or Puppet Provide configuration management to deploy code, automate tasks, configure instances, perform upgrades, etc. Has three offerings  OpsWorks for Chef Automate OpsWorks for Puppet OpsWorks Stacks  AWS creation compatible with Chef recipe Uses Chef solo client installed on EC2 instances to run Chef recipes Supports on-prem servers with agent installed      AWS OpsWorks Stacks\n Stacks  Collection of resourced needed to support a service or application   Layers  Different components of application delivery hierarchy Stacks can be cloned - but only within the same region When you create stack you must specify a region, this stack can control only resources in that region    AWS System Manager\n Centralized console for a wide variety of system management tasks Designed for managing a large fleet of systems - tens or hundreds SSM Agent enables System Manager features and support all OSs supported by OS as well as back to Windows Server 2003 SSM Agent is installed by default on recent AWS-provided base AMIs for Linux and Windows Manages AWS-based and on-prem based systems via the agent  Services:\n Inventory - collects information from managed instances about applications, versions, meta data State Manager - create states that represent a certain configuration is applied to instances Logging - CloudWatch Log agent and stream logs directly to CloudWatch from instances Parameter Store - storing configuration data, connecton strings, passwords Insights Dashboard - account-level view of Cloudtrail, Config, Trust Advisor Resource Groups - group resource through tagging for organization Mantenance Window - defined schedules for instances to patch, update apps, run scripts and more Automation - automating routine maintenance tasks and scripts Run Command - Run commands without SSH or RDP Patch Manager - automates the application of certain patches  uses baselines to defined which patches are auto-approved for update (e.g. pre-defined baseline for Windows Server 2008 - 2016) supports creation of custom basedline to define applying critical, optional, and important updates   AWS System Manager Documents  defines the actions that Systems Manager performs on your managed instances    AWS System Manager Documents\n Command Document  Run Command and State Manager Used to execute commands. State Manager uses command documents to apply a configuration. These actions can be run on one or more targets at any point during the lifecycle of an instance.   Policy Document  Used wiht State Manager Enforce a policy on your targets   Automation Document  Used with Automation Used when performing common maintenance and deployment tasks    Enterprise Applications  Amazon App-Stream  Enables user to run apps in the browser   AWS Client VPN Amazon Chime  Online meeting and video conferencing service   Alexa for Business  Deploy Alexa functionality and skills internally Management functionality more appropriate for an enterprise organization than buying and provisioning individual Alexa device   Amazon WorkLink  Provides secure access to internal web applications for mobile devices When mobile user requests an app it\u0026rsquo;s rendered on a secure machine then image is sent to the mobile client   Amazon WorkSpaces  Workspace as a service   AWS Connect  Fully managed contact center with configurable call handling, inbound / outbound telephony, interactive voice response, chatbot and analytics   Amazon WorkDocs  Document storage and collaboration platform: supports version management, sharing and collaborative edits   Amazon WorkMail  Fully managed email and calendar as a service Comptaible with Microsoft Exchange (Outlook), IMAP, Android and iOS clients    AWS Machine Learning Landscape AI Services App Developers, no ML experience required\n Amazon Comprehend Amazon Lex Amazon Polly Amazon Rekognition Amazon Translate Amazon Transribe Amazon Personalize  ML Services ML Developers and Data Scientists\n Amazon Sage Maker  Ground Truth Notebooks Training Hosting Algorithms Marketplace    ML Frameworks \u0026amp; Infrastructure ML Researchers and Academics\n Frameworks  MX Net TensorFlow   Interfaces  Gluon Keras   Amazon GreenGrass Amazon EC2 AWS Deep Learning AMIs  "
},
{
	"uri": "https://majdarbash.github.io/aws-csap/cost-management/",
	"title": "Cost Management",
	"tags": [],
	"description": "",
	"content": " Concepts Cost Optimization Strategies  Appropriate Provisioning Right-Sizing Purchase Options Managed Services Optimized Data Transfer   Tagging and Resource Groups  Resource Groups Resource Groups   Spot Instances and Reserved Instances  Reserved Instances Spot Instances   Cost Management Tools  Concepts  CapEx (Capital Expenses) OpEx (Operational Expenses) Total Cost of Ownership (TCO)  A comprehensive look at the entire cost model of a given decision or option often including both hard costs and soft costs.   Return on Investment (ROI)  The amount an entity can expect to receive back within a certain amount of time given an investment.    Cost Optimization Strategies Appropriate Provisioning  Provision the resources you need and nothing more Consolidate where possible for greater density and lower complexity (multi-database RDS, containers) CloudWatch can help by monitoring utilization  Right-Sizing  Using lowest-cost resource that still meets the technical specifications. Architecting for the most consistent use of resources is best versus spikes and valleys. Loosely coupled architectures using SNS, SQS, Lambda and DynamoDB can smooth demand and create more predictability and conistency.  Purchase Options  For permanent applications or needs, Reserved Instances provide the best cost advantage. Spot instances are best for temporary horizontal scaling. EC2 fleet lets you define target mix of On-Demand, Reserved and Spot instances to meet your needs. AWS Pricing can vary from region to region Consider potential savings by location resources in a remote region if local access is not required. Route53 and CloudFront can be used to reduce latency of a remote region.  Managed Services  Leverage managed services such as MySQL RDS over self-managed options such as MySQL on EC2 Cost savings gained through lower complexity and manual intervention (saving on soft costs)  Optimized Data Transfer  Data going out and between AWS regions can become a significant cost component. Direct Connect can be a more cost-effective option given data volume and speed.  Tagging and Resource Groups Resource Groups  Tags are arbitrary name/value pairs that you can assign to virtually all AWS assets to serve as metadata. Tagging strategies can be used for Cost Allocation, Security, Automation, and many other uses.  Example: tag used in IAM policy to implement access controls to certain resources.   Enforcing standardized tagging can be done via AWS Config Rules or custom scripts Most resources can have up to 50 tags.  Resource Groups  Resource Groups are grouping of AWS assets defined by tags. Create custom consoles to consolidated metrics, alarms and config details around given tags.  Common Resource Groupings:\n Environments - DEV,QA,PRD Project Resources Collection of resources supporting key business processes Resources allocated to various departments or cost centers  Spot Instances and Reserved Instances Reserved Instances  Purchase usage of EC2 instance in advance for a significant discount over On-Demand pricing Provides capacity reservation when used in a specific AZ AWS Billing automatically applies discounted rates when you launcgh an instance that matches your purchased RI. EC2 has three RI types: Standard, Convertible and Scheduled. RIs can be shared across multiple accounts within Consolidated Billing. If you find you don\u0026rsquo;t need your RI\u0026rsquo;s, you can try to sell them on the Reserved Instance Marketplace.  Standard vs. Convertiable Reserved Instances\n Terms: 1 / 3 year(s) Average Discount: Standard - 40%-60%, Convertible - 31%-54% Change AZ, Instance Size, Networking Type: yes Change instance family, OS, Tenancy, Payment Options: Standard - no, Convertible - yes Benefit from Price Reductions: Standard - no, Convertible - yes Sellable on RI Marketplace: Standard - yes, Convertible - coming soon  RI Attributes:\n Instance type Platform Tenancy - Defaul / shared Availability Zone (optional)  if zone is selected, discount applies to AZ - Zonal RI if no zone is specified, no reservation is created but the discount is applied to any instance in the family in any AZ in the region - Regional RI Zonal RI can be changed to Regional RI via console or AWS API Instance size flexibility is only available for Linux/Unix Regional RIs with default tenancy. Not available for Windows, RHEL, or SLES.    Spot Instances  Excess EC2 capacity that AWS tries to sell on a market exchange basis Customer creates a Spot Request and specifies AMI, desired instance types, and other key information Customer defines highest price willing to pay for instance. If capacity is constrained and others are willing to pay more, your instance might get terminated or stopped. Spot request can be a \u0026ldquo;fill and kill\u0026rdquo;, \u0026ldquo;maintain\u0026rdquo; or \u0026ldquo;duration-based\u0026rdquo; For \u0026ldquo;One-Time Request\u0026rdquo;, instance is terminated and ephemeral data is lost. For \u0026ldquo;Request and Maintain\u0026rdquo;, instance can be configured to Terminate, Stop or Hivernate until price point can be met again. Price and demand fluctuate between AZs  Dedicated Instances and Hosts Dedicated Instances:\n Dedicated instance is virtualized instances on hardware just for your AWS account May share hardware with other non-dedicated instances in the same account Available as On-Demand, Reserved Instances and Spot Instances Cost additional 2# per hour per region  Dedicated Host:\n Physical serveres dedicated to just your use You then have control over which instances anre deployed on that host. Available as On-Demand or with Dedicated Host Reservation Useful if you have server-bound software licenses that use metrics like per-core, per-socket or per-VM Each dedicated host can only run one EC2 instances size and type  Cost Management Tools  AWS Budgets  Set predefined limits and notifications Can be based on Cost, Usage, Reserved Instance Utilization or RI Coverage Useful as a method to distribute cost and usage awareness and responsibility to platform users   Consolidated Billing  Enable a single Payer account that\u0026rsquo;s locked down to only those who need access. Economies of scale by bringing together resource consumption across accounts.   Trusted Advisor  Run a series of checks on your resources and proposes suggested improvements Can help recommend cost optimization adjustments like RIs or scaling adjustments Core checks are available to all customers Full Trusted Advisor benefits require a Business or Enterprise support plan    "
},
{
	"uri": "https://majdarbash.github.io/aws/",
	"title": "AWS WhitePapers &amp; Deep Dives",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-cmls/",
	"title": "AWS Certified MLS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csap/",
	"title": "AWS Certified SAP",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/",
	"title": "AWS Certified SAA",
	"tags": [],
	"description": "",
	"content": "Please note that the content below is based from my notes.\n"
},
{
	"uri": "https://majdarbash.github.io/genetic-algorithm/",
	"title": "Genetic Algorithm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/ml-neural-networks/",
	"title": "Neural Networks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/blockchain/",
	"title": "Blockchain",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/python-2.7/",
	"title": "Python 2.7",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/symfony/",
	"title": "Symfony 3",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/software-architecture-and-design/",
	"title": "Software Architecture &amp; Design",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/",
	"title": "Random",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/aws-applications/",
	"title": "AWS Applications",
	"tags": [],
	"description": "",
	"content": " SQS (Simple Queue Service)  the first AWS service distributed queue system temporary repository for messages that are waiting for processing one component in the application queues messages to be consumed by another component in the application can be used to decouple the components of an application messages can contain up to 256 kb of text\nyou can store up to 2GB per message, however, it will be stored and retrieved from S3 in this case messages can be retrieved using AWS SQS API auto-scaling events can be configured based on queue sizes as well pull-based, not pushed based\nmessages should be consumed from the queues, SQS will not push the messages messages can be kept in the queue from 1 minute to 14 days the default retention period is 4 days visibility timeout - the amount of time that the message is invisible in the SQS queue after a reader picks up that message. If the job is not processed within visibility timeout, the message will become visible again and can be processed by another reader. maximum visibility timeout is 12 hours polling types:  short polling - returns immediately even if the message queue being polled is empty long polling - doesn't return a response until a message arrived in the message queue or the long poll times out   queue types:  Standard Queue  nearly unlimited number of transactions per second a message is delivered at least once\noccasionally more than one copy of a message might be delivered out of order allows high throughput provides best-effort ordering\nensures that messages are generally delivered in the same order as they are sent   FIFO Queue  first-in-first-out ordered queues the order is strictly preserved and guaranteed the message remains available until a consumer processes and deletes it; duplicates are not introduced into the queue support message groups that allow multiple ordered message groups within a single queue limited to 300 transactions per second (TPS)      SWF (Simple Workflow Service)  makes it easy to coordinate work across distributed application components workflow executions can last up to 1 year presents a task-oriented API ensures that the task is assigned only once keeps track of all the tasks and events in an application SWF actors:  Workflow Starters\nAn application initiating a workflow.\n(e.g. an e-commerce website following the placement of order) Deciders\nControl the flow of activity tasks in a workflow execution. If something has finished (or failed) in a workflow, a Decider decides what to do next. Activity Workers\nCarry out the activity tasks    SNS (Simple Notification Service)  Service for sending notifications from the cloud Can deliver notifications to devices (via Push notifications), SMS, Email, SQS queues or to any other HTTP endpoint Push notifications to Apple, Google, FireOS, Windows devices and Android devices in China with Baidu Cloud Push Recipients are grouped using Topics One topic can support deliveries to multiple endpoint types The message published to a topic will be delivered to each subscriber Messages published to SNS are stored across different AZs Push-based delivery (no polling) Simple APIs and easy integration with applications Inexpensive, pay-as-you-go model with no up-front costs  SNS vs SQS  Both Messaging Services in AWS SNS - Push SQS - Polls (Pulls)  Elastic Transcoder  Media Transcoder in the cloud Convert media files from their original source format into different formats that will play on smartphones, tablets, PCs, etc. Provides transcoding presets for popular output formats Pay based on the minutes that you transcode and the resolution at which you transcode  API Gateway  Fully managed service that makes it easy for developers to publish, maintain, monitor and secure APIs at any scale. Features  Expose HTTPS endpoints to define a RESTful API Serverless-ly connect to services like Lambda \u0026amp; DynamoDB Send each API endpoint to a different target Run efficiently with low cost Scales automatically Track and control usage by API key Throttle requests to prevent attacks Connect to CloudWatch to log all requests for monitoring Maintain multiple versions of your API   Configuration  Define an API Define Resources and nested Resources (URL Paths) For each resource:  Select supported HTTP methods (verbs) Set security Choose target (such as EC2, Lambda, DynamoDB, etc.) Set request and response transformations     Deployment  Uses API Gateway domain, by default Can use a custom domain Now supports AWS Certificate Manager: free SSL/TLS certs   API Caching (TTL specified)\nWill help you reduce the number of requests made to your endpoint, improving the latency of the requests to your API. Same Origin Policy\nWeb browser permits scripts contained in a first web page to access data in a second web page, but only if both web pages have the same origin. This prevents XSS attacks. CORS can be enabled on the API Gateway  CORS (Cross-Origin Resource Sharing)\nallows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served. \u0026nbsp;\n The browser makes HTTP OPTIONS call The server returns a response listing other domains that are approved to GET this URL      Kinesis  Service working with the streaming data Makes it easy to load and analyze streaming data Streaming Data is data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes). Types  Kinesis Streams Kinesis Firehose Kinesis Analytics   Kinesis Streams  Receive data from producers and retain it until consumed Producers - Produce streaming data and stream it to Kinesis streams Consumers - receive data from Kinesis Streams and act on it Retention: 24 hours - 7 days Consists of Shards  5 transactions per second for reads a maximum total data read rate of 2MB per second up to 1000 records per second for writes maximum total write rate of 1MB per second   Data capacity of the stream is determined by the number of shards and their capacities   Kinesis Firehose  Data received could optionally execute a lambda function and will be output to it's the destination which could be S3, ElasticSearch There's no data persistence Data will be forwarded immediately to the target destination   Kinesis Analytics  analyzes the data inside both of Kinesis Firehose and Kinesis Streams, with on-the-fly analysis capability    FAQs  SQS Billing is calculated per request, plus data transfer charges for data transferred out of Amazon SQS 1 million requests per month - fall under the free tier Batch operations cost the same as other SQS requests. Grouping messages into batches, you can reduce the SQS costs. All the messages in SQS have a globally unique ID that SQS returns when the message is delivered to the message queue. This ID\u0026nbsp; is useful for tracking the receipt of a particular message in the message queue. Dead letter queues receive messages from other source queues after a maximum number of processing attempts cannot be completed. SQS message can contain up to 10 metadata attributes - applications can determine how to process the message based on the metadata instead of inspecting the entire message. (attributes: name-type-value triples) SentTimestamp attribute - contains information when the message was sent by a producer and queued by SQS. SenderId attribute - contains either the AWS account ID or the IP address for the sender. Amazon SQS APIs provide deduplication functionality for FIFO queues that prevents your message producer from sending duplicates. Any duplicates introduced by the message producer are removed within a 5-minute deduplication interval. If using standard queues you may experience duplicates - the application must be designed to be idempotent - not affected when processing the same message more than once). Queue type can be chosen only on creation. If you want to convert from one type to another you will have to recreate the queue. In FIFO queues, messages are ordered based on the message group ID. It's a required field when sending a message to the queue. After you're done processing the message, you are responsible for deleting it. Server-side encryption (SSE) - SSE encrypts messages as soon as Amazon SQS receives them using keys in AWS KSM (AWS Key Management Service). The messages are stored in encrypted form and are decrypted when sending a message to an authorized consumer. SSE encrypts the body of the message. The queue metadata, message metadata, and per-queue metrics are not encrypted. SSE uses AES-GCM 256 algorithm. Amazon SQS is PCI DSS Level 1 certified and HIPAA Eligible Service. SQS messages can contain text data, including XML, JSON and unformatted text. The number of inflight messages is limited to 120k for standard queue and 20k for FIFO queue. Inflight messages are those messages that have been received by a consuming component but have not yet been deleted.\u0026nbsp; Queue names are limited to 80 characters.  "
},
{
	"uri": "https://majdarbash.github.io/categories/aws-certified-saa/",
	"title": "AWS Certified SAA",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/",
	"title": "Docs",
	"tags": [],
	"description": "",
	"content": "MAJDARBASH.GITHUB.IO Majd Arbash Technical Lead, Solutions Architect, Software Engineer  github.com/majdarbash    linkedin.com/in/majdarbash    majdarbash.github.io    AWS WhitePapers \u0026amp; Deep Dives   AWS Certified MLS   AWS Certified SAP   AWS Certified SAA   Genetic Algorithm   Neural Networks   Blockchain   Python 2.7   Symfony 3   Software Architecture \u0026amp; Design   Random   "
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/ha-arhictecture-high-availability/",
	"title": "HA Arhictecture: High Availability",
	"tags": [],
	"description": "",
	"content": " ELB, ALB (Elastic Load Balancer, Application Load Balancer)  a physical/virtual device designed to balance network load across multiple web servers at least 2 public Subnets are required when provisioning a load balancer ELBs do not have a pre-defined IPv4 addresses; you resolve to them using a DNS name types of load balancers  Application Load Balancers  Best suited for HTTP / HTTPS traffic Operates at Layer 7 Application-aware Advanced request routing, sending specified requests to specific web servers Register target groups Load balancing rules are supported   Network Load Balancers  Load balancing TCP traffic Operates at Layer 4 Capable of handling millions of requests per seconds with ultra-low latencies   Classic Load Balancers  Legacy / previous generation load balancers HTTP / HTTPS / TCP traffic Operates at layer 4, supports Layer-7 specific features, such as X-Forwarded and sticky sessions Errors: 504 - Gateway timeout - ec2 instances are not responding Registers instances     Health Checks  Checks the instance health by talking to it Instances are reported as InService, OutofService LBs have their own DNS name. You are never given an IP address Connection Draining - the number of seconds to allow traffic to be flowing (300 seconds by default)    Auto-Scaling  Auto-Scaling Group  subnets should be assigned - instances will be distributed on these subnets Scaling Policy  Target tracking scaling policy Simple scaling policy Scaling policy with steps   Scaling warm-up time\nthe time required to warm up an instance - during this time instance won't contribute to the metrics When deleting the auto-scaling group, the instances beneath it will be deleted as well.   Launch Configuration  IP Address Type  Default public IP assignment (as per subnet) Assign public IP to every instance Do not assign a public IP to an instance      HA Architecture  You should plan for failure Netflix (Simian Army Projects)\nInjecting failure into production systems\n(https://medium.com/netflix-techblog/the-netflix-simian-army-16e57fbab116) More than one AZ should be used (2 AZs at least) Always Design for failure Use Multiple AZ's and Multiple Regions wherever you can In RSD Read Replica - creates a replica of AWS for performance purposes and not for HA Multi-AZ configuration means replication across different AZs supporting seamless failover Scaling out - adding ec2 instances using ASGs Scaling up - increasing resources of ec2 instances (modifying instance type) S3 storage classes  Highly Available  Standard S3 Standard S3 Infrequently access   Non-HA  Reduced redundancy storage S3 single AZ      HA Wordpress Site  2 x Cloud Front distributions  Cloud Front distribution for static media content Cloud Front distribution for the main site   2 x S3 Buckets  S3 Bucket for media S3 Bucket for code synchronization   RDS MySQL/Aurora with Mutli-AZ enabled Writer Node  LAMP installed CRON commands  Sync/var/www/html/wp-content/uploads to media bucket\naws s3 sync --delete /var/www/html/wp-content/uploads s3://bucket-for-media-name/ Sync /var/www/html to code bucket\naws s3 sync --delete /var/www/html s3://bucket-for-code-name/     Reader Node(s)  LAMP installed Scales in 3 AZs using ASG, min size is 2 instances Traffic received by Cloud Front distribution for the main site  Route 53 domain record points at Cloud Front distrbution ALB is defined as an origin ALB forwards traffic to Target Groups   On user-data instances will pull the code content from S3 bucket-for-code Periodically instances will run sync commands to get any updates done to the code from the Writer Node CRON command  aws s3 sync --delete s3://bucket-for-code-name/ /var/www/html   .htaccess rewrite rule is added to serve the wp-content/uploads files from Cloud Front distribution serving the static media bucket from bucket-for-media-name   Simulating failure  Terminate an EC2 instance RDS, Multi-az  You may reboot with failover to simulate the failure This may still take your site offline for a couple of minutes      Cloud Formation  Is a way of completely scripting your cloud environment From AWS website:  \"AWS CloudFormation provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This file serves as the single source of truth for your cloud environment.\"   https://aws.amazon.com/quickstart/  Built by solution architects and partners Cloud Formation templates to help you build your production or test environment quickly and starting using it immediately   "
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/route53/",
	"title": "Route53",
	"tags": [],
	"description": "",
	"content": "  Route53 gets its name from port 53 of DNS server IPv4 space is 32 bit fields having over 4 billion different addresses. IPv6 solves the depletion issue and has as address space of 128 bits For each public hosted zone Route 53 automatically created NS record and SOA record (Start of Authority Record) NS Record - is used by Top Level Domain servers to direct traffic to the Content DNS server which contains the authoritative DNS records When creating a public hosted zone, Route 53 gives you NS records in different Top-Level Domains. The start of authority (SOA) record identifies the based DNS information about the domain. SOA record components  NS that created the SOA record Email of the administrator The current version of the data file The default number of seconds for the time-to-live file on resource records   Domain to IP request flow  Top Level Domain NS Records SOA   The lower the TTL (time to live), the faster changes to DNS records take to propagate through the internet CNAME records can't be used for naked domain names (zone apex record). It must be either A record or an Alias Given a choice between Alias record and CNAME always choose an Alias record Common DNS Types  SOA Records NS Records A Records CNAMES MX Records PTR Records   You can buy domain names directly with AWS It can take up to 3 days to register depending on the circumstances Routing Policies  Simple Routing\nYou can only have one record with multiple IP addresses. If you specify multiple values in a record, Route 53 returns all values to the user in a random order.\nSimple routing cannot be associated with a health check. Weighted Routing\nTraffic can be split based on different weights assigned. Latency-based Routing\nAllows you to route your traffic based on the lowest network latency for your end user. Failover Routing  Active/passive set up Specifying primary and secondary records Health check is associated with the primary record   Geolocation Routing\nLets you choose where your traffic will be sent based on the geographic location of your users.\n This is not a latency based routing - the routes are locked down based on geolocation.   Geoproximity Routing (Traffic Flow Only)\nGeorproximity routing lets Route53 route traffic to your resources based on the geographic location of your users and your resources. You can optionally choose to route more traffic or less to a given resource by specifying a value, know as bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource. To use this you must use Route53 Traffic flow. Multivalue Answer Routing\nExactly the same as Simple Routing, however, allows you to use health checks on each record. Route53 will return values for healthy resources.   Health Checks  You can set health checks on individual record sets If a record set fails a health check it will be removed from Route53 until it passes the health check You can set SNS notifications to alert you if a health check is failed   Health Checks can be created and associated with Route 53 records  FAQs  Route 53 is built using AWS's highly available and reliable infrastructure. Each Amazon Route 53 hosted zone is served by its own set of virtual DNS servers. They are assigned by the system when the hosted zone is created. Amazon Route 53 charges are based on actual usage of the service for Hosted Zones, Queries, Health Checks, and Domain Names. Access to Route53 can be controlled using IAM. You can configure Amazon Route 53 to log information about the queries that Amazon Route 53 receives including date-time stamp, domain name, query type, location etc. to CloudWatch Logs. Amazon Route53 uses anycast network - is a networking and routing technology that helps your end users' DNS queries get asnwered from the optimal Route53 location given network conditions. Route53 account is limited to a maximum of 500 hosted zones and 10,000 resource record sets per hosted zone and 50 domains. Multiple hosted zones can be created for domain. In addition to standard record types supported by Route53, alias records are supported, which is Route 53-specific extension to DNS. Alias can be used to map your entires to AWS Resources. Wildcard entries are supported. Zone apex can be mapped to AWS Resources by using alias records. Traffic Flow makes it easy for developers to create policies that route traffic based on constraints like: latency, endpoint health, multivalue; answers, weighted round robin, and geo. In addition to these, Traffic Flow also supports geoproximity based routing with traffic biasing. Traffic Policy is the set of rules that routes end users' request to one of the application's endpoints. Policy Record associates the traffic policy with the appropriate DNS name within an Amazon Route 53 hosted zone that you own. You are billed from Traffic Flow per Policy Record. Private DNS should be attached to a VPC Route53 Health checks could verify the expected content of the web server by using the \"Enable String Matching\" option. Amazon Route 53\u0026rsquo;s metric based health checks let you perform DNS failover based on any metric that is available within Amazon CloudWatch, including AWS-provided metrics and custom metrics from your own application. Domains registered are configured to renewal automatically. Route53 provides privacy protection at no additional charge. Route53 registers top-level domains through either Amazon Registar or Gandi. Route53 Resolver is a regional DNS service that provides recursive DNS lookups for names hosted in EC2 as well as public names on the internet. Route 53 is Authoritative and Recursive DNS. Authoritative DNS - contains the final answer to a DNS query. Recursive DNS - forwards the query directly to a specific recursive DNS server. "
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/iam/",
	"title": "IAM",
	"tags": [],
	"description": "",
	"content": " \nIAM (Identity Access Management)\n Centralized control of your AWS account Shared Access to your AWS account Granular Permissions Identity Federation (including Active Directory, Facebook, etc...) Multifactor Authentication Temporary access to users/devices and services Password rotation policy PCI DSS Compliance  Models:\n Users Groups Policies Roles  AWS provides IAM Pre-defined Roles per Job Function, otherwise custom roles and policies can be defined.\n IAM is universal. It does not apply to regions. The root account is the account created when first setting up AWS account - It has complete Admin access. Users have no permissions by default. New Users are assigned Access Key ID \u0026amp; Secret Access Keys when programmatic access is enabled. Access Key Id and Secret Access Key will be displayed once. If lost, they will have to be regenerated. MFA should be set up on the root account.  "
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/databases/",
	"title": "Databases",
	"tags": [],
	"description": "",
	"content": " Relational Database on AWS: (RDS) used for OLTP (Online Transaction Processing)  Supported Types  SQL Server Oracle MySQL Server PostgreSQL Aurora MariaDB   Run on virtual machines You cannot log in to these operating systems however Patching of the RDS Operating System and DB is Amazon's responsibility RDS is NOT Serverless Aurora Serverless is Serverless  Read-Replicas vs Muti-AZ  Multi-AZ  For Disaster Recovery AWS handles cross-region replication AWS does an automatic failover of the instance Available for SQL Server, Oracle, MySQL Server, PostgreSQL, MariaDB You can force a failover from one AZ to another by rebooting the RDS instance.   Read Replicas  Improves Performance Up to 5 copies of Read-Replicas All read traffic can be going to read replica Available for MySQL Server, PostgreSQL, MariaDB, Aurora You can create read replicas of read replica as well, however, the latency will be greater then. Read replicas can be promoted to be their own databases - this will break the replication. Read replica can be in a second region. Automated backups should be turned on in order to be able to create read replicas.    RDS Backups\n Automated Backups  Allow you to recover your database to any point in time within a \"retention period\". Retention period can be between 1 and 35 days. Enabled by default Backups stored in S3 Free storage space equal to the size of your database On recovery, RDS will choose the most recent daily backup (full snapshot) and then apply transaction logs relevant to the restore \"point in time\". Backups are taken within the defined window. During the backup window, storage I/O may be suspended and latency may be elevated. Deleted when RDS instance is deleted   Snapshots  Done manually Stored even after deletion of RDS instance   Restoring snapshots or automated backups will create a new RDS instance with a new DNS endpoint.  Encryption:\n Encryption at rest is supported. Encryption is done using AWS Key Management Service (KMS). Automated backups, snapshots and read replicas of encrypted storage will be encrypted as well. Is available for all RDS supported types  DynamoDB (NoSQL database, Non-Relational Databases)  Terms:  Collection Document Key Value Pairs   Fast and flexible NoSQL database service Single-digit millisecond latency at any scale Fully managed database Great fit for mobile, web, gaming, ad-tech, IoT and many other applications Stored on SSD storage Spread across 3 geographically distinct data centers Eventual Consistent Reads (Default) Strongly Consistent Reads (1-second rule)  Allocated resources:\n Read Capacity Unit (for an item up to 4 KB)  One strongly consistent read per second Two eventual consistent reads per second   Write Capacity Unit (for an item up to 1KB)  One write per second    AWS Redshift Data Warehousing\n Business Intelligence Tools Pulling very large and complex data sets. Usually used by management to do queries on data OLTP vs OLAP\n(Online Transaction Processing vs Online Analytics Processing) Data Warehousing databases use a different type of architecture both from a database perspective and infrastructure layer.  Amazon Redshift (OLAP)\n Amazon's Data Warehouse Solution Fast and powerful, fully managed, petabyte-scale data warehouse service in the cloud Configuration  Single Node (160GB) Multi-Node  Leader Node (manages client connections and receives queries) Compute Node (store data and perform queries and computations). Up to 128 Compute Nodes.     Advanced Compression  Columnar data stores can be compressed much more than row-based data stores because similar data is stored sequentially on disk. Does not require indexes or materialized views, so uses less space than traditional relational database systems. When loading data into an empty table, Amazon Redshift automatically samples your data and selects the most appropriate compression scheme.   Massive Parallel Processing (MPP)  Distributes data and query load across all nodes. Makes it easy to add odes to your data warehouse and enables you to maintain fast query performance as your data warehouse grows.   Backups  By default 1 day retention period, up to 35 days. Always attempts to maintain at least three copies of your data (the original and replica on the compute nodes and a backup in Amazon S3) Can also asynchronously replicate your snapshots to S3 in another region for disaster recovery.   Pricing  Compute Node Hours You will not be charged for leader node hours. Backup charges Data transfer (only within a VPC, not outside it)   Security Considerations  Encrypted in transit using SSL Encrypted at rest using AES-256 encryption Redshift takes care of key management  Manage your key through HSM (Hardware Security Module) AWS Key Management Service     Availability  Available in 1AZ Can restore snapshots to the new availability zone    Aurora  Amazon's own proprietary database that is compatible with MySQL and PostgreSQL 2 copies of your data are contained in each AZ with a minimum of 3 AZs. 6 copies of your data. You can share Aurora Snapshots with other AWS accounts. 2 types of replicas available  Aurora Replicas MySQL replicas   Automated failover is only available with Aurora Replicas. You can do migrations from MySQL to Aurora only by creating an Aurora read replica and promoting it. Provides up to 5 times better performance than MySQL. Start with 10GB, Scales in 10GB increments to 64TB (Storage Autoscaling) Compute resources can scale up to 32vCPU and 244GB of Memory Designed to transparently handle the loss of up to two copies of data without affecting database write availability and up to three copies without affecting read availability. Automated backups are always enabled and they do not impact database performance. Taking snapshots does not impact performance.  ElasticCache Web service that makes it easy to deploy, operate and scale an in-memory cache in the cloud. Improves the performance of web applications. Retrieves information from fast, managed, in-memory caches.\n Memcached  Can be scaled horizontally   Redis  Multi-AZ You can do backups and restores of Redis    FAQs  By default customers are allowed to have up to a total of 40 Amazon RDS DB instances. Database limits per RDS instance are imposed based on software limitation. Maintenance windows - defines the time range when DB instance modifications, database engine version upgrades, and software patching occurs, in the event they are requested or required. Auto Minor Version Upgrade setting will automatically schedule the minor version upgrade in the next maintenance window. When a major version is deprecated in RDS a minimum 6 month period will be granted to upgrade to a supporter major version. Billing components include: DB instance hours, Storage, I/O requests per month, Provisioned IOPS per month, Backup Storage, Data transfer. Amazon RDS supported storage types are: Provisioned IOPS (SSD) Storage (OLTP workloads) and General Purpose (SSD) Storage (for moderate I/O requirements) RDS Automated Backups - allows point in time recovery using transaction logs. Snapshots - are user-initiated backups - backing up your DB instance. RDS master user account is a native database user account which you can use to connect to your DB Instance. You can encrypt connections between your application and the DB instance using SSL/TLS. Amazon RDS generates an SSL/TLS certificate for each DB Instance. Once an encrypted connection is established, data transferred between the DB Instance and your application will be encrypted during transfer. Amazon RDS supports encryption at rest for all database engines, using keys you manage using AWS Key Management Service (KMS). By default, Amazon RDS chooses the optimal configuration parameters for your DB Instance taking into account the instance class and storage capacity. When you create or modify your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous \u0026ldquo;standby\u0026rdquo; replica in a different Availability Zone. When you run a DB instance as a Multi-AZ deployment, the \u0026ldquo;primary\u0026rdquo; serves database writes and reads. In addition, Amazon RDS provisions and maintains a \u0026ldquo;standby\u0026rdquo; behind the scenes, which is an up-to-date replica of the primary. The standby is \u0026ldquo;promoted\u0026rdquo; in failover scenarios. After failover, the standby becomes the primary and accepts your database operations. You may observe elevated latencies relative to a standard DB instance deployment in a single Availability Zone as a result of the synchronous data replication performed on your behalf. Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. RDS initiates failover in these events: \u0026nbsp;Loss of availability in primary Availability Zone, Loss of network connectivity to primary, Compute unit failure on primary, Storage failure on primary. AWS will emit a DB instance event in case of failover. You can alsouse RDS Event Notifications to be notified of specific events. You can initiate a forced failover when rebooting your instance. Automatic Backups must be enabled on instance to be able to create read replicas. Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle allow you to create up to 5 read replicas for a given source DB instance. Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle allow you to enable Multi-AZ configuration on read replicas to support disaster recovery and minimize downtime from engine upgrades. Amazon Aurora, Amazon RDS for MySQL and MariaDB: You can create a second-tier read replica from an existing first-tier read replica. By creating a second-tier read replica, you may be able to move some of the replication load from the master database instance to a first-tier Read Replica. Please note that a second-tier Read Replica may lag further behind the master because of additional replication latency introduced as transactions are replicated from the master to the first tier replica and then to the second-tier replica. "
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/serverless/",
	"title": "Serverless",
	"tags": [],
	"description": "",
	"content": " Lambda  Compute service to upload and run your code AWS Lambda takes care of provisioning and managing the underlying infrastructure Lambda scales out (not up) automatically Usage  Event-driven compute service which runs code in response to events. Events could be internal AWS events. Compute service to run your code in response to HTTP requests using AWS API Gateway or API calls made using AWS SDKs.   Lambda functions are independent, 1 event = 1 function Lambda is serverless Lambda functions can trigger other lambda functions, 1 event can trigger multiple functions Architectures can get extremely complicated, AWS X-ray allows you to debug what is happening Lambda can do things globally, you can use it to back up S3 buckets to other S3 buckets, etc Lambda is cost effective No servers, no maintenance is required  Lambda is the Ultimate Extraction Layer  Data Centres Hardware Assembly Code/Protocols High-Level Languages Operating Systems Application Layer/AWS APIs AWS Lambda  Languages supported by Lambda  Node.js Java Python C# Go PowerShell  Pricing  Number of Requests Duration\n(from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms)  How to Build Alexa Skill  Skill Service  AWS Lambda   Skill Interface  Invocation Name Intent Schema Slot Type Utterances    Cloud Architecture  Traditional\nELB - EC2 Instances - RDS Serverless\nAPI Gateway - Lambda - DynamoDB "
},
{
	"uri": "https://majdarbash.github.io/2019/04/17/aws-certification-solution-architect-iam-s3/",
	"title": "S3",
	"tags": [],
	"description": "",
	"content": " Billing  Billing Alarms can be created from CloudWatch. Billing Notifications should be enabled from the Billing Preferences section.  S3 (Simple Storage Service)  Provides developers and IT teams with secure, durable, highly scalable object storage Provides simple web services interface to store and retrieve data S3 is a safe place to store the files S3 and Glacier are not block storages S3 is Object-Based - allows you to upload files Files can be 0 bytes to 5TB Successful uploads will generate an HTTP 200 code Unlimited storage Files are stored in Buckets Objects consist of:  Key (name of the object) Value (the data) Version ID (important for versioning) Metadata (data about data you are storing) Subresources  Access Control List Torrent     Files can be from 0 Bytes to 5TB There is unlimited storage Files are stored in Buckets S3 is a universal namespace. Names must be unique globally. Data Consistency  Read after Write consistency for PUTS of new Objects\nIf you write a new file and read it immediately after, you will be able to view that data Eventual Consistency for overwrite PUTS and DELETEs (can take some time to propagate)\nIf you update an existing file or delete a file and read it immediately, you may get the older version, or you may not. Changes to objects can take a little bit of time to propagate.   Tiered Storage Available Lifecycle Management Versioning Encryption MFA Delete Secure your data using Access Control Lists and Bucket Policies Storage Classes  S3 Standard  99.99% availability 99.999999999% durability for S3 information. (11x9s) Stored redundantly across multiple devices in multiple facilities Designed to sustain a loss of 2 facilities concurrently   S3 - IA (Infrequently Accessed)  For data that is accessed less frequently but requires rapid access when needed Lower fee that S3, but you are charged a retrieval fee   S3 One Zone - IA (Infrequently Accessed, was called before RRS - Reduced Redundancy Storage)  Lower-cost option for infrequently accessed data, but do not require the multiple Availability Zone data resilience.   S3 - Intelligent Tiering  Uses machine learning Optimizes costs automatically by moving data to the most cost-effective access tier, without performance impact or operational overhead   S3 Glacier  secure, durable and low-cost storage class for backup and data archiving retrieval times are configurable from minutes to hours expedited, standard and bulk retrievals data is encrypted by default   S3 Glacier Deep Archive  lowest-cost storage class where a retrieval time of 12 hours is acceptable      S3 Billing  Storage Number of requests Storage Management Pricing Data Transfer Pricing Transfer Acceleration Cross-Region Replication  Access \u0026amp; Encryption  By default, all newly created buckets are PRIVATE Control access to the buckets using:  Bucket Policies Access Control Lists   Encryption in Transit  SSL/TLS (HTTPS)   Encryption At Rest (Server Side)\n(SSE = Server Side Encryption)\n SSE-S3, S3 Managed Keys - AES-256 SSE-KMS, AWS Key Management Service SSE-C, Customer Provided Keys    Versioning  Stores all versions of an object (including all writes and even if you delete an object) Great backup tool Once enabled, Versioning cannot be disabled, only suspended. Integrates with Lifecycle rules Versioning's MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security. Size of the bucket is a sum of all versions of the files stored in the bucket A specific version of the file can be deleted Deletion of a file will place a delete marker  Lifecycle Management Tools \u0026amp; Glacier  Allows you to automate moving your objects between the different storage tiers Can be used in conjunction with versioning Can be applied to current versions and previous versions  Cross Region Replication  Versioning must be enabled on both the source and destination buckets for CRR to work Regions must be unique CRR will not replicate the objects created before the CRR Rule was added Delete markers are not replicated Deleting individual versions or delete markers will not be replicated All subsequently updated files will be replicated automatically  Transfer Acceleration  takes advantage of CloudFront's globally distributed edge locations can improve upload and access times can be tested using speed comparison tool:\n(http://s3-accelerate-speedtest.s3-accelerate.amazonaws.com)  CloudFront  Edge Location - the location where the content will be cached Origin - the origin of all the files that the CDN will distribute. It can be an S3 Bucket, an EC2 Instance, an Elastic Load Balancer, or Route 53 Distribution - The name that is given to the CDN which consists of a collection of Edge Locations If Edge Location does not have a file in the cache, it will download it from the Origin using optimized networks Objects are cached for the life of the TTL (Time to Live) Edge locations are not just read-only, you can write to them to Types of Distribution supported:  Web Distribution RTMP - Used for Media Streaming   Invalidation  Clears the cache from the Edge Locations   You can invalidate cached objects, but you will be charged  Snowball Petabyte-scale data transporter solution that uses secure appliances to transfer large amounts of data into and out of AWS.\n Snowball  Import to S3 Export from S3 Types  50TB 80TB   Using it can be cheaper than using high-speed internet   Snowball Edge  is a 100TB data transfer device with on-board storage and compute capabilities. Can be used to move large amounts of data into and out of AWS. Applications will continue to run even when they are not able to access the cloud   Snowmobile  Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. Can transfer up to 100 PB per SnowMobile, 45-foot long ruggedized shipping container, pulled by a semi-trailer truck.    Storage Gateway Connects on-premise software appliance with cloud-based storage to provide seamless and secure integration between an organization's on-premises IT environment and AWS's storage infrastructure.\nThe service enables you to securely store data to the AWS cloud for scalable and cost-effective storage.\nCan be installed as a VM image on a host in a data center. Supports either VMware ESXi or Microsoft Hyper-V hypervisors.\nPhysical appliances are available as well.\nTypes of Storage Gateways:\n File Gateway (NFS)\nFor files: files are stored as objects in your S3 buckets and accessed through an NFS mount point. Ownership, permissions, and timestamps are stored in S3 user-metadata. Volume Gateway (iSCSI)\nAn application can use the disk volumes using iSCSI block protocol.\nData written to the volumes can be asynchronously backed up as point-in-time snapshots of your volumes, and stored in the cloud as Amazon EBS snapshots.\nSnapshots are incremental backups and only changed blocks will be charged.\n Stored Volumes\nDAta will be stored locally and asynchronously backed-up to S3 in the form of EBS. (1GB - 16TB volume size) Cached Volumes\nData is stored on AWS S3, while retianing frequently accessed data locally in your storage gateway. This minimizes the need to scale on-premises storage infrastructure, while still providing your applications with low-latency access to their frequently accessed data. (1GB - 32TB volume size)   Tape Gateway (TPL)\nData archiving to AWS Cloud. Lets you leverage your existing tape-based backup application infrastructure to store data on virtual tape cartridges that you create on your tape gateway.\nTape gateway is preconfigured with a media changer and tape drives, which are available to your existing client backup applications as iSCSI devices.  FAQs  The total volume of data is unlimited Individual objects can have a max size of 5Tb Largest object uploaded in a single put is 5Gb For objects larger then 100Mb users should consider using multi-part upload functionality Amazon uses S3 for its developers and a wide variety of projects Amazon S3 is a simple key-based object store. Tags can be added to the objects to organized the data. Pricing components include: storage used, data transfer and data requests Amazon Macie - AI-powered security service that helps you prevent data loss by discovering, classifying, and protecting sensitive data stored in Amazon S3 Amazon S3 uses a combination of Content-MD5 checksums and cyclic redundancy checks (CRCs) to detect data corruption AZs are automatically assigned in Amazon S3 based on the storage class used If the source object is uploaded using the multipart upload feature, then it is replicated using the same number of parts and part size. For example, a 100 GB object uploaded using the multipart upload feature (800 parts of 128 MB each) will incur request cost associated with 802 requests (800 Upload Part requests + 1 Initiate Multipart Upload request + 1 Complete Multipart Upload request) when replicated. You will incur a request charge of $0.00401 (802 requests x $0.005 per 1,000 requests) and a charge of $2.00 ($0.020 per GB transferred x 100 GB) for inter-region data transfer. After replication, the 100 GB will incur storage charges based on the destination region. "
},
{
	"uri": "https://majdarbash.github.io/2017/09/30/docker-building-nginx-cookbook-in-dockerized-container/",
	"title": "How to build Docker Image for NGINX using Chef (chef-solo)?",
	"tags": [],
	"description": "",
	"content": " Docker + Chef-solo + Your Cookbooks The previous example runs nginx stack on ubuntu:14.04 from the recipes obtained from the chef supermarket. The requirements to be downloaded are mentioned in the Berksfile.\nSo, how do we add custom build recipes?\nLet's build our custom nginx recipe which will be provisioned in docker container using chef-solo.\n1. Download chef-dk to your local machine 2. Create the skeleton |_ Berksfile \\\n(will specify the dependencies to be downloaded from the supermarket.io  currently we don't need any, but would still the file for potential use)\n|_ solo.rb\n|_ solo.json\n|_ Dockerfile\n|_ cookbooks/ (will contain your cookbooks, initialized using chef commands)\n3. Tailor made cookbook - special for you! chef generate cookbook cookbooks/nginx\nAt this stage you may want to populate your recipe with necesary commands.\ncookbooks/nginx/recipes/default.rb\n # # Cookbook:: nginx # Recipe:: default # # Copyright:: 2017, The Authors, All Rights Reserved. package [\u0026lsquo;nginx\u0026rsquo;] do action :install end\npackage [\u0026lsquo;php7.0-fpm\u0026rsquo;] do action :install end\npackage [\u0026lsquo;php-imagick\u0026rsquo;, \u0026lsquo;php-curl\u0026rsquo;, \u0026lsquo;php-gd\u0026rsquo;, \u0026lsquo;php-mcrypt\u0026rsquo;, \u0026lsquo;php-xml\u0026rsquo;, \u0026lsquo;php-mbstring\u0026rsquo;, \u0026lsquo;php-soap\u0026rsquo;, \u0026lsquo;php-mysql\u0026rsquo;, \u0026lsquo;php-pear\u0026rsquo;] do action :install end\npackage [\u0026lsquo;ntp\u0026rsquo;, \u0026lsquo;htop\u0026rsquo;, \u0026lsquo;vim\u0026rsquo;] do action :install end\ntemplate \u0026lsquo;/etc/nginx/sites-enabled/default\u0026rsquo; do source \u0026lsquo;default.conf\u0026rsquo; end \ncookbooks/nginx/templates/default.conf\n ## Default server configuration  server { listen 80 default_server; listen [::]:80 default_server;\n# SSL configuration # # listen 443 ssl default_server; # listen [::]:443 ssl default_server; # # Note: You should disable gzip for SSL traffic. # See: https://bugs.debian.org/773332 # # Read up on ssl_ciphers to ensure a secure configuration. # See: https://bugs.debian.org/765782 # # Self signed certs generated by the ssl-cert package # Don't use them in a production server! # # include snippets/snakeoil.conf; root /app; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; server_name _; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ =404; } # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # location ~ \\.php$ { include snippets/fastcgi-php.conf; # With php7.0-cgi alone: # fastcgi_pass 127.0.0.1:9000; # With php7.0-fpm: fastcgi_pass unix:/run/php/php7.0-fpm.sock; } # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #}  }\nVirtual Host configuration for example.com  You can move that to a different file under sites-available/ and symlink that to sites-enabled/ to enable it.  #server {\nlisten 80; listen [::]:80;  server_name example.com;  root /var/www/example.com; index index.html;  location / { try_files $uri $uri/ =404; } #} \n4. File adjustments for ubuntu:16.04 solo.rb\n # Solo.rb root = File.absolute_path(File.dirname(__FILE__)) file_cache_path root cookbook_path root + '/cookbooks'  solo.json\n { \"run_list\": [ \"recipe[nginx::default]\" ] }  Berksfile\n #Berksfile source 'https://supermarket.chef.io'  Dockerfile\n FROM ubuntu:16.04 RUN apt-get -y update RUN apt-get -y install curl build-essential libxml2-dev libxslt-dev git RUN curl -L https://www.opscode.com/chef/install.sh | bash RUN echo \"gem: --no-ri --no-rdoc\"  ~/.gemrc RUN /opt/chef/embedded/bin/gem install berkshelf RUN apt-get -y update RUN apt-get -y install python-software-properties RUN apt-get -y update\nADD ./Berksfile /Berksfile ADD ./solo.rb /var/chef/solo.rb ADD ./solo.json /var/chef/solo.json ADD ./cookbooks /var/chef/cookbooks\nRUN cd / \u0026amp;\u0026amp; /opt/chef/embedded/bin/berks vendor /var/chef/cookbooks RUN chef-solo -c /var/chef/solo.rb -j /var/chef/solo.json\nCMD service php7.0-fpm start \u0026amp;\u0026amp; nginx -g \u0026ldquo;daemon off;\u0026rdquo; \n5. Good to go: build, run, enjoy ! in order to experiment the setup I included a www/ directory in the project root with some files. We will mount this directory to the docker container after building our image !\nImage is build and stored locally:\n docker build -t majdarbash/nginx .  Running our image:\n # running our image with default command (CMD will be executed) docker run -d -p 8080:80 majdarbash/nginx docker run -d -p 8080:80 majdarbash/nginx bash (overriding CMD entry point, to access bash, e.g. troubleshopt) running with mounted volume in www (note that nginx will serve from /app based on the chef template above) docker run -v ~/Developer/docker-chef-solo-skeleton/www:/app -p 8080:80 -d majdarbash/nginx\n\nYou can find the full git code on:\nhttps://github.com/majdarbash/docker-chef-solo-skeleton\n\n6. From open-source to open-source Time to share it back !\nUse the following commands to publish your image to the docker hub:\n # logged in to docker hub (enter credentials for hub.docker.io) docker login pushing the image to the repository docker push majdarbash/nginx \nNext time you or someone needs an image, all you need to do is:\n docker pull majdarbash/nginx docker run -v [local_directory]:/app -p 8080:80 -d majdarbash/nginx \n"
},
{
	"uri": "https://majdarbash.github.io/categories/random/",
	"title": "Random",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2017/09/30/docker-provisioning-docker-container-with-chef-solo/",
	"title": "How to provision Docker Images using Chef-Solo?",
	"tags": [],
	"description": "",
	"content": " Provisioning docker image with chef using chef-solo Building basic Dockerfile FROM ubuntu RUN echo \"Hello World\"  Adding chef-solo Dockerfile\nFROM ubuntu RUN apt-get -y update RUN apt-get -y install curl build-essential libxml2-dev libxslt-dev git RUN curl -L https://www.opscode.com/chef/install.sh | bash RUN echo \"gem: --no-ri --no-rdoc\"  ~/.gemrc RUN /opt/chef/embedded/bin/gem install berkshelf  Chef-solo Chef-solo requires solo.rb and solo.json.\nThese files will be created in the same directory:\n|_ Dockerfile (created above)\n|_ solo.rb\n|_ solo.json\n|_ Berksfile\nsolo.rb\n# Solo.rb root = File.absolute_path(File.dirname(__FILE__)) file_cache_path root cookbook_path root + '/cookbooks'  Berksfile\n#Berksfile site :opscode cookbook 'build-essential' cookbook 'git' cookbook 'nginx'  solo.json\n# solo.json { \"run_list\": [ \"recipe[nginx::default]\" ] }  Extenidng Dockerfile to run the recipes So far the Dockerfile installed chef client.\nThe added script will run the cookbooks defined in the solo.json file.\nFROM ubuntu:14.04 RUN apt-get -y update RUN apt-get -y install curl build-essential libxml2-dev libxslt-dev git RUN curl -L https://www.opscode.com/chef/install.sh | bash RUN echo \"gem: --no-ri --no-rdoc\"  ~/.gemrc RUN /opt/chef/embedded/bin/gem install berkshelf \u0026mdash;\u0026mdash;\u0026ndash; added part \u0026mdash;\u0026mdash;\u0026mdash;- RUN apt-get -y update RUN apt-get -y install python-software-properties RUN apt-get -y update\nADD ./Berksfile /Berksfile ADD ./solo.rb /var/chef/solo.rb ADD ./solo.json /var/chef/solo.json\nRUN cd / \u0026amp;\u0026amp; /opt/chef/embedded/bin/berks install \u0026ndash;path /var/chef/cookbooks RUN chef-solo -c /var/chef/solo.rb -j /var/chef/solo.json RUN echo \u0026ldquo;daemon off;\u0026rdquo; \u0026raquo; /etc/nginx/nginx.conf\nCMD [\u0026ldquo;nginx\u0026rdquo;] \nNow, you can just build it up, using\ndocker build -t majdarbash/test_docker\nAfter building the image, you can run it exposing the port 8080:\ndocker run -d -p 8080:80 majdarbash/test_docker\n"
},
{
	"uri": "https://majdarbash.github.io/categories/cheat-sheets/",
	"title": "Cheat Sheets",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2017/09/30/markdown-cheat-sheet/",
	"title": "CS: Markdown",
	"tags": [],
	"description": "",
	"content": " Content provided from:\nhttps://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet\n{% raw %}# H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Alternatively, for H1 and H2, an underline-ish style:\nAlt-H1 Alt-H2 {% endraw %}\nH2 H3 H4 H5 H6 Alternatively, for H1 and H2, an underline-ish style:\nAlt-H1 Alt-H2  {% raw %} Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this. {% endraw %} \nEmphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. ~~Scratch this.~~\n {% raw %} 1. First ordered list item 2. Another item * Unordered sub-list. 1. Actual numbers don't matter, just that it's a number 1. Ordered sub-list 4. And another item. Some text that should be aligned with the above item.\n Unordered list can use asterisks   Or minuses   Or pluses {% endraw %}    First ordered list item Another item  Unordered sub-list.    Actual numbers don't matter, just that it's a number  Ordered sub-list   And another item. Some text that should be aligned with the above item.\n   Unordered list can use asterisks Or minuses Or pluses   {% raw %} [I'm an inline-style link](https://www.google.com) I\u0026rsquo;m a reference-style link\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\n{% endraw %} \nI'm an inline-style link\nI'm a reference-style link\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself\nURLs and URLs in angle brackets will automatically get turned into links.\nhttp://www.example.com or http://www.example.com and sometimes\nexample.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\n {% raw %} Here's our logo (hover to see the title text): Inline-style: Reference-style: {% endraw %} \nHere's our logo (hover to see the title text):\nInline-style:\nReference-style:\n {% raw %} ```javascript var s = \"JavaScript syntax highlighting\"; alert(s); ``` s = \u0026#34;Python syntax highlighting\u0026#34; print s No language indicated, so no syntax highlighting. But let\u0026amp;#039;s throw in a \u0026lt;b\u0026gt;tag\u0026lt;/b\u0026gt;. {% endraw %} \n{% raw %}javascript\nvar s = \"JavaScript syntax highlighting\";\nalert(s);{% endraw %}\n{% raw %}python\ns = \"Python syntax highlighting\"\nprint s{% endraw %}\n{% raw %}No language indicated, so no syntax highlighting.\nBut let's throw in a tag.{% endraw %}\n{% raw %} No language indicated, so no syntax highlighting in Markdown Here (varies on Github). But let's throw in a tag. {% endraw %} No language indicated, so no syntax highlighting in Markdown Here (varies on Github).\nBut let's throw in a tag.\n{% raw %}  Blockquotes are very handy in email to emulate reply text.  This line is part of the same quote. Quote break.\n This is a very long line that will still be quoted properly when it wraps. Oh boy let\u0026rsquo;s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. {% endraw %}\n {% raw %} Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 {% endraw %} Colons can be used to align columns.\n   Tables Are Cool     col 3 is right-aligned $1600   col 2 is centered $12   zebra stripes are neat $1    The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown.\n   Markdown Less Pretty     Still renders nicely   1 2 3    {% raw %}  Blockquotes are very handy in email to emulate reply text.  This line is part of the same quote. Quote break.\n This is a very long line that will still be quoted properly when it wraps. Oh boy let\u0026rsquo;s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. {% endraw %}\n  Blockquotes are very handy in email to emulate reply text.\nThis line is part of the same quote.  Quote break.\n This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote.  {% raw %} Three or more...  Hyphens\n Asterisks\n Underscores\n{% endraw %}\nThree or more...\nHyphens\nAsterisks\nUnderscores\n{% raw %} Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a separate paragraph.\nThis line is also a separate paragraph, but\u0026hellip; This line is only separated by a single newline, so it\u0026rsquo;s a separate line in the same paragraph.\n{% endraw %}\nHere's a line for us to start with.\nThis line is separated from the one above by two newlines, so it will be a separate paragraph.\nThis line is also a separate paragraph, but...\nThis line is only separated by a single newline, so it's a separate line in the same paragraph.\n"
},
{
	"uri": "https://majdarbash.github.io/2017/09/29/docker-cheat-sheet/",
	"title": "CS: Docker Commands",
	"tags": [],
	"description": "",
	"content": " Resources:  docker.io hub.docker.com (repository of docker images) docs.docker.io (comprehensive guide - start from here! :) )  # Create image using this directory's Dockerfile docker build -t friendlyname . # Run \"friendlyname\" mapping port 4000 to 80 docker run -p 4000:80 friendlyname # Same thing, but in detached mode docker run -d -p 4000:80 friendlyname # List all running containers docker container ls # List all containers, even those not running docker container ls -a # Gracefully stop the specified container docker container stop # Force shutdown of the specified container docker container kill # Remove specified container from this machine docker container rm # Remove all containers docker container rm $(docker container ls -a -q) # List all images on this machine docker image ls -a # Remove specified image from this machine docker image rm # Remove all images from this machine docker image rm $(docker image ls -a -q) # Stop all containers docker stop $(docker ps -aq) # Log in this CLI session using your Docker credentials docker login # Tag for upload to registry docker tag username/repository:tag # Upload tagged image to registry docker push username/repository:tag # Run image from a registry docker run username/repository:tag # Searching for image docker search # Pull an \"ubuntu\" image to the local machine docker pull ubuntu # Attaching to the existing container docker exec -it [conainer hash] bash # Claim back some space docker prune volume"
},
{
	"uri": "https://majdarbash.github.io/2017/08/19/best-safest-way-upgrade-wordpress-site/",
	"title": "How to upgrade Wordpress? Best and safest way to upgrade",
	"tags": [],
	"description": "",
	"content": " Upgrading wordpress can be a bit of a hassle especially if you are doing so using the wp-admin section. Here is the list of possible issues you may encounter during the core upgrade and quick ways to resolve these issues:\n Wordpress is asking you for your ftp credentials during the update process\nIt may be possible that the process running your web application is unable to write to the files . This can be either apache2, nginx or php-fpm. The issue can be resolved by granting write access to the user running your web server application. Message appears: another upgrade is in progress - but upgrade was not accomplished\nThis clearly indicates that the upgrade attempt was not successful. In order to retry the upgrade, you have to unlock the system. This can be done using the query:\ndelete from wp_options where option_name=\"core_updater.lock\";  Wordpress is stuck on \"Site is under maintenance\".\nWhen doing upgrade, system will be switched to maintenance mode. This will be identified by adding a \".maintenance\" file in your project root. If you are sure that the maintenance did not succeed and want to continue running your wordpress application, this can be done, by removing the \".maintenance\" file, located in your project look.  The easiest and most preferable way to perform an update would be by having shell access to your host. \u0026nbsp;I would recommend downloading the latest version of the wordpress. After this wordpress files should be extracted to substitute the existing files and update.php script should be executed from your domain name.\u0026nbsp;Do not forget to backup your existing code and database before the upgrade!\nFollow the instructions below to upgrade your wordpress:\n Backup your existing files and database: mkdir /tmp/backup cd /tmp/backup tar czf backup.tgz /var/www/[your site directory] mysqldump -uroot -p [database name] \u0026gt; database.sql\n Download and extract the contents of the latest wordpress code cd /tmp wget http://wordpress.org/latest.zip unzip latest.zip cd /var/www/[your site directory] cp -avr /tmp/wordpress/* . rm -rf /tmp/wordpress /tmp/latest.zip\n Run the upgrade script: http://[your wordpress site]/wp-admin/upgrade.php   If something goes wrong, you can always locate your backup files in /tmp/backup , extract them , recover the database dump and go back to the previous state.\nDuring the migration process and for other cases of wordpress troubleshooting it may be useful to turn on debugging. This can be achieved by switching these parameters in wp-config.php file:\n// turns on debugging, output is displayed on the web page define('WP_DEBUG', true); // the debug log will be saved to wp-content/debug.log define( \u0026lsquo;WP_DEBUG_LOG\u0026rsquo;, false);\nHope this helps :) enjoy migrating your wordpress site successfully !\n"
},
{
	"uri": "https://majdarbash.github.io/2017/06/27/angularjs-seo-implications/",
	"title": "Overcoming SEO challenges with AngularJS 1-based Applications",
	"tags": [],
	"description": "",
	"content": "Building a completely AngularJS based frontend can be really fun. Frontend developers enjoy having complete control of the routing, views, reusability of components and\u0026nbsp;testing the frontend as a stand-alone entity. At the same time, backend developers provide the frontend app with APIs and effectively invest the time into building reliable and cleaner API structures with having a clear separation of frontend and backend.\nHowever a lot of SEO specialists are still arguing about the impact of SEO with AngularJS. The core differences are routing and runtime content generation.\nGoogle has confirmed that their bots are able to crawl and render javascript on the pages. For better compatibility a lot of SEO optimization techniques relies on the concept of pre-rendering to assure full compatibility with any search engine crawler.\nThe concept of pre-rendering is pretty straight-forward. It mainly relies on a simple concept of detecting search engines and serving snapshots (pre-generated html pages), while users can still enjoy the AngularJS frontend. To achieve this we need:\n1. Generate snapshot pages There are several tools to achieve this. You can simply google, or try out:\n https://github.com/cburgdorf/grunt-html-snapshot https://github.com/yearofmoo-articles/AngularJS-SEO-Article  2. Serve snapshot pages to search engine crawlers Here you need to play with your Apache or NGINX configurations:\nApache:\nRewriteEngine On RewriteCond %{REQUEST_URI} ^/$ RewriteCond %{QUERY_STRING} ^_escaped_fragment_=/?(.*)$ RewriteRule ^(.*)$ /snapshots/%1? [NC,L] NGINX:\nserver { # ... your config for your server daemon # listen 80; # server_name localhost; # root /path/to/root/ # index index.html; if ($args ~ \"_escaped_fragment_=/?(.+)\") { set $path $1; rewrite ^ /snapshots/$path; } } \u0026nbsp;\n"
},
{
	"uri": "https://majdarbash.github.io/2017/03/15/php-coding-style-fixer/",
	"title": "CI Pipeline: Unifying and automating code using Coding Style Fixer",
	"tags": [],
	"description": "",
	"content": "This tool will automatically fix your PHP code style. You can run it manually, or set it as pre-commit hook and it will run automatically.\nInstallation\n{% raw %}composer require friendsofphp/php-cs-fixer {% endraw %} Usage:\nDefault configuration will apply PSR1 and PSR2 rules. To run the default configuration:\nIf you would like to apply specific rules, you can set the rules argument of these command.\n{% raw %}--dry-run lets you run the fixer without making changes. --diff will output all the changes made by the fixer. {% endraw %} If you want to set the configuration statically in the codebase to be used by developers as a guideline, you can add this file to the root directory. When detected php-cs-fixer will apply the rules defined there.\u0026nbsp;.php_cs.dist\nReferences:\nGithub repository: https://github.com/friendsofphp/php-cs-fixer\nComposer:\u0026nbsp;https://packagist.org/packages/friendsofphp/php-cs-fixer\n"
},
{
	"uri": "https://majdarbash.github.io/2017/02/08/testing-installed-ssl-certificate/",
	"title": "How to test your SSL installation?",
	"tags": [],
	"description": "",
	"content": "You can test the installed SSL certificate using this command:\nopenssl s_client -connect hostname:443 -showcerts | grep \"^ \""
},
{
	"uri": "https://majdarbash.github.io/2017/01/23/learning-algorithm-neuron/",
	"title": "Closer look into Learning Algorithm of a Neuron",
	"tags": [],
	"description": "",
	"content": "[latexpage]\nIn perceptron model weights are changed to get a better set of weights. In multi-layer neural network this algorithm won't work and we don't use\u0026nbsp;perceptron learning algorithm.\nFor multi-layer nets the objective is the\u0026nbsp;actual\u0026nbsp;outputs values reaching target output values.\nLinear Neurons The neuron has a real-valued output which is weighted sum of its inputs:\n\\begin{equation} \u0026nbsp;y = \\sum_{i}{x_i w_i} = W^T X \u0026nbsp;\\end{equation}\n$W$ - Weight vector\n$X$ - input vector\nThe aim will be to\u0026nbsp;minimize the error. Error in this case is the squared difference between the desired output and the actual output.\nDelta Rule \u0026amp; Iterative Algorithm \\begin{equation}\u0026nbsp;\\Delta w_i = \\varepsilon x_i (t - y) \\end{equation}\nwhere:\n$\\varepsilon$ - learning rate\n$t - y$ - (difference between the target and the estimate, i.e. the residual error)\nBased on iterative algorithm delta rule adjustments will be done in the system to ensure that weights are adjusted by $ \\Delta w_i $ with each iteration which should lead with\u0026nbsp;to better result in case of Linear Neuron.\nNote: (Deriving Delta Rule is explained separately)\nError Surface of a Linear Neuron Assume\u0026nbsp;the space where \"all the horizontal dimensions\" correspond to the weights and a vertical dimension corresponds to the error.\nIn this case errors made on each set of weights would define the error surface, which is a quadratic bowl.\n\u0026nbsp;\n\nYou can get the same on octave, using the source code:\nvx = [-5:0.5:5]; vy = vx; [x, y] = meshgrid(vx, vy); z = x.^2 + y.^2; surfc(x, y, z) xlabel('w1') ylabel('w2') zlabel('E') title('Error Surface of a Linear Neuron with Two Input Weights') Logistic Neurons \\begin{equation} z = b + \\sum_{i}{x_i w_i} \\end{equation}\n\\begin{equation} y = \\dfrac{1}{1+e^{-z}} \\end{equation}\nDeducing the delta rule In order to find the derivatives needed for learning the weights of a logistic unit we need to find the quation for the following:\n\\begin{equation} \\Delta w_i = \\dfrac{\\partial E}{\\partial w_i} \\end{equation}\nusing the chain rule, we would get:\n\\begin{equation} \\dfrac{\\partial E}{\\partial w_i} = \\sum_{n}{\\dfrac{\\partial y^n}{\\partial w_i} \\dfrac{\\partial E}{\\partial y^n}} \\end{equation}\nAnother usage of chain rule and we would get:\n\\begin{equation} \\dfrac{\\partial y}{\\partial w_i} = \\dfrac{\\partial z}{\\partial w_i} \\dfrac{d y}{d z} \\end{equation}\nBy definition of $y$ and $z$ we would obtain that:\n\\begin{equation} \\dfrac{\\partial z}{\\partial w_i} = x_i \\end{equation}\n\\begin{equation} \\dfrac{d y}{d z} = y (1 - y) \\end{equation}\nwhich means that:\n\\begin{equation} \\dfrac{\\partial y}{\\partial w_i} = \\dfrac{\\partial z}{\\partial w_i} \\dfrac{d y}{d z} =\u0026nbsp;x_i y (1 - y) \\end{equation}\nwrapping this up to the main question:\n\\begin{equation} \\Delta w_i = \\dfrac{\\partial E}{\\partial w_i} = - \\sum_{n}{x_i ^n y^n (1 - y^n)(t^n - y^n) } \\end{equation}\nBy original delta rule, applied on linear neuron we can interpret this result as a delta rule, adjusted by $y^n (1 - y^n)$ - the slope of the logistic function.\nLearning by Randomly perturbing weights One way to adjust the\u0026nbsp;weights would be to do a random change on one of the weight and investigate if this improves the performance of the network. If so, save the weight and repeat again.\nThis is a form of reinforcement learning.\nIt is not efficient: we need to run the network on the whole training data to determine whether the performance was improved or not by the weight change.\nFinite difference approximation This method is considered more efficient and also relies on perturbing weights. Here we don't perturb the weights randomly, however we adjust the weights one by one and measure the impact.\nfor each weight:\n add to $w_i$\u0026nbsp;a small constant $\\epsilon$ and evaluate the error : $E_i^+$. Then we subtract the same constant $\\epsilon$ \u0026nbsp;and evaluate the error again $E_i^-$ reset $w_i$ to original value and proceed to the next weight when done, update the weights vector by:\n\\begin{equation} \\Delta w_i \u0026nbsp;= - \\mu \\dfrac{E_i^ + E_i^-}{2 \\epsilon}\u0026nbsp;\\end{equation}  The Backpropagation\u0026nbsp;algorithm In \u0026nbsp;contrast to the previously mentioned algorithms, the back propagation algorithm is the algorithm for taking one training case, and computing efficiently for every weight in the network, how the error will change as, on that particular training case, as you change the weight.\nMain topics will affect the performance of\u0026nbsp;BP algorithm:\n derivatives to be applied on the weights - how much to update the weights? frequency of weight updating how to prevent the network from overfitting  Frequency of weight update:\n online: after each training case\nError will be going low and high, jumping around and zig-zagging\u0026nbsp;after each training case. If there's enough training data, the weights should eventually converge to better results. full batch: after running the whole training data\nWeight changes $\\Delta w_i$ performed on the network will be more stable and will be more into the right direction towards optimal\u0026nbsp;$W^*$. Disadvantage is however that this operation is costly if the training data is big mini-batch: after a small sample of training cases\nHelps do certain weight adjustment before waiting for the whole training data, and not too soon, in contrast to online weight adjustment.  How much to update:\n Use a fixed learning rate Adapt the global learning rate\nTake bigger steps when we are moving in the right direction, and make the steps smaller when the improvement is converging (performance is changing less) Don't use steepest descent  Overfitting:\nTraining data contains useful relations which we want to model, and a lot of regularities which come in form of noise.\u0026nbsp;In any set of chosen cases there is a sampling error, due to the choice of these particular cases. When fitting a model we may overfit: i.e. we can be modeling the sampling error along other regularities. This can lead to really poor network design.\nOverfitting can be reduced by using the following methods:\n weight-decay\nkeeping the weights of the network small, or some weights close to zero weight-sharing\ngiving the same values to certain weights early stopping\nstarting training,\u0026nbsp;monitor the network training and stop when the training is getting worse model averaging\ntraining different neural nets and averaging them to reduce the error Bayesian fitting of neural nets\nanother method of model averaging dropout\nmaking model more robust by randomly emitting hidden units when training generative pre-training  Backpropagation cannot be used with binary threshold neurons: Backpropagation works with derivatives. In a binary threshold neuron the derivatives of the output function are zero so the error signal will not be able to propagate through it.\n"
},
{
	"uri": "https://majdarbash.github.io/categories/ml-neural-networks/",
	"title": "ML Neural Networks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2017/01/15/perceptrons/",
	"title": "First Generation of Neural Networks: Perceptrons",
	"tags": [],
	"description": "",
	"content": "We will look into how perceptrons are structured and what they actually do. Perceptrons are the first generation of neural networks. Research on perceptrons was conducted since 1960s.\n Network Architecture Types Feed-forward network  Information flows from the input layer in one direction to the output layer \"deep\" neural network term is used to indicate existing of more than one hidden layer.  [caption id=\"attachment_1312\" align=\"aligncenter\" width=\"277\"]Feed-forward Neural Network[/caption]\nRecurrent neural network  Information can flow around in cycles They are more biologically realistic Difficult to train Are able to remember information in the hidden state  In 2011 Ilya Sutskever designed a recurrent neural network which was generating one character at a time.\n[caption id=\"attachment_1317\" align=\"aligncenter\" width=\"292\"]Recurrent Neural Network[/caption]\nSymetrically connected network  Similar to recurrent networks They have symmetrical connections between units (same weight in both directions) Easier to analyze than recurrent networks  Perceptrons  Perceptrons are the first generation of neural networks Have some limitation (published in 1969 Minsky and Papert, \"Perceptrons\" book) Is still widely used today for tasks with huge feature vectors, containg many million of features. Binary threshold unit is used as a decision unit  Statistical Pattern Recognition Approach:\n convert raw input into a vector or feature activations learn how to weight each feature activation to get a single scalar quantity if this quantity is above a certain threshold, we decide that the input vector is a positive example of a target class  [caption id=\"attachment_1346\" align=\"aligncenter\" width=\"300\"]Standard Perceptron Architecture[/caption]\nBinary Threshold Neurons (with bias) \\begin{equation} z = b + \\sum_{i}{x_i w_i} \\end{equation}\n\\begin{equation} y = \u0026nbsp;\\begin{cases} 1, \u0026nbsp;if \u0026nbsp;z \\geq\u0026nbsp;$\\Theta$ \\\\ 0, otherwise \\end{cases} \\end{equation}\nz - total input calculation\ny - output of the neuron\n[caption id=\"attachment_1166\" align=\"aligncenter\" width=\"322\"]Binary Threshold Neuron[/caption]\nLearning biases Biases can be treated like weight, and can be trained accordingly. We can provide input \"1\" to the \"bias neuron\" and include it in training like any other neuron.\n[caption id=\"attachment_1366\" align=\"aligncenter\" width=\"300\"]Binary Threshold Neuron - Training Biases[/caption]\nTraining\u0026nbsp;Perceptrons Using this training algorithm it is guaranteed that the right answer will be found for each training case if any such set exists.\n add extra component with the constant value of \"1\" to the input vector. The weight of this component is minus the threshold. pick the training cases using policy which makes sure that every training case will keep getting picked (i.e. don't change as we go)  output unit correct: leave its weights output unit incorrecly outputs to \"0\": add the input vector to the weight vector output unit incorrectly outputs to \"1\": subtract\u0026nbsp;the input vector from the weight vector    Geometrical View If we would imagine the weight space as n-dimensional space where each dimension corresponds to the value of a certain weight, then a point in this space would represent a set of weight values. Training cases in this sense would correspond to the planes.\n Considering a training case in which the correct answer is one, the weight vector\u0026nbsp;needs to be on the correct side of hyperplane in order to get the answer right. Having the weight vector on the same side as input vector, means the angle between these two should be less than 90 degrees, implies that the scaler product on the input vector with a weight vector will be positive.\nWhen the weight vector is on the wrong side of the plane, the angle of the weight vector and the input vector will be negative, and the scalar product will be less than zero and we will get a wrong answer. Looking from the other perspective: for any input vector with the correct answer = 0, any weight vector which lies on the angle of 90.  Inputs can be thought of as constains. This means that inputs constrain the set of weights that give the correct classification results by paritioning the space into two halves.\nThere are mathematics proofs that learning of perceptrons would lead to a feasible space if there is any feasible space.\n\u0026nbsp;\nLimitations of Perceptrons Binary threshold neurons cannot sometimes satisfy easiest cases:\n1.\u0026nbsp;Cases that are not linearly separable Imagine the case of sets of 2D inputs, where we want to classify the same feature inputs. In these terms, the points (0,0) and (1,1) should lead to the answer = 1, while (0,1) and (1,0) would lead to the answer = 0.\u0026nbsp;In this example, there's no single set of weights which would satisfy all the constraints.\nWe can notice that there's a set of training cases which are not linearly separable and would lead to non-existence of weight plane which can properly classify the input.\n2. Translations with wrap-arounds Another case which perceptrons could not resolve is the case of discrimination of simple patterns when you translate them with wrap-around.\nBased on this, Minskys and Papert's paper \"Group Invariance Theorem\" (1960s) says that the part of a Perceptron that learns cannot learn to do this if the transformations form a group. (Translations with wrap-around form a group)\nNetworks withou hidden units are very limited in what they can learn to model. What is required is multiple layers of adaptive, non-linear hidden units. We need the way to adapt all the weights, and not just the last layer.\n"
},
{
	"uri": "https://majdarbash.github.io/2016/12/05/downloading-m3u8-videos/",
	"title": "How to download the streaming m3u8 videos formats?",
	"tags": [],
	"description": "",
	"content": "Ubuntu: sudo apt-get install ffmpeg ffmpeg -i http://.../playlist.m3u8 -c copy -bsf:a aac_adtstoasc output.mp4 Mac: brew install ffmpeg ffmpeg -i http://.../playlist.m3u8 -c copy -bsf:a aac_adtstoasc output.mp4"
},
{
	"uri": "https://majdarbash.github.io/2016/10/20/react-native-the-shoe-app/",
	"title": "React Native: Building the &#34;Shoes&#34; App",
	"tags": [],
	"description": "",
	"content": "Overview I was recently experimenting building a react native app. After\u0026nbsp;digging a bit deeper into hybrid development, I have got attracted to the concept of producing a native code, in contrast to the web view approach in most of hybrid apps.\nSo here's what we've got. Setting it up:\nIf you go to https://facebook.github.io/react-native/docs/getting-started.html\u0026nbsp;you'll get some instructions how to set it up.\nAndroid installation  Install JDK Install Android SDK Adding android SDK path to the ~/.bashrc  export ANDROID_HOME=/usr/local/opt/android-sdk    JDK can be installed using this link:\nhttp://www.oracle.com/technetwork/java/javase/downloads/index-jsp-138363.html\nAndroid SDK can be installed using:\nbrew install android-sdk\nAfter finishing these 3 steps, we would need to install some android packages.\nOpen Android package manager:\nandroid These packages will be required to run the android simulator:\n Android SDK Build-tools version 23.0.1 Android 6.0 (API 23) Android Support Repository Intel x86 Atom System Image (for Android 5.1.1 - API 22) Intel x86 Emulator Accelerator (HAXM installer)  You would also require to download IntelHAXM.dmg file, which you can do from:\nhttps://software.intel.com/en-us/android/articles/intel-hardware-accelerated-execution-manager\nRun the installation file after download dmg.\n Install IntelHAXM.dmg  /extras/intel/Hardware_Accelerated_Execution_Manager/IntelHAXM.dmg  is\u0026nbsp;/usr/local/opt/android-sdk on OS X   To check that configuration is done correctly  kextstat | grep intel   You should see message  com.intel.kext.intelhaxm    At this point we would like to create an android virtual device (AVD):\n# launching avd manager android avd  Click \"Create\" and choose the following options for your AVD:\n\nNow your AVD is created and you can start your emulator using:\nemulator -avd # in our example: emulator -avd reactnative iOS installation iOS emulator will be installed automatically as part of Xcode installation. You can install Xcode from the the app store and choose the particular emulator to start from within Xcode.\nInstallation Before proceeding let's make sure that we have the following tools in place:\n brew android node watchman (brew install watchman)  Check out the installation post for more information about\u0026nbsp;how to install these tools.\n# install the react native cli npm install -g react-native-cli Run this script to create your first app (our app is called \"Shoes\"):\nreact-native init Shoes At any point throughout this tutorial, you can run your app using these commands:\n# run your app on iOS react-native run-ios run your app on Android react-native run-android\nIt's time for fun now, let's start coding to build our beautiful shoe app !\nThe \"Shoes\" App Everything is in place now, time to start the work!\nLet's navigate to the directory where we initialized the shoe app:\ncd Shoes directory listing ls -F\n\nWe have got several files here. T0 give possibility to distinguish the code based on the version used, we have 2 entry scripts: index.ios.js and index.android.js which will be parsed and executed based on the corresponding app platform.\n This is how the default index.ios.js script would look like:\n/** * Sample React Native App * https://github.com/facebook/react-native * @flow */ import React, { Component } from \u0026lsquo;react\u0026rsquo;; import { AppRegistry, StyleSheet, Text, View } from \u0026lsquo;react-native\u0026rsquo;;\nexport default class Shoes extends Component { render() { return ( Welcome to React Native!  To get started, edit index.ios.js  Press Cmd+R to reload,{'\\n\u0026rsquo;} Cmd+D or shake for dev menu   ); } }\nconst styles = StyleSheet.create({ container: { flex: 1, justifyContent: \u0026lsquo;center\u0026rsquo;, alignItems: \u0026lsquo;center\u0026rsquo;, backgroundColor: \u0026lsquo;#F5FCFF\u0026rsquo;, }, welcome: { fontSize: 20, textAlign: \u0026lsquo;center\u0026rsquo;, margin: 10, }, instructions: { textAlign: \u0026lsquo;center\u0026rsquo;, color: \u0026lsquo;#333333\u0026rsquo;, marginBottom: 5, }, });\nAppRegistry.registerComponent(\u0026lsquo;Shoes\u0026rsquo;, () =\u0026gt; Shoes);\nWe can notice here that our script\u0026nbsp;consists from several\u0026nbsp;sections.\nIn the top\u0026nbsp;we are importing couple of components which will be needed to run the script. Some components are imported from react, while others from react native modules.\n AppRegistry\nwill be used at the end of the file to register the entry script StyleSheet\nWe are able to define the stylesheet rules by using StyleSheet.create() and passing object of styles as argument View\nThis components acts like container to our app Text\nLabel components, accepts styling  You will notice also that the default class has render() function which is supposed to return the objects to be parsed and rendered in the native app.\nNotation used in return statement of render function() is called JSX - a Javascript syntax\u0026nbsp;extension which make it\u0026nbsp;look like XML.\u0026nbsp;Of course you don't have to write using JSX and can use a plain javascript, however JSX will be much more comfortable.\nYou can read more about JSX here:\nhttps://facebook.github.io/react/docs/jsx-in-depth.html\nAs mentioned in the previous section, we will use the following command to run our code as we developer:\nreact-native run ios Building the Home Page We will start by creating a folder called src in our project root.\n\nWithin the src/ folder we will place our Home page script called home.js.\u0026nbsp;In addition, let's create a file called shoes.js which will be our root components which can be used both within index.ios.js and index.android.js. This is what I expected to have at this point:\nsrc/\n|__ \u0026nbsp;src/shoes.js\n|__ \u0026nbsp;src/home.js\nLet's start working on our beautiful home page. At this point I would like to achieve a blank page with some text in the middle, let's say \"Hello world!\". This won't be a great achievement in comparison to what\u0026nbsp;was already generated in index.ios.js, however it will give as an ideas about how to better organize the code and wrap the default components into more complex\u0026nbsp;reusable components.\nThis is how my home.js\u0026nbsp;file would look like at this point:\n\n\u0026nbsp;\nIn the shoes.js file we would like to refer to import home.js file and import it:\n\nAt this point we have got the Shoes components ready and it's using the Home component. Placing  tags within the Shoes component indicates that rendering Shoes component, will require to instantiate and render Home components. All we want to do now in index.ios.js is to include the\u0026nbsp;Shoes component and set it as an entry point to our app.\n\nNow, try to run using:\nreact-native run-ios \nGreat ! First step accomplished, we got something working!\n"
},
{
	"uri": "https://majdarbash.github.io/categories/events/",
	"title": "Events",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/2016/09/29/facebook-developers-29th-september-2016-dubai/",
	"title": "Facebook For Developers - 29th September 2016, Dubai",
	"tags": [],
	"description": "",
	"content": "Hi there,\nHere are couple of notes for the even I attended. We had a beautiful session starting from 9.50 am talking about how to monetize and distribute the facebook apps and some open source tools. Afterwards, there was a drill down into some technical topics explored in details in sort of a workshop.\nInteresting Technologies:  Facebook Analytics for App - an free tool\u0026nbsp;to track and analyze the behavior of the users within the facebook app.\nOpposed to cookie tracking / session tracking analytics to profile and analyze the user\u0026nbsp;data - or guess his profile information based on connection, location and other parameters, facebook analytics will give you a very deep insights into use profile -\u0026nbsp;based on the facebook profile - which will help you to clearly segment and understand your users. FBStart (fbstart.com) - facebook campaign which targets to help the startups in 3 main areas:  tools - offering some tech tools for a certain limit value, which would help launch your startup mentorship - help and consultation to build a succesful project community - community of other enthusiasts and developers who joined fbstart initiative - sharing ideas and helping each other    Also today, I discovered that facebook has quite a huge number of open source projects. I have heard about several ones, however I had a more comprehensive overview about all other technologies that facebook open sourced:\n Augmented\u0026nbsp;Traffic Control  should be installed in between the network source and the target device simulation of certain network conditions to determine the app usability and performance   Network Connection Class  is an Android Library is used to listen to the current network traffic and categorize\u0026nbsp;the quality of the network this will help you customize your app further for a better customer experience. https://github.com/facebook/network-connection-class   GraphQL - this is where the data is stored - allows execution of very specific requests and returning very specific responses, which will lead to faster apps, less network consumption  Github has declared using GraphQL for some of their APIs. New GraphQL alternatives have emerged afterwards in Ruby, Python, etc...   Redex - if you're building an android app, you should probably consider optimizing your\u0026nbsp;apk through Redex, which will produce a better and more optimized version of the dex code. FlowType / FBInfer - may help you to write some javascript, sort of code assist tool - which can warn you about potential memory leaks, declaration typos / misuse of variables. Nuclide (nuclide.io) - can be installed on top of Atom to make React \u0026nbsp;/ React Native development easier (code assist and syntax checks to the Atom editor) Device Year Class  is an Android Library analyzes an Android device's specifications and calculates which year the device would be considered \"high end\u0026rdquo; based on the devices \"high end\" year you can decide to optimize the CPU, memory or network consumption for a better and smoother experience of your app. https://github.com/facebook/device-year-class   React  tool with which changed the way Web UI are generated and can be rendered code is written in JavaScript UI = Render(state), where state is UI values at specific point in time and render is a function, which manipulates the appearance of certain components based on the state makes UI quite powerful and fast, as the rendering affects only specific component\u0026nbsp;of the page where the change took place   React Native  tool with which you can produce native apps using javascript code code is written in JavaScript native code chunks can be used within the react native app to\u0026nbsp;futher customise the \"common code\" part for a better and more native UX https://facebook.github.io/react-native    You can find more information about\u0026nbsp;facebook open source projects:\nhttps://code.facebook.com/projects\nFacts:  Android is the leading player in the emerging markets - if your app is targeting emerging markets, you'd better put some heavy focus on the android app development People using iPhones tend to have better network connection conditions, which makes the app testing more straightforward:  Alternatively, android apps should be heavily tested under more severe\u0026nbsp;conditions to offer optimal UX.   According to statistics, in 2015, 2 billion people cannot afford a 500 MB data packages - network consumption is still a challenge and should be considered carefully when building apps "
},
{
	"uri": "https://majdarbash.github.io/tags/.svn/",
	"title": ".svn",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/.svn-files/",
	"title": ".svn files",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/genetic-algorithm/01-algorithm-overview/",
	"title": "1.1 - Genetics Algorithm - Algorithm Overview",
	"tags": ["artificial intelligence", "genetics algorithm", "school timetabling problem"],
	"description": "",
	"content": "1.1.1 – Introduction \nGenetic Algorithm is a directed search algorithms based on the mechanics of biological evolution developed by John Holland, University of Michigan (1970’s) to understand the adaptive processes of natural systems and design artificial systems software that retains the robustness of natural systems this algorithms provide efficient, effective techniques for optimization and machine learning applications so it is widely used today in business, scientific and engineering circles.\nSome GA Application Types:\n   Domain\n Application Types\n   **Control** Gas pipeline, pole balancing, missile evasion, pursuit   **Design** Semiconductor layout, aircraft design, keyboard configuration, communication networks   **Scheduling** Manufacturing, facility scheduling, resource allocation   **Robotics** Trajectory planning   **Machine Learning** Designing neural networks, improving classification algorithms, classifier systems   **Signal Processing** Filter design   **Game Playing** Poker, checkers, prisoner’s dilemma   **Combinatorial Optimization** Set covering, travelling salesman, routing, bin packing, graph coloring and partitioning    1.1.2 - Description 1.1.2.1 - Charles Darwin Theory Charles Darwin Theory: “Living organisms are fighting the forces of nature to survive. Those who are the fittest (strongest, fastest, biggest) are most likely to survive, Those who survive mate and reproduce (selection) and children are similar (inheritance), but not exactly like parents because of cross-fertilization and mutation, thus children can be more or less fitness than parents also, Children repeat the path of their parents, after several generations the organisms become much fitter.”\n1.1.2.2 - Genetic algorithm Suppose that there are many possible solutions for the problem: x1, x2, x3, x4… The main idea is to view each solution xi of the problem as an individual living organism. The number of possible solution can be incredibly large n ◊ ∞, so we consider m \u0026lt; n and chose a Population: P(t) = {x1t, x2t, … , xmt} ,with time the organisms and the whole population will be evolving.\n1.1.2.3 - Crossover Crossover is replacing some genes in the parent by the corresponding genes of the other.\nFor example:\nP1 = 101 | 0010 ⇒ O1 = 101 | 1001\nP2 = 011 | 1001 ⇒ O2 = 011 | 0010\n1.1.2.4 - Mutation Mutation is randomly choosing a gene and replacing it with other gene; mutation helps to add diversity to the population and help avoiding local maximum\nFor example:\nO1 = 1011001 ⇒ O1 = 1001101\n1.1.2.5 - Fitness Function Fitness Function is a function that tells you how good the individual is.\n\n1.1.2.6 - Selection Methods Select some of the population for reproduction there are several method for selection\n Roulette Wheel selection: Probability selection from all population with probability proportional of their fitness Ranked selection: few fittest individual  1.1.2.7 - Summary Genetic Algorithm Summary:\n Choose the initial population Select parent chromosomes Perform crossover Perform mutation Evaluate fitness of the new population Repeat 2 until satisfied  "
},
{
	"uri": "https://majdarbash.github.io/genetic-algorithm/02-problem-specification/",
	"title": "1.2 - Genetics Algorithm - Problem Specification",
	"tags": ["artificial intelligence", "genetics algorithm", "school timetabling problem"],
	"description": "",
	"content": "1.2.1 - Introduction We have a set of teachers who needs a schedule for a set of subjects over a set of periods, this schedule must take into account characteristics of periods of week, the limited availability of teacher and rooms, and some constraints which should be fulfilled.\nThe problem of assigning teachers to subjects and groups for timetabling problem has been studied by many authors. Tillet (1975) was the first to address it, using an integer linear programming model.\nThen other authors like Shih and Sullivan (1977), Schniederjans and Kim (1987) and also Badri et al. (1998) developed linear programming models a different approach is followed by Dyer and Mulvey (1976), Bloomfield and McSharry (1979), Mulvey (1982) and Dinkel et al. (1989) who developed capacitated flow models for the problem.\n1.2.2 - Problem Description Our timetabling problem has the following elements:\n A set of groups: A set of subjects: A set of teachers: we have to types of them:   Full time teachers: are all week periods free teachers, they can attend at any time and have no forbidden periods at any time of the week. Part time teachers: they have certain periods to attend in, we must respect those periods in assigning subject.   A set of periods of the school week: the week is divided into five days each day has Five periods the total number of periods for each group is 5*5=25 period, it means a maximum of 30 lessons per week to each group.  1.2.3 - Problem Elements (Objectives and Constraints of the Problem) In a course timetabling problem, generally, constraints are considered in two types. One of them is called hard constraints. Every acceptable timetable must satisfy these Constraints, they are:\n (H1) Every lesson has to be assigned to a period or a number of periods, depending on the lesson\u0026rsquo;s length. (H2) No teacher can give two simultaneous lessons. (H3) No room can be assigned to two simultaneous lessons. (H4) No teacher can give a subject which is out of his specialist, or the set of subjects he can give. (H5) Forbidden periods of the teacher must be respected (no part time teacher can give out of his available times. (H6) The teacher of a specified subject of a certain group must be unique.  The other type of constraints is the soft type, without those constraints an acceptable solution will be generated but the more we stick to those constraints the higher quality of the solution will be fulfilled.\nThose soft constraints are:\n (S1) All lessons of the same section cannot be assigned to the same day must be respected. (S2) Class timetables should be as compact as possible, eliminating idle times for students. (S3) Class timetables should be as compact as possible, eliminating idle times for students. (S4) In class timetables, if possible students should not have a day with a single lesson.  1.2.4 - Genetic Algorithm Application 1.2.4.1 - Flow Chart  Choose the initial population. Select parent chromosomes. Perform Crossover. Perform Mutation. Evaluate fitness of the new population. Repeat 2 until satisfied.  1.2.4.2 - Elements of Our Solution  Nod x: which is a set of variables x (R, S, T, P)R: represents room\nS: represents subject\nT: represents teacher\nP: represents period\nthe number of those variables is equal to the number of all lessons of all groups of all classes. The initial solution (chromosomes):\nWe start working from the initial solution, which is the totally randomly generated population. When starting working we can reach the newer and newer population with better fitness function results, until totally satisfying the hard constraints. When all the hard constraints are satisfied we start working on the soft constraints until they are satisfied too. It’s essential during the work of this algorithm that all the hard constraints should be satisfied, while it’s not such essential that the soft constraints are satisfied. The chromosome is deemed to be a solution, if it satisfies all the hard constraints, and the optimal solution is the one that satisfies all the soft constraints too, producing a fitness function value of 0.   Parent Chromosomes:\nParent chromosomes represent a parent with 2 children – male and female chromosomes, which are the actual data of the parent chromosome. These chromosomes are named as parent chromosomes, because they will reproduce using the crossover and mutation modifications to produce newer and more adaptive to the environment chromosomes, which will be decided by the fitness function, and choosing the most fitted parents for the next population.   Population:\nPopulation represents the current state in the process of the evolution. Population is a set of chromosomes, which is limited to specific size. By continuous reproduction, and choosing the best fit for the next population, the present population is expected to develop to produce better and better results. Crossover Point:\nCrossover is one of the methods used for reproduction. Using this method reproduction is being made. This reproduction method is based on taking parent chromosomes and choosing a point – crossover point, where each one of the parent chromosomes will be divided into two parts. Then, the parent chromosomes reproduce by exchanging their parts, to produce two new chromosomes.One of the parameters needed to be identified how the crossover points are chosen. The basic method for choosing the crossover point is to choose a random cell within the chromosome. The second parameter that needs to be identified is the number of crossovers that has to been. Just as we are doing crossover in one point, we can do it in more than one point that might give a more variety to the reproduced chromosomes. Mutation:\nMutation is another method of reproduction. Mutation is singular method of reproduction, as it is performed on a single chromosome, and not on the parent chromosomes (male and female). Using this method, we choose one of the cell parameters that we want to differentiate from others – to mutate. Then we mutate it to produce some new cell that introduces new data to the chromosome, and thus can affect its fitness.The two parameters that need to be identified here are: what should be mutated – some property of a cell, or all properties of a cell, and the second parameter – what is the best number of mutations that need to be done to introduce some variety, and not to get further from the optimal solution.   Objective function f(x):\nObjective function is the function which takes chromosome as an argument, evaluates it and gives the number, which represents the fitness, or the adaptation of the chromosome to the current environment.\nIn our scenario, the fitness function gives optimal value, when it equals 0. The fitness function is supposed to be evaluated checking the chromosome to match the hard and soft constraints. In our program the genetic algorithm goes through 2 stages of counting fitness function:\n1 – hard stage – here we estimate the hard constraints within the chromosome\n2 – soft stage – here we estimate the soft constraints within the chromosome, is used when the chromosome hard stage is accomplished. Stopping criteria:\nThe basic stopping criteria which can be considered is reaching zero as a fitness function of soft stage. As our program is multi-threaded, user can view the progress, pause and resume the algorithm while it’s running. These options help a user a lot, as the program does not have to make decision when to stop. Depending on the results, and user decision, the stop criteria will be decided, unless the fitness function of the second, soft stage reaches zero.  1.2.4.3 - Programming Elements Used Here you can find the description of the programming elements used in the implementation of the Genetic Algorithm on the problem of Time-Tabling. To support the implementation we have used the OOP approach in our implementation, so the classes where organized into a specific hierarchy to make it easier and more organized, thus producing cleaner and faster implementation.\nHarshly saying, the classes are organized into the following hierarchy:\n\nEach of the above stated classes are described below in detail:\n\nBuffer is a static class that is used in order to communicate with the database and buffer some data. This will speed up the performance and make communication with the database easier.FetchData() – method fetches the data from the database by executing the “SELECT ALL” query, in order to speed up performance of the systemgetRandomCell() – fetches random cell, which should be validgetRandomGCCell() – fetches the random cell, according to the provided groupid argumentWriteChromosome() – saves the provided chromosome to the specified text fileReadChromosome() – reads a saved into text file chromosome and returns it as a result of the method.\n\nCell class represents the smallest building block of the chromosome. If we want to define the chromosome as a schedule, than we can define a cell as a one-slot entry, which specifies the slot number, teacher, room and subject and group to be taught.We provided the cell object with a copy construct, to return a new instance copy of the same cell. Also as we can notice from the above description, GenerateValidCell() will perform a call to the getRandomCell() / getRandomGCCell() of the Buffer static class in order to make current cell as a generated random valid cell, with or without the pre-specified groupid value.MutatedBy() method will accept a value, indicating what is the mutation performed by, and perform a mutation on the cell, executing the corresponding query. The available mutation types are: mutate by all (new generated random cell), mutate by group, mutate by period, mutate by room and mutate by teacher. Notice that the result of the mutation will be a valid cell._cumul property is very important when counting the fitness function. As the fitness function iterates through the current cell, the _cumul property will record the cumulative impact of the current cell on the fitness function total value._tillnow property will be also important as it also record the cumulative impact, but not totally cumulative, but just on the period of last genAlgoSettings.CrossOverCellNumbers number of cells. Then it can be used in future to define which interval of chromosome had the greatest impact on the total fitness function value.f – the flag which will indicate if the current cell has any contribution on worsening the fitness function value. Can be helpful in drawing the chromosome and coloring the bad cells.\n\nAs we said before, a Cell is the smallest building block of the chromosome, so the chromosome is the schedule itself, defined by a set of Cells. Chromosome is the item within the population, an item which will be evaluated by the fitness function, and finally, the output of the program.elements property – is an array of cells, which define the chromosome itselfff – defines the latest fitness function value, which is saved for faster access to avoid evaluating chromosome each timefv – fitness values, which assess the compliance on the chromosome to each of the 4 hard constraintsnumber – number of elements within the chromosomecopy() – the same copy constructor to copy the chromosome as a whole producing a new instance, and new instances of the contained cells\nFF() – evaluate the chromosome using the fitness function – this will update the ff and fv properties of the current chromosome\nGenerateChromosome() - will generate a totally random chromosome of number number of cells and is used to introduce the initial random population\nMarkUsed() – will mark the current chromosome as used, by activating the used property / flag. This will be helpful to avoid the chromosomes that were already chosen as parents in the iteration through the present population. This way we can ensure that in the next iteration the chromosomes used will be not identical to the same parents already chosen.\nclearFlags() – clears all the flags f of a cell\nclearTillNow() - clears all the _tillnow and _cumul properties of the cells to be reused when transferring from the current population to the new population.\nUpdateGroups() – retrieves and updates the groupid’s values of each cell in the chromosome, according to the _subject and _groupnumber properties. This will ensure the consistency of data, when the chromosome of some old data was saved, and need to be retrieved, but the values of the groupid’s has been changed.\nMutation() – mutates the chromosome, using the procedure defined in the Reproduction.Mutation.Run() static method.\n\nFitness function is a very important static class, which defines the procedures and heuristics for counting the fitness function value. Actually the main methods for counting the fitness function values are the count() and countSoft() method. These methods accept a chromosome as an input parameter, and return the total fitness function value as a result of execution.The interesting thing about the fitness function class that it is designed to operate in 2 modes: soft mode and hard mode. Depending on the current stage value, defined by the HeuristicSwitcher.Stage property the fitness function will decide whether to count its value based on the soft or hard constraints. the chromosome function FF() (Chromosome.FF()) will check for the value of HeuristicSwitcher.Stage to match Stages.Hard or Stages.Soft, and will choose to correspondingly return FitnessFunction.count() or FitnessFunction.countSoft() for the current chromosome.The constraints and their weights are multiplied correspondingly to show the impact of the constraint of the fitness function. They are organized as follows:Hard Constraints / Weights:\nHC1() / hc1w, HC2() / hc2w, HC3() / hc3w, HC4() / hc4w\nSoft Constraints / Weights:\nSC1() / sc1w, SC2() / sc2w, SC3() / sc3w, SC4() / sc4w\n\nThis is considered as the main class of our implementation. The GeneticAlgorithm class is responsible for the main code flow management. The class contains several essential properties, which contribute to the work of the algorithm:\n PresentPopulation – the current population, which is the main source of reproduction to create the new population NewPopulation – new population, which is produced step by step by taking parents from the present population, performing and crossovering to reproduce into the new population. ResultChromosome – here the result chromosome of the best fitness function should be saved.  The algorithm consists of the three major parts: init() – initialization, connection to the database, and preprocessing of the data, input() – generating first random PresentPopulation, which will be the initial seed of the program, and run() – implementation of the algorithm itself to obtain the ResultChromosome using the PresentPopulation seed.\n\nHeuristic Controller – this class is responsible for monitoring the performance of the currently used heuristic of reproduction and changing it to obtain better results. This way we ensure that the program will not enter the infinite loop and will try to reproduce in different types of defined techniques to reach better results. In order to monitor the performance of currently active heuristic, several properties are needed to be recorded and constantly reviewed:This is done by two methods of the static class: checkPerformance() – which will check the performance of the current heuristic based on some specific metrics, and takeAction() – takes some specific appropriate heuristic modification action, in case the performance using the active heuristic is unsatisfactory. Check performance works in the following way:\n Gradient – is the difference between the old and new values for the fitness functions (the change in fitness function values) AccGradientAvg – Gradient Average till the current moment in time (iteration) Stepping = Gradient / AccGradientAvg – shows the share of the current gradient from the gradient average. If the Stepping produced is less than the SteppingThreshold, then it’s considered a bad result  The algorithm checks for the number of consequent bad results. If it is more than GenAlgoSettings.SteppingNumberThreshold, than the whole heuristic after several number of chances is considered as bad, and a next heuristic is executed within the takeAction() method.\n\nHeuristic Switcher checks for the currentHeuristic number and redirect the program to the next heuristic using the NextHeuristic() method. A heuristic within our implementation is defined as a set of values for optional attributes, such as mutateby, CrossOverNumber, MutationNumber, SteppingNumberThreshold, SmartCrossOver and CrossOverCellNumbers. Using these parameters we will be able to define specific heuristic, way of reproduction that can be used by the parent chromosomes to reproduce in different ways. The Stage parameter saves the value, defining if the program is current running in the hard or soft stage.\n\nA male and a female chromosome is considered as parent, which will in future reproduce to obtain a better population. This parent object has a copy() constructor, which will call the copy constructors of both the chromosomes to produce a completely new copy of the parent instance.CrossOver() is a method which will recall the Reproduction.CrossOver.Run() in order to following the specific procedure for the current parent chromosomes. MarkUsed() will mark both the parent chromosomes, male and female as used.\n\nPopulation is a set of chromosomes, which should be used to reproduce and obtain better population through evolution. The population contains the property number, which is the pointer to the last filled chromosome in the population. The count property returns the total number of chromosomes within the current population.This class contains several assisting methods to assist the work of the population in an easier way:\n addChromosome() – adding a new chromosome to the population addParents() – add parent to the current population, i.e. adding male and female chromosomes to the current population ChooseParents() – choose the best 2 chromosomes as parents, with the smallest fitness function values, and mark them as used to avoid re-choosing them on the next iteration. clearUsed() – clear used flag for all the chromosomes in the population GeneratePopulation() – generate a population of random valid chromosomes.  getAllFitness() – perform fitness function execution FF() for all the chromosomes in the population.\n\nReproduction class performs the actual reproduction, corresponding to the two methods: CrossOver and Mutation. The two methods of reproduction are defined as classes within the static reproduction class.\n\nStages Enumerator – indicates the HARD and SOFT stage of the running the algorithm. This will determine the fitness function count method chosen and the heuristics and their order.\n"
},
{
	"uri": "https://majdarbash.github.io/genetic-algorithm/03-user-manual/",
	"title": "1.3 - Genetics Algorithm - User Manual",
	"tags": ["artificial intelligence", "genetics algorithm", "school timetabling problem"],
	"description": "",
	"content": "1.3.1 - Operation Specification The program should be executed as follows:\nData entries for the program should be supplied within the tables in the “genalgo.accdb” database file. These tables contain the teacher definitions, teacher available times, subjects, teacher subjects mapping, rooms and corresponding room types. From all these tables in the preprocessing stage, the groups and then the cells tables are created, which will be used then as an input to our program.\nWe can execute our program from the “SPGenAlgo.exe”. This file will load the database file into memory, read input data from it, and run our Genetic Algorithm implementation in easy-to-use, interactive and multi-threaded way, for best performance and more user-friendly environment. These is the screen displayed when running the program:\n\nThere are several useful labels and form components, which give us very useful feedback about the progress of the program:\n\n\nAfter a while, after satisfying all the hard constraints in the program, the program will give the feedback, through the current stage flag.\n1.3.2 - Performance Metrics According to our test and performance estimations, we have found out that it takes about 450 seconds to obtain the fitness function value below 10 within the hard stage. As about the soft stage, the value is not well defined, and cannot be estimated or tested, cause it’s totally dependent on the case we are working with. Sometimes, the working schedule, may not satisfy any of the soft constraints at all.\n\nAfter working about 740 seconds on satisfying the hard constraints we have obtained the following results:\n\nAccording to our estimations, it would take about 700 – 1000 seconds to obtain the hard constraints satisfied, i.e. a maximum of about 15 minutes.\n"
},
{
	"uri": "https://majdarbash.github.io/genetic-algorithm/04-future-vision-for-optimality/",
	"title": "1.4 - Genetics Algorithm - Future Vision for Optimality",
	"tags": ["artificial intelligence", "genetics algorithm", "school timetabling problem"],
	"description": "",
	"content": "We are proud to see amazing results for the timetabling problem. As stated in the section before, that the maximum time for satisfying essential hard constraints was 15 minutes. This time is good enough, but in worse cases, the performance will be worse of course. This is why we introduce a new area for improvement in future – we would consider that it may be really efficient if we define and implement new heuristics within the HeuristicSwitcher, which might help us a lot in finding solution faster.\nThe second notice is that the values of the all the parameters might be not crisp values, but some fuzzy values. This idea directly introduces the concept of integrating this system with some fuzzy system to describe the Heuristic used, and may also be used to provide feedback about teachers, courses, rooms and times.\nA big field of improvement may be to consider some swapping techniques, or procedures that can be done to improve the schedule. We don’t mean here just to set some parameters in order to create a Heuristic for working with Ordinary CrossOver and Mutation or Smart CrossOver, but we mean here that some set of steps might be applied on each algorithm in order to become more efficient, without the need to apply complex, and relatively random CrossOver and Mutation techniques.\nAnd finally, the Genetic Algorithm Implementation was a great part of project. We have discovered new ways and understood it, and modified it to accommodate our needs, introducing Heuristics, SmartCrossOvers, Mutation By and some other components, to make the Algorithm smarter, in order to obtain the results we have just seen in a good time like this.\n"
},
{
	"uri": "https://majdarbash.github.io/random/accessing-docker-with-non-root-user-privileges/",
	"title": "Accessing Docker with non-root user privileges",
	"tags": [],
	"description": "",
	"content": "Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.39/containers/json: dial unix /var/run/docker.sock: connect: permission denied\nDoes this error look familiar?\nWhat is happening is that docker client is communicating with the engine using socket file. As docker service is launched using root user and socket file is accessible to every user in \u0026ldquo;docker\u0026rdquo; group, you will not be able to connect to this file from docker client executed launched with your current user access.\n2 possible solutions:\n Use docker with root privileges  sudo docker ps Grant your user access to the socket file by adding to the \u0026ldquo;docker\u0026rdquo; group  sudo usermod -a -G docker $USER  Publish Date: 2019-06-13\n "
},
{
	"uri": "https://majdarbash.github.io/tags/account/",
	"title": "account",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/overview-caf-whitepaper/",
	"title": "An Overview of the AWS Cloud Adoption Framework (2017)",
	"tags": ["aws", "whitepaper", "cloud-adoption-framework", "caf"],
	"description": "",
	"content": " Six Perspective of AWS CAF Mapping the Journey to the Cloud AWS CAF Perspectives: Additional Detail  Business Perspective: Value Realization People Perspective: Roles and Readiness Governance Perspective: Prioritization and Control Platform Perspective: Applications and Infrastructure Security Perspective: Risk and Compliance Operations Perspective: Manage and Scale    Six Perspective of AWS CAF The perspective below describe the common roles and objectives to update staff skills and organizational processes.\n Business Perspective  Business Managers, Finance Managers, Budget Owners, and Strategy Stakeholders Optimize business value as they move their operations to the cloud   People Perspective  Human Resources, Staffing, and People Managers Ensure business governance in the cloud, and manage and measure cloud investments to evaluate their business outcomes   Governance Perspective  CIO, Program Managers, Project Managers, Enterprise Architects, Business Analysts, and Portfolio Managers Ensure business governance in the cloud, and manage and measure cloud investments and evaluate their business outcomes   Platform Perspective  CTO, IT Managers, Solution Architects Deliver and optimize cloud solutions and services   Security Perspective common roles  CISO, IT Security Managers, and IT Security Analysts Ensure that architecture deployed in the cloud aligns to the organization\u0026rsquo;s security control requirements, resiliency, and compliance requirements   Operations Perspective  IT Operations Managers and IT Support Managers Ensure system health and reliability during the move of operations to the cloud and then to operate using agile, ongoing, cloud computing best practices    Mapping the Journey to the Cloud  Understand your organization\u0026rsquo;s current state, the target state and the transition required Set goals and create work streams that will enable staff to thrive in the cloud Engage stakeholders with relevant AWS CAF Perspectives to own organizational and operational change for their area Explore capability gaps in terms of AWS CAF Skills and Processes, define work streams and interdependencies between work streams Optimize collaboration on AWS  AWS CAF Perspectives: Additional Detail Business Perspective: Value Realization Ensure that IT is align with business needs and that IT investments can be traced to demonstrable business results.\nAWS CAF Business Perspective Capabilities:\n IT Finance  Capability to plan, allocate, and manage the budget for IT expenses given changes introduced with the cloud services consumption model Common budgeting change: moving from capital asset expenditures and maintenance to consumption-based pricing   IT Strategy  Capability to leverage IT as a business enabler Leveraging cloud services to reduce the need to maintain applications Enabling IT to focus on business alignment   Benefits Realization  Capability to measure the benefits received from IT investments Total Cost of Ownership (TCO) or Return on Investment (ROI) calculations coupled with budget management Quantifying and evaluation TCO and ROI changes with cloud services Directly link consumption with specific business processes   Business Risk Management  Capability to understand the business impact of preventable, strategic, and external risks to the organization With the move to the cloud, many of these constraints are reduced or eliminated    People Perspective: Roles and Readiness AWS CAF People Perspective Capabilities:\n Resource Management  Capability to project personnel needs and to attract and hire the talent necessary to support organization\u0026rsquo;s goals Acquiring new skills and processes to forecast and staff based on organization\u0026rsquo;s needs   Incentive Management  Capability to ensure that workers receive competitive compensation and benefits for the value they bring to your organization Some IT roles move from being commoditized to being highly specialized with high market demand Ability to provide an environment for attracting and retaining talent plays a key role in successful adoption   Career Management  Capability to ensure the personal fulfillment of your employees, their career opportunities, and their finance security Introducing change to IT career paths, requiring HR managers and people managers to update career management skills and processes   Training Management  Capability to ensure employees have the knowledge and skills necessary to perform their roles and comply with organizational policies and requirements Frequently update the knowledge and skills require to implement and maintain cloud services Training modalities may need to be revised to embrace the speed of change and innovation   Organizational Change Management  Capability to manage the effects and impacts of business, structural, and cultural change introduced with cloud adoption Central to successful cloud adoption Skills to manage ongoing change    Governance Perspective: Prioritization and Control AWS CAF Governance Perspective Capabilities:\n Portfolio Management  Capability to manage and prioritize IT investments, programs, and projects in alignment with your organization\u0026rsquo;s business goals New skills and processes to evaluate cloud services and a workload\u0026rsquo;s eligibility for the cloud   Program and Project Management  Capability to manage one or several related projects to improve organization performance and complete the projects on time and on budget Updating the skills and processes to take advantage of the agility and cost management features of the cloud services Skills to manage agile-style projects   Business Performance Management  Capability to measure and optimize processes in support of your organization\u0026rsquo;s goals Rapidly experiment with new means of process automation and optimization New skills to define cloud-centric KPIs Create processes to ensure cloud consumption is mapped to business outcomes   License Management  Capability to procure, distribute and manage the licenses needed for IT systems, services, and software New skills for procurement and license management and new processes for evaluating license needs    Platform Perspective: Applications and Infrastructure AWS CAF Platform Perspective Capabilities:\n Compute Provisioning  Capability to provide processing and memory in support of enterprise applications Skills and processes to provision cloud services Move from being focused on real-world logistics to being focused on virtual and fully automated processes   Network Provisioning  Capability to provide computing networks to support enterprise applications Skills and processes required to design, implement, and manage this transition   Storage Provisioning  Capability to provide storage in support of enterprise applications Skills and processes required to provision cloud-based block and file storage   Database Provisioning  Capability to provide database and database management systems in support of enterprise applications Skills and processes to provisioning standard RDMS in the cloud and leveraging cloud-native databases   Systems and Solution Architecture  Capability to define and describe the design of a system and to create architecture standards for the organization Architect skills to codify architectures in templates and create new processes for workload optimization   Application Development  Capability to customize or develop applications to support your organization\u0026rsquo;s business goals Skills and processes for Continuous Integration and Continuous Deployment (CI/CD)    Security Perspective: Risk and Compliance Helps you structure the selection and implementation of security controls that meet your organization\u0026rsquo;s needs.\nAWS CAF Security Perspective Core Capabilities:\n Identity and Access Management  Capability that enables you to create multiple access control mechanisms and manage the permissions for each of these within your AWS account   Detective Control  Capability for native logging as well as services that you can leverage to provide greater visibility near to real time for occurrences in the AWS environment Consider integrating AWS logging features into centralized logging and monitoring solutions Provide holistic visibility near to real time   Infrastructure Security  Capability to shape your AWS security controls in an agile fashion Automating your ability to build, deploy and operate your security infrastructure IT Security teams update their skills and processes so that they can leverage these new features   Data Protection  Capability for maintaining visibility and control over data, and how it is accessed and used in the organization   Incident Response  Capability to respond, reduce harm, and restore operation during and after a security incident Shifting primary focus of the security team from response to performing forensics and root cause analysis    Operations Perspective: Manage and Scale AWS CAF Operations Capabilities:\n Service Monitoring  Capability to detect and respond to issues and application health issues Can be highly automated, resulting in greater service uptime Skills to leverage cloud features for monitoring and automate many of the existing service monitoring processes   Application Performance Monitoring  Capability to ensure application performance meets its defined requirements Skills and processes to use features to monitor and right-size the cloud services to meet performance requirements   Resource Inventory Management  Capability to align your organization\u0026rsquo;s assets in a way that provides the best, most cost efficient service Removes the need to manage hardware assets and life cycle Simplify management of software licensing, leveraging on-demand techniques Skills and processes to ensure operations teas can manage cloud assets   Release Management / Change Management  Capability to manage, plan, and schedule changes to the IT environment Leveraging CI/CD techniques to rapidly manage releases and roll-backs   Reporting and Analytics  Capability to ensure compliance with your organization\u0026rsquo;s reporting policies and to ensure ongoing analysis and reporting of performance against key KPIs such as SLAs and OLAs (Operational-level agreements) Skills and processes to take advantage of new features to provide better detail and granularity in reporting and analytics   Business Continuity / Disaster Recovery (BC/DR)  Capability to operate in the event of a significant failure of IT services and the capability to recover from those failures within the time parameters defined by your organization   IT Service Catalog  Capability to select, maintain, advertise, and deliver an SLA or set of IT Services Control mechanism to ensure that your organization selects the services that provide the best business value while minimizing business risk    "
},
{
	"uri": "https://majdarbash.github.io/tags/angular/",
	"title": "angular",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/angular-js-core-concepts/",
	"title": "AngularJS 1: Core concepts",
	"tags": ["angular", "js"],
	"description": "",
	"content": "Core Concepts  Directives - HTML annotations that trigger Javascript behaviors Modules - Where our application components live Controllers - Where we add application behaviour Expressions - How values get displayed within the page  Module / Controller Initialization var app = angular.module(‘store’, \\[\\]); ng-app=\u0026quot;store\u0026quot;\u0026gt; … this way the html inside the body tag will be treated as a part of angular application\n{{ expression }} - this will be evaluated using the javascript\n(function(){ var app = angular.module(\u0026#34;store\u0026#34;, []); app.controller(\u0026#34;StoreController\u0026#34;, function(){ this.product = gem; }); var gem = { name: \u0026#34;Product name\u0026#34;, price: 2.95, description: \u0026#34;description\u0026#34;, canPurchase: false, soldOut: true }; })();  \u0026lt;html ng-app=\u0026#34;store\u0026#34;\u0026gt; \u0026lt;body ng-controller=\u0026#34;StoreController as store\u0026#34;\u0026gt; \u0026lt;div ng-hide=\u0026#34;store.product.soldOut\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;{{ store.product.name }}\u0026lt;/h1\u0026gt; \u0026lt;div\u0026gt;{{ store.product.price }}\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;{{ store.product.description }}\u0026lt;/div\u0026gt; \u0026lt;button ng-show=\u0026#34;store.product.canPurchase\u0026#34;\u0026gt;Add to Cart\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Loops with ng-repeat \u0026lt;div ng-repeat=\u0026#34;product in store.products\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;{{ product.name }}\u0026lt;/h1\u0026gt; \u0026lt;div\u0026gt;{{ product.price }}\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;{{ product.description }}\u0026lt;/div\u0026gt; \u0026lt;button ng-show=\u0026#34;product.canPurchase\u0026#34;\u0026gt;Add to Cart\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; Publish Date: 2015-04-08\n "
},
{
	"uri": "https://majdarbash.github.io/tags/apache/",
	"title": "apache",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/apache2/",
	"title": "apache2",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/artificial-intelligence/",
	"title": "artificial intelligence",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/assets/",
	"title": "assets",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/asynchronous-programming/",
	"title": "asynchronous programming",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/aurora/",
	"title": "aurora",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/automating-elasticity-whitepaper/",
	"title": "Automatic Elasticity Whitepaper (2018)",
	"tags": ["aws", "whitepaper"],
	"description": "",
	"content": " Types of Elasticity** Monitoring AWS Service Usage and Costs** Tagging Resources Automating Elasticity  Automating Time-Based Elasticity Automatic Volume-Based Elasticity    Types of Elasticity**  Time-based elasticity  Turning off resources when they are not being used   Volume-based elasticity  Matching scale to the intensity of demand, whether that\u0026rsquo;s compute cores, storage sizes, or throughput    Monitoring AWS Service Usage and Costs**  Cost Optimization Monitor  Detailed billing reports Break down by period, account, resource, or custom resource tags   Cost Explorer  View data up to the last 13 months Get recommendations on what Reserved Instances to purchase See trends that can help you understand your costs    Tagging Resources  Tagging resources helps get better visibility and control over cloud IT costs down  Automating Elasticity Automating Time-Based Elasticity  AWS Instance Scheduler  Start and stop schedules for your EC2 instances Deployed using an AWS CloudFormation template Specify the required parameter during the deployment Recurring AWS Lambda function will automatically start and stop appropriately tagged EC2 instances   Amazon EC2 API tools  Instances can be terminated programmatically using EC2 APIs Use StopInstances and TerminateInstances   AWS Lambda  Serverless functions can be used to shutdown instances that are not being used Trigger Lambda by Amazon CloudWatch Events   AWS Data Pipeline  Run AWS CLI commands on a set schedule   Amazon CloudWatch  Collect and track metrics and log files, set alarms and automatically react to changes in your AWS resources    Automatic Volume-Based Elasticity Auto-Scaling can be used for::\n Amazon EC2 Auto Scaling - allows to scale EC2 instances Amazon Elastic Container Service Amazon EC2 Spot Fleets Amazon EMR clusters Amazon AppStream 2.0 stacks and fleets Amazon DynamoDB  "
},
{
	"uri": "https://majdarbash.github.io/tags/aws/",
	"title": "aws",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/aws-applications/",
	"title": "AWS Applications",
	"tags": [],
	"description": "",
	"content": "SQS (Simple Queue Service)  the first AWS service distributed queue system temporary repository for messages that are waiting for processing one component in the application queues messages to be consumed by another component in the application can be used to decouple the components of an application messages can contain up to 256 kb of text\nyou can store up to 2GB per message, however, it will be stored and retrieved from S3 in this case messages can be retrieved using AWS SQS API auto-scaling events can be configured based on queue sizes as well pull-based, not pushed based\nmessages should be consumed from the queues, SQS will not push the messages messages can be kept in the queue from 1 minute to 14 days the default retention period is 4 days visibility timeout - the amount of time that the message is invisible in the SQS queue after a reader picks up that message. If the job is not processed within visibility timeout, the message will become visible again and can be processed by another reader. maximum visibility timeout is 12 hours polling types:  short polling - returns immediately even if the message queue being polled is empty long polling - doesn\u0026rsquo;t return a response until a message arrived in the message queue or the long poll times out   queue types:  Standard Queue  nearly unlimited number of transactions per second a message is delivered at least once\noccasionally more than one copy of a message might be delivered out of order allows high throughput provides best-effort ordering\nensures that messages are generally delivered in the same order as they are sent   FIFO Queue  first-in-first-out ordered queues the order is strictly preserved and guaranteed the message remains available until a consumer processes and deletes it; duplicates are not introduced into the queue support message groups that allow multiple ordered message groups within a single queue limited to 300 transactions per second (TPS)      SWF (Simple Workflow Service)  makes it easy to coordinate work across distributed application components workflow executions can last up to 1 year presents a task-oriented API ensures that the task is assigned only once keeps track of all the tasks and events in an application SWF actors:  Workflow Starters\nAn application initiating a workflow.\n(e.g. an e-commerce website following the placement of order) Deciders\nControl the flow of activity tasks in a workflow execution. If something has finished (or failed) in a workflow, a Decider decides what to do next. Activity Workers\nCarry out the activity tasks    SNS (Simple Notification Service)  Service for sending notifications from the cloud Can deliver notifications to devices (via Push notifications), SMS, Email, SQS queues or to any other HTTP endpoint Push notifications to Apple, Google, FireOS, Windows devices and Android devices in China with Baidu Cloud Push Recipients are grouped using Topics One topic can support deliveries to multiple endpoint types The message published to a topic will be delivered to each subscriber Messages published to SNS are stored across different AZs Push-based delivery (no polling) Simple APIs and easy integration with applications Inexpensive, pay-as-you-go model with no up-front costs  SNS vs SQS  Both Messaging Services in AWS SNS - Push SQS - Polls (Pulls)  Elastic Transcoder  Media Transcoder in the cloud Convert media files from their original source format into different formats that will play on smartphones, tablets, PCs, etc. Provides transcoding presets for popular output formats Pay based on the minutes that you transcode and the resolution at which you transcode  API Gateway  Fully managed service that makes it easy for developers to publish, maintain, monitor and secure APIs at any scale. Features  Expose HTTPS endpoints to define a RESTful API Serverless-ly connect to services like Lambda \u0026amp; DynamoDB Send each API endpoint to a different target Run efficiently with low cost Scales automatically Track and control usage by API key Throttle requests to prevent attacks Connect to CloudWatch to log all requests for monitoring Maintain multiple versions of your API   Configuration  Define an API Define Resources and nested Resources (URL Paths) For each resource:  Select supported HTTP methods (verbs) Set security Choose target (such as EC2, Lambda, DynamoDB, etc.) Set request and response transformations     Deployment  Uses API Gateway domain, by default Can use a custom domain Now supports AWS Certificate Manager: free SSL/TLS certs   API Caching (TTL specified)\nWill help you reduce the number of requests made to your endpoint, improving the latency of the requests to your API. Same Origin Policy\nWeb browser permits scripts contained in a first web page to access data in a second web page, but only if both web pages have the same origin. This prevents XSS attacks. CORS can be enabled on the API Gateway  CORS (Cross-Origin Resource Sharing)\nallows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served.  The browser makes HTTP OPTIONS call The server returns a response listing other domains that are approved to GET this URL      Kinesis  Service working with the streaming data Makes it easy to load and analyze streaming data Streaming Data is data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes). Types  Kinesis Streams Kinesis Firehose Kinesis Analytics   Kinesis Streams  Receive data from producers and retain it until consumed Producers - Produce streaming data and stream it to Kinesis streams Consumers - receive data from Kinesis Streams and act on it Retention: 24 hours - 7 days Consists of Shards  5 transactions per second for reads a maximum total data read rate of 2MB per second up to 1000 records per second for writes maximum total write rate of 1MB per second   Data capacity of the stream is determined by the number of shards and their capacities   Kinesis Firehose  Data received could optionally execute a lambda function and will be output to it\u0026rsquo;s the destination which could be S3, ElasticSearch There\u0026rsquo;s no data persistence Data will be forwarded immediately to the target destination   Kinesis Analytics  analyzes the data inside both of Kinesis Firehose and Kinesis Streams, with on-the-fly analysis capability    FAQs  SQS Billing is calculated per request, plus data transfer charges for data transferred out of Amazon SQS 1 million requests per month - fall under the free tier Batch operations cost the same as other SQS requests. Grouping messages into batches, you can reduce the SQS costs. All the messages in SQS have a globally unique ID that SQS returns when the message is delivered to the message queue. This ID is useful for tracking the receipt of a particular message in the message queue. Dead letter queues receive messages from other source queues after a maximum number of processing attempts cannot be completed. SQS message can contain up to 10 metadata attributes - applications can determine how to process the message based on the metadata instead of inspecting the entire message. (attributes: name-type-value triples) SentTimestamp attribute - contains information when the message was sent by a producer and queued by SQS. SenderId attribute - contains either the AWS account ID or the IP address for the sender. Amazon SQS APIs provide deduplication functionality for FIFO queues that prevents your message producer from sending duplicates. Any duplicates introduced by the message producer are removed within a 5-minute deduplication interval. If using standard queues you may experience duplicates - the application must be designed to be idempotent - not affected when processing the same message more than once). Queue type can be chosen only on creation. If you want to convert from one type to another you will have to recreate the queue. In FIFO queues, messages are ordered based on the message group ID. It\u0026rsquo;s a required field when sending a message to the queue. After you\u0026rsquo;re done processing the message, you are responsible for deleting it. Server-side encryption (SSE) - SSE encrypts messages as soon as Amazon SQS receives them using keys in AWS KSM (AWS Key Management Service). The messages are stored in encrypted form and are decrypted when sending a message to an authorized consumer. SSE encrypts the body of the message. The queue metadata, message metadata, and per-queue metrics are not encrypted. SSE uses AES-GCM 256 algorithm. Amazon SQS is PCI DSS Level 1 certified and HIPAA Eligible Service. SQS messages can contain text data, including XML, JSON and unformatted text. The number of inflight messages is limited to 120k for standard queue and 20k for FIFO queue. Inflight messages are those messages that have been received by a consuming component but have not yet been deleted. Queue names are limited to 80 characters.  "
},
{
	"uri": "https://majdarbash.github.io/aws/best-practices-ddos-resiliency-whitepaper/",
	"title": "AWS Best Practices for DDoS Resiliency (2019)",
	"tags": ["aws", "whitepaper", "aws-best-practices", "ddos", "resiliency"],
	"description": "",
	"content": " Denial of Service Attacks Infrastructure Layer Attacks Mitigation Techniques Monitoring Support  Protecting the business from the impact of Distributed Denial of Service attacks (DDoS):\n Maintaining availability and responsiveness of your application Avoid unnecessary cost of scaling infrastructure in response to an attack  Denial of Service Attacks DDoS attacks:\n Uses multiple sources, distributed groups of malware infected devices to orchestrate an attack against a target Hosts generate a flood of packets or requests to overwhelm the target DDoS are most common at layers 3,4,6 and 7 of the OSI model  Examples of attacks:\n HTTP floods, DNS query floods - Application layer TLS Abuse - Presentation layer Interhost communication - Session layer SYN floods - Transport layer UDP reflection attacks - Network layer Physical addressing - Data link layer Media, signal and binary transmission - Physical layer  Infrastructure Layer Attacks UDP Reflection Attack:\n Attacker generates a valid UDP request Attacker spoofs the source IP, by indicating the victim\u0026rsquo;s IP address Request is sent to an intermediate server  Server is tricked into sending its UDP response packets to the victim Response is several times larger then request Amplification factor is the ration of response size to request size     SYN Flood Attacks\n Malicious client sends a large number of SYN packets, without sending the final ACK packets to complete the handshakes Server is waiting for a response to the half-open TCP connections Server runs out of capacity to accept new TCP connections  Application Layer Attacks\n HTTP flood attack  Attacker sends HTTP requests that appear to be from a role user of the web application HTTP floods may target a specific resource HTTP floods may attempt to emulate human interaction with application, making it increasingly difficult to use techniques like request rate limiting   Cache-busting attacks  Query string variations are used to circumvent content delivery network (CDN) caching CDN must contact the origin server and that can cause additional strain on the application web server   WordPress XML-RPC flood attack  Known as WordPress pingback flood Attacker misuses the XML-RPC API function Pingback feature allows Site A to notify site B about link created, which will then attempt to fetch Site A to verify existence of the link Attacker misuses this capability to cause Site B to attack Site A   DNS Query Flood  Attacker uses many well-formed DNS queries to exhausted the resources of DNS server May include cache-busting component, randomizing the subdomain string to bypass the local DNS cache of any given resolver   TLS attacks  TLS is computationally expensive Attacker can reduce server\u0026rsquo;s availability by sending unintelligible data Attacker may complete the TLS handshake but perpetually renegotiates the encryption method Attacker can attempt to exhaust server resources by opening and closing many TLS sessions    These attacks are not DDoS attacks, but their automated nature can look similar to DDoS attack:\n Scraper Bots  Automate attempts to access a web application to steal content or record competitive information, e.g. pricing   Brute force and credential stuffing  Gaining unauthorized access to secure areas of an application    Mitigation Techniques  AWS Shield Standard Protection  Applies to all customers with no additional charges Most frequently occurring network and transport layer DDoS attacks   Use Amazon CloudFront and Amazon Route 53  Using Edge services, operating from edge locations improves DDoS resilience of your application AWS Shield DDoS mitigation systems are integrated with edge services Stateless SYN Flood mitigation techniques verifies the incoming connections before passing them to the protected service Automatic traffic engineering systems that can disperse or isolate the impact of large volumetric DDoS attacks AWS WAF - application layer defense   AWS Shield Advanced  Protects an application hosted on any AWS region Available for Classic Load Balancer, Application Load Balancer and Elastic IP Addresses  Protect Network Load Balancer (NLB) or EC2 instances using Shield Advanced with EIPs   Available globally from CloudFront and Route 53   AWS WAF  Allows to mitigate application layer flood attacks Block IP addresses using rate-based rules Use AWS Marketplace solutions to block malicious IP addresses that are included in reputation lists Review web server logs and WAF\u0026rsquo;s logging to understand the traffic that is being analyzed by your Web ACL Whitelist only Query String parameters that are relevant to be cached for your application, to mitigate cache-busting attacks   AWS Firewall Manager  Allows to centrally configure and manage AWS WAF rules across your organization AWS Organizations Master account can designate an administrator account Administrator account is authorized to create Firewall Manager policies Also allows creating policies to manage AWS Shield resource and VPC security groups   Attack Surface Reduction  Limit internet exposure of internal resources (e.g. Database, worker nodes behind Load Balancers \u0026hellip;)  Obfuscating AWS Resources Use SGs and NACLs   If using CloudFront with an origin inside VPC, AWS Lambda should be used to automatically update your SG rules to allow CloudFront traffic  Malicious users cannot bypass Amazon CloudFront and WAF Use X-Shared-Secret to help validate that requests made to your origin were sent from Amazon CloudFront   Use API Gateway as an entryway to applications running on EC2  Help protect the internal resources Use x-api-key to protect against direct traffic to API Gateway (CloudFront traffic only) Protect your backend from excess traffic by configuring standard or burst rate limits for each method in your REST APIs     ALB, Auto Scaling  Scale to absorb application layer traffic CloudWatch alarms can be set to initiate Auto Scaling events   Ensuring enough transit capacity and diversity is available  Instances with 25 Gigabit interface, or enhanced networking can handle larger volume of traffic - this helps prevent interface congestion   Use CloudFront and Load Balancers  Amazon Cloud Front, Application Load Balancer, Classic Load Balancers or Network Load Balancer handle TLS negotiation This protects your application from TLS-based attacks Scales to handle TLS abuse attacks ALB can route only well-formed requests, mitigating common DDoS attacks like SYN floods or UDP reflection attacks Offload traffic by leveraging CloudFront caching at edge For Amazon S3 buckets, use AWS CloudFront with Origin Access Identity (OAI) to protect your bucket    AWS Shield Advanced Features:\n Access to the AWS DDoS Response Team (DRT) for assistance in mitigating DDoS attacks that impact application availability DDoS attack visibility by using the AWS Management Console, API and CloudWatch metrics / alarms Access to Global Threat Environment dashboard (overview of DDoS attacks observed and mitigated by AWS) Access to AWS WAF - at no additional cost - for the mitigation of application layer DDoS attacks (used with Amazon CloudFront or ALB) Automatic baselining of web traffic attributes, when used with AWS WAF Access to AWS Firewall Manager, at no additional cost, for automated policy enforcement Sensitive detection threshold which routes traffic into DDoS mitigate system and can improve time-to-mitigate attacks against Amazon EC2 or NLB, when used with EIP Cost protection allowing to request a limited refund of scaling-related costs that result from DDoS attack Enhanced SLA that is specific to AWS Shield Advanced customers  Monitoring Most common CloudWatch Metrics used to detect and react to DDoS attacks:\n AWS Shield Advanced  DDoSDetected DDoSAttackBitsPerSecond DDoSAttackPacketsPerSecond DDoSAttackRequestsPerSecond   AWS WAF  AllowedRequests BlockedRequests CountedRequests   Amazon CloudFront  Requests TotalErrorRate   Amazon Route 53  HealthCheckStatus   ALB  ActiveConnectionCount ConsumedLCUs -\u0026gt; number of load balancer capacity units used by your load balancer HTTPCode_ELB_4XX_Count HTTPCode_ELB_5XX_Count NewConnectionCount ProcessedBytes RejectedConnectionCount RequestCount TargetConnectionErrorCount TargetResponseTime UnHealthyHostCount   NLB  ActiveFlowCount -\u0026gt; total number of concurrent TCP flows or connections from clients to targets ConsumedLCUs -\u0026gt; number of load balancer capacity units used by your load balancer NewFlowCount ProcessedBytes   Auto Scaling  GroupMaxSize   Amazon EC2  CPUUtilization NetworkIn -\u0026gt; number of bytes received by the instance on all network interfaces    Support  Consider subscribing to Business Support with 24x7 access to Cloud Support Engineers to assist with DDoS attack issues Consider Enterprise SUpport for mission critical workloads for fastest response from a Senior Cloud Support Engineer AWS Shield Advanced gives you access to DRT - AWS DDoS Response Team  "
},
{
	"uri": "https://majdarbash.github.io/random/aws-cdk/",
	"title": "AWS CDK",
	"tags": [],
	"description": "",
	"content": " Open source software framework to model and provision your cloud application resources using familiar languages.  Supported Languages  TypeScript/Javascript Python Java C#  Workflow  Initialize CDK project: cdk init --language=typescript Write Code - code your infrastructure using one of supported languages above Synthesize - produces CloudFormation templates for the written code: cdk synth Deploy - deploys CloudFormation template as stack to your AWS account: cdk deploy  Examples Github: Creating Highly Available Network using AWS CDK + TypeScript\nimport * as cdk from \u0026#39;@aws-cdk/core\u0026#39;; import { Peer, Port, SecurityGroup, SubnetType, Vpc } from \u0026#39;@aws-cdk/aws-ec2\u0026#39; export class AppStack extends cdk.Stack { params = { \u0026#39;cidr\u0026#39;: \u0026#39;10.10.0.0/16\u0026#39; } constructor(scope: cdk.Construct, id: string, props?: cdk.StackProps) { super(scope, id, props); const vpc = new Vpc(this, \u0026#39;HAVpc\u0026#39;, { cidr: this.params.cidr, maxAzs: 3, subnetConfiguration: [{ cidrMask: 26, name: \u0026#39;isolatedSubnet\u0026#39;, subnetType: SubnetType.ISOLATED }] }) } } "
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/aws-cloud-best-practices/",
	"title": "AWS Cloud Best Practices",
	"tags": [],
	"description": "",
	"content": "Difference between Traditional and Cloud Computing Environments  IT Assets as Provisioned Resources  Quick deployment time No hardware commitment   Global, Available, and Scalable Capacity Higher-Level Managed Services\nIn addition to EC2 instances, you can benefit from a lot of great services which will be provisioned and maintained with minimum effort. Built-In Security Architecting for Cost  AWS provides fine-grained billing, which enables you to track the costs associated with all aspects of your solutions   Operations on AWS  Infrastructure as Code Higher levels of automation of the operational processes as the supporting services, e.g. AWS Auto Scaling and self-healing architectures Full automation through DevOps processes for delivery pipeline and management   Disposable Resources Instead of Fixed Servers  Design Principles  Scalability Disposable resources instead of fixed servers Automation Loose coupling Services, not servers Databases Managing increasing volumes of data Removing single points of failure Optimize for cost Caching Security  Scalability   Scaling Vertically\nIncrease in the specifications of an individual resource, such as upgrading a server with a larger hard drive or a faster CPU\n  Scaling Horizontally\nIncrease in the number of resources, such as adding more hard drives to a storage array or adding more servers to support an application\n  Stateless Applications\n An application that does not need knowledge of previous interactions and does not store session information Given the same input provides the same response to any end-user Can scale horizontally because any available compute resources can service any requests    Distributing the load\n Push model  ELB, ALB, Network Load Balancer, Route53 load balancing   Pull model  for asynchronous, event-driven workloads SQS, Amazon Kinesis compute resources pull and consume messages, processing them in a distributed fashion      Stateful Components\n Can be scaled with session affinity Session Affinity  Bind all the transactions of a session to a specific compute resource Existing sessions do not directly benefit from the introduction of newly launched compute nodes      Instantiating Compute Resources  Bootstrapping  You can set up new EC2 instances with user data scripts and cloud-init directives You can use simple scripts and configuration management tools such as Chef or Puppet   Golden Images  Can be used to launch EC2 instances, Amazon RDS DB instances, and Amazon Elastic Block Store (Amazon EBS) volumes Results in faster start times and removes dependencies to configuration services or third-party repositories Important in auto-scaled environments to quickly and reliably launch additional resources as a response to demand changes.   Containers  Docker—an open-source technology that allows you to build and deploy distributed applications inside software containers. Launching from Docker image Amazon Elastic Container Service (Amazon ECS) and AWS Fargate Alternative container environment: Kubernetes and Amazon Elastic Container Service for Kubernetes (Amazon EKS)   Hybrid  Some parts are in a golden image, while others are configured dynamically through a bootstrapping action.    Infrastructure as Code  AWS CloudFormation templates give you an easy way to create and manage a collection of related AWS resources provision and update them in an orderly and predictable fashion CloudFormation templates can live with your application in your version control repository  Automation, Infrastructure Management, and Deployment  Serverless  AWS CodeBuild, and AWS CodeDeploy support the automation of the deployment of these processes   AWS Elastic Beanstalk:  You can use this service to deploy and scale web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. 17 Developers can simply upload their application code, and the service automatically handles all the details, such as resource provisioning, load balancing, auto scaling, and monitoring.   Amazon EC2 Recovery  Creating CloudWatch alarm that monitors EC2 instance and recover if impaired   AWS Systems Manager  You can automatically collect software inventory, apply OS patches, create a system image to configure Windows and Linux operating systems, and execute arbitrary commands.   Auto Scaling  You can maintain application availability and scale your Amazon EC2, Amazon DynamoDB, Amazon ECS, Amazon Elastic Container Service for Kubernetes (Amazon EKS) capacity up or down automatically according to the conditions you define    Alarms and Events  Amazon CloudWatch alarms Amazon CloudWatch Events AWS Lambda scheduled events AWS WAF security automation  Services, Not Servers  Loose Coupling Well-Defined Interfaces  Various components to interact with each other only through specific, technology-agnostic interfaces, such as RESTful APIs Can modify the underlying implementation without affecting other components   Amazon API Gateway  Fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale   Service Discovery  Because each service can be running across multiple compute resources, there needs to be a way for each service to be addressed EC2-hosted service, a simple way to achieve service discovery is through Elastic Load Balancing (ELB). Because each load balancer gets its own hostname, you can consume a service through a stable endpoint.   Asynchronous Integration  Another form of loose coupling between services One component generates events and another that consumes them SQS, Amazon Kinesis, cascading Lambda events, AWS Step Functions, or Amazon Simple Workflow Service Decouples components and introduces additional resiliency   Managed Services  Provide building blocks that developers can consume to power their applications   Serverless Architectures  Can reduce the operational complexity of running applications. It is possible to build both event-driven and synchronous services for mobile, web, analytics, CDN business logic, and IoT without managing any server infrastructure. These architectures can reduce costs because you don’t have to manage or pay for underutilized servers, or provision redundant infrastructure to implement high availability.    Databases  Anti-Patterns  If your application primarily indexes and queries data with no need for joins or complex transactions—especially if you expect a write throughput beyond the constraints of a single instance—consider a NoSQL database instead If your schema cannot be denormalized and the application requires joins or complex transactions, RDBS should be considered Large binary files should be stored in Amazon S3 with metadata in the database.   Databases  managed database services that offer enterprise performance at an open-source cost AWS offers different database technologies based on your workload Scalability  RDBS (Relational Databases) can scale up by upgrading to a larger instance and can scale horizontally by adding more read replicas Write capacity can be scaled horizontally by data partitioning or sharding. Data is split across multiple database schemas each running its own autonomous primary DB instance. RDS removes the operational overhead of running those instances, however, sharding introduces complexity in your application     Data Warehouse  combines transactional data from disparate sources to make them available for analysis and decision making Amazon Redshift is a managed data warehouse service providing a scalable, highly available and cost-effective solution.   Search  Searching enables datasets to be queried that are not precisely structured. AWS supports search services:  Amazon ElasticSearch (ES) Amazon CloudSearch     Graph Databases  Uses graph structures for queries The graph is defined as a consisting of edges (relationships), which directly relate to nodes (data entities) in the store. Relationships allow faster retrieval of complex hierarchical structures in relational systems.   Managing Increasing Volumes of Data  Data lake architecture    Removing Single Points of Failure  Introducing redundancy  Standby Redundancy\nWhen a resource fails, functionality is recovered on a secondary resource with the failover process. During the failover time, the resource remains unavailable. Active Redundancy\nRequests are distributed to multiple redundant compute resources. When one of them fails, the rest can simply absorb a larger share of the workload.   Detect Failure  You should aim at automatic failure detection and reacting to failure. ELB, Route53 with health checks, ASGs and other methods will help you automatically recover from the failure. Design Good Health Checks\nConfiguring the right health checks for your application helps determine your ability to respond correctly and promptly to a variety of failure scenarios. The health checks should reliably assess the health of the back-end nodes. Simple TCP check won\u0026rsquo;t detect the health state of a web server.   Durable Data Storage  Synchronous Replication\nThe transaction is acknowledged only after being durably stored in both the primary location and its replicas. This will protect the integrity of data in the event of failure. In this case, the primary node is coupled with the replicas. Asynchronous Replication\nDecouples the primary node from the replica, however, introduces replication lag - used to horizontally scale the system\u0026rsquo;s read capacity for queries that can tolerate the replication lag. Quorum-based replication\nCombines synchronous and asynchronous replication to overcome the challenges of large-scale distributed database systems. Replication to multiple nodes can be managed by defining the minimum number of nodes that must participate in a successful write operation. Examples:  Redis in AWS ElasticCache provide asynchronous communication - recent transactions can be lost in the event of a failover RDS with Multi-AZ provides synchronous replication to keep data on the standby node up-to-date with the primary.     Automated Multi-Data Center Resilience  Disaster Recovery Plan\n(Could consider failover to a distant second data center in the event of major disruption)  Low probability but huge impact risk   AZs provide a solution for short disruption, during which you    Fault Isolation and Traditional Horizontal Scaling  The measures above are insufficient if there is something harmful about the requests themselves. Same scenarios which caused the failure of the primary instances could be replayed to fail the failover instances Shuffle Sharding  Fault-isolating improvement Instances are grouped into shards Each customer will be distributed to a specific shard The impact is reduced in direct proportion with the number of shards The client could try every endpoint in a set of sharded resources, until one succeeds, making the client fault tolerant    Optimize for Cost  Right Sizing  Benchmarking may help in understanding the instance type and number of instances you require   Cost Optimization is an iterative process  Application and its usage will evolve over time   Elasticity  Autoscaling can help optimize the cost Automate turning off production workloads when not in use Replacing ec2 workloads with services   Take advantage of the variety of purchasing options  Reserved Instances Spot Instances    Caching  Application Data Caching  Amazon ElastiCache Amazon DynamoDB Accelerator (DAX)\nFully managed, highly available, in-memory cache for DynamoDB that delivers performance improvements from milliseconds to microseconds, for high throughput   Edge Caching  Static content cached at Amazon CloudFront edge location    Security  AWS WAF (Web Application Firewall) IAM  Granular set of policies for access control of users IAM roles can be assigned to instances to grant access to the resources   Data Encryption (in transit / at rest) AWS is responsible for the security of underlying cloud infrastructure You are responsible for securing the workloads you deploy to AWS Amazon Cognito  Allows client devices to access AWS resources through temporary tokens with fine-grained permissions   Security as Code  AWS CloudFormation   Real-Time Auditing  AWS Config AWS Inspector AWS Trusted Advisor AWS CloudTrail AWS CloudWatch Logs    "
},
{
	"uri": "https://majdarbash.github.io/aws/aws-global-network-reinvent/",
	"title": "AWS global network infrastructure (reInvent 2019)",
	"tags": [],
	"description": "",
	"content": " Network Aspects Nitro Network Architecture Building a scalable data center Availability Zones Transit Centers Physical Network Encryption AWS global network backbone Inside an Edge POP Summary  Network Aspects  Security Availability Scalability Performance Global Reach  Nitro Network Architecture  Nitro controller offloads in hardware lot of network features  ACL, Security Groups, VPC Peering, etc..   Gives consistent network performance VPC encryption - hardware accelerated encryption   Building a scalable data center  Networking building blocks  Make it easy to scale in right-sized segments Strong isolation boundaries Large amounts of network capacity   Networking technology  Routes  Single-chip routers  Constrained failure domain Fixed port types Many devices to manage Simpler forwarding architecture     Connectivity  Host Rack Networking  Partition placement groups  Ensures that all of instances in a partition group do not share any of underlying hardware with a resource in another partition group   Spread placement groups  Guarantee that each instance on the placement group is placed on distinct rack, with each rack having its own network and power source       Control plane  Using Single-chip-based platforms over large chassis-based platforms      Network Pattern\n Core cell  Provides external network connectivity   Spine cell  Interconnects placement groups   Access cell  Provides connectivity for underlying host rack based on the require number of uplinks   Host rack   Availability Zones  Failure isolation from other AZs Directly connects to other AZs One or more data centers Low-latency \u0026amp; close proximity Scalability  Transit Centers  Provide internet and inter-region (backbone) connectivity All AZs are connected redundantly Located in facilities with dense internet connection   Physical Network Encryption  Any link outside of AWS physical control, including between AWS data centers, and across AWS backbone is protected All traffic between AWS Regions (except China) is carried on the AWS backbone Most links protected with MACSEC or optical encryption using AES-256 Small number of short-distance links use laser monitoring  AWS global network backbone  AWS Direct Connect Internet Connectivity AWS Global Accelerator Region to Region communication AWS CloudFront to AWS services  Benefits of having global backbone\n Security  Traffic traverses AWS infrastructure rather than the internet   Availability  Controlling scaling and redundancy Traffic operates over Amazon-controlled infrastructure   Reliable Performance  Controlling specific paths customer traffic traverses   Connecting close to customers  Avoid internet hot spots or sub-optimal external connectivity    Building a global backbone network\n Latency Matters  Optimal in normalized situations Minimize additional latency during path failures   100G the new normal for backbone links Similar design patterns and operations to the data centers Extreme auditing of fiber paths  End-to-end latency Path hazards Repair expectations   Path diversity  Understanding shared risk link groups (SRLGs)   Capacity/Scale  Underlying optical transport capabilities    Inside an Edge POP  Multiple AWS Services  AWS Direct Connect  Low-latency access into AWS Access to all AWS regions Multiple customer-facing edge routes for redundancy Multiple Direct Connect locations for redundancy   Amazon CloudFront \u0026amp; Amazon Route53  At the Amazon global network perimeter Low-latency to external networks Origin fetches traverse the AWS network backbone IPv4 and IPv6 DNS anycast services   AWS Shield  Traffic scrubbing platforms to protect customers automatically Stopped at the internet edge before traffic reaches the backbone   AWS Global Accelerator   AWS global network access  Optimal interconnection with external networks AWS Region transit centers   External internet connectivity  Summary  Strong isolation from failures Extensive network monitoring and auto-remediation systems Large amounts of redundancy and over-provisioning Easily scalable at every layer Custom hardware and end-to-end control  "
},
{
	"uri": "https://majdarbash.github.io/aws/aws-migration-whitepaper/",
	"title": "AWS Migration Whitepaper (2018)",
	"tags": [],
	"description": "",
	"content": " The Cloud Adoption Framework (AWS CAF) Organization Change Management Business Drivers Migration Strategies  Choosing the Right Migration Strategy   Building a Business case for Migration People and Organization  Creating a Cloud Center of Excellence (CCoE)   Migration Readiness and Planning (MRP)  Assessing Migration Readiness Application Discovery Application Portfolio Analysis Migration Planning Technical Planning The Virtual Private Cloud Environment   Migrating  First Migrations - Build Experience Application Migration Process Team Models    The Cloud Adoption Framework (AWS CAF) The business plan should consider and incorporate the following areas:\n business people governance platform security operations  Organization Change Management  Organization change management framework helps drive the desired changes throughout your organization Accelerates cloud transformation   Mobilize Team Align Leaders Envision the Future Engage the Organization Enable Capacity Make it Stick  Mobilize Team, Align Leaders Objectives:\n Confirm sponsorship SEcure resource, expertise Form strong coalition of leaders Build momentum Key Actions: Form team to lead change - executive sponsors, stakeholders, line leaders, PMO, change management communication, training, etc. Establish program charter, roles, milestones Build guiding coalition, mobilize leadership Shape program governance structure Access and align change leadership roles  Envision the Future, Engage the Organization Objectives:\n Articulate vision and roadmap for transition to Cloud Mobilize organizations, build commitment, create change urgency Establish communication channels to gain and maintain buy-in, support and participation throughout entire transition Key Actions: Leaders communicate future Cloud vision (via comprehensive messaging plan) Impacted business leaders reinforce new op model (process/tech/org) Identify/assess impacted stakeholders Enlist and mobilize Change Champion Network Drive ongoing communication, feedback - two-way conversations Address \u0026ldquo;How does this impact me?\u0026rdquo; Celebrate progress and control issues  Enable Capacity, Make it Stick Objectives:\n Ensure successful transition to Cloud Align IT org structure, roles, processes with AWS Cloud Platform Ensure all IT staff/key stakeholders can operate in new environment Ensure Cloud benefits and objectives are achieved Key Actions: Identify change impacts to IT roles, policy, org structure, process, etc Modify IT roles, org structure, job descriptions and processes (if needed) to support AWS Cloud Align IT staff to new operating model Develop and implement targeted training Setup measurement structures Measure and evaluate outcomes Course correct where needed  Business Drivers  Operational Costs  Costs of running your infrastructure Matching supply and demand Investment risk for new applications IT operating model   Workforce Productivity  How efficiently you are able to get your services to market Workforce productivity improvements of 30%-50% following a large migration   Cost Avoidance  Setting up environment that does not create unnecessary costs Hardware refresh cost   Operational Resilience  Reducing organization\u0026rsquo;s risk profile and the cost of risk mitigation Improved performance Improved uptime and risk-related costs   Business Agility  Ability to react quickly to changing market conditions Speed, standardization, and flexibility develop when you use DevOps models, automation, monitoring, and auto-recovery or high-availability capabilities    Migration Strategies \u0026ldquo;6 R\u0026rdquo; Framework:\n Re-host Re-factor / Re-architect Re-purchase Retire Retain  Read More\n Choosing the Right Migration Strategy Factors:\n Business drivers for cloud adoption Time considerations Business and financial constraints Resource requirements  Building a Business case for Migration Questions:\n What is the future expected IT cost on AWS versus existing (base) cost? What are the estimated migration investment costs? What is the expected ROI, and when will hte project be cash flow positive? What are the business benefits beyond cost savings? How will using AWS improve our ability to respond to business changes?  Categories:\n Run cost analysis  TCO comparison of run costs on AWS post-migration vs. current operating model Impact of AWS purchasing/pricing options (RI, Volume discounts) Impacts of AWS discounts (Enterprise Discount Program, service credits, etc..)   Cost of change  Migration planning/consulting costs Compelling events (planned refresh, data center lease renewal, divestiture) Change management (training establishment of a Cloud Center of Excellence, governance, and operations model)   Labor productivity  Estimate of reduction in number of hours spent conducting legacy operational activities (requisitioning, racking, patching) Productivity gains from automation Developer productivity   Business value  Agility Cost avoidance Risk mitigation    People and Organization Establish operational processes and form a Cloud Center of Excellence (CCoE). CCoE is dedicated to mobilizing the appropriate resources and will lead your company through organizational and business transformations over the course of the migration effort.\nCreating a Cloud Center of Excellence (CCoE)  The CCoE structure will evolve and change as your organization transforms. Diverse, cross-functional representation is key. Treat the cloud as your product and the application team leaders as your customers. Drive enablement, not command and control. Build company culture into everything you do. Organizational change management is central to business transformation. Use intentional and targeted organizational change management to change company culture and norms. Embrace a change-as-normal mindset. Change of applications, IT systems, and business direct is expected. Operating model decisions will determine how people fill roles that achieve business outcomes.  Structure of CCoE\n CCoE is comprised of two functional groups:  Cloud Business Office (CBO)  Making sure that the cloud services meet hte needs of your internal customer IT should adopt a customer-centric model toward business application owners Organizational change management Stakeholder requirements Governance Cost Optimization Vendor management, internal marketing, communications and status updates to users   Cloud Engineering  Infrastructure automation Operational tools and processes Security tooling and controls Migration landing zones Performance, availability, security      Migration Readiness and Planning (MRP)  Set of tools, processes and best practices to prepare an enterprise for cloud migration Aligns to AWS Cloud Adoption Framework and is execution driven  Assessing Migration Readiness  Have you clearly defined the scope and the business case for the migration? Have you evaluated the environment and applications in scope through the lenses of AWS CAF? Is your VPC secure, can it act as a landing zone for all applications in scope? Have your operations and employee skills been reviewed and updated to accommodate the change? Do you have the experience necessary to move the tech-stacks that are in scope?  Migration Readiness Assessment (MRA):\n Identifies readiness gaps Makes recommendations to fill those gaps in preparation for a large migration effort To be completed interactively involving key stakeholders   Application Discovery  Understanding on-premises environment Determine what servers / devices exist Determine applications running on the devices Configuration Management Database (CMDB) helps with high-level analysis Recommend using automated discovery tool  Environment may change over time Plan how to keep your data current by continuously running automated discovery tool    Discovery Tools\n AWS Marketplace, Migration category AWS ADS - Application Discovery Service  Discovery server inventories and performance characteristics Uses an appliance connect for virtual servers Uses agents installed on physical or virtual hosts    Features of application discovery tool:\n Automatically discover infra and application running in your data center Maintain the inventory Reveal application / infrastructure dependencies Inventory versions of operating systems Determine performance baselines and optimization opportunities Provide means to categorize applications and servers  Application Portfolio Analysis  Broad categorization of resources aligned by common traits Identify special cases that need special handling Identify the order of migration and the migration strategy (based on 6 R\u0026rsquo;s)  Migration Planning  Review of project management methods, tools, and capabilities to assess any gaps Define project management methods and tools to be used during the migration Define and create the Migration Project Charter/Communication Plan including reports and escalation procedures Develop a project plan, risk/mitigation log, and roles and responsibilities matrix (e.g. RACI) to manage the risks that occur during the project and identify ownership for each resource involved Procure and deploy project management tools to support the delivery of the project Identify key resources and leads for each of the migration work streams Facilitate the coordination and activities outlined in the plan Outline resources, timelines, and cost to migrate the target environment to AWS  Technical Planning  Gather information about architecture of each application Get through each application before beginning execution of the plan Be agile: do a deep analysis of the first two to three prioritize apps, begin the migration  Continue deeper analysis of the next applications in parallel   Iterative process, with backlog of applications for each migration team  The Virtual Private Cloud Environment Security AWS CAF Security Perspective - structured approach to help build a foundation of security, risk, and compliance capabilities that will accelerate your readiness and planning for a migration project.\nAWS CAF 10 themes:\n Five core security themes  Identity and Access Management Detective Control Infrastructure Security Data Protection Incident Response   Five augmenting security themes (operational excellence through availability, automation and audit)  Resilience Compliance validation Secure continuous integration/continuous deployment (CI/CD) Configuration vulnerability analysis Security big data analytics    Operations\nDefines current operating procedures and identifies the process changes and training that is needed for successful cloud adoption. Determine Cloud Operation Model (COM) for a particular application or set of application when envisioning the future state.\n Service Monitoring Application Performance Monitoring Resource Inventory Management Release Management / Change Management Reporting and Analytics Business Continuity / Disaster Recovery IT Service Catalog  Create Cloud Services Organizations - to build an organization that is capable of delivering and consuming cloud services, which consists of:\n CCoE - Cloud Center of Excellence CBO - Cloud Business Office Cloud Shared Services teams COM - Cloud Operations Model  Platform\n Compute Provisioning Network Provisioning Storage Provisioning Database Provisioning Systems and Solution Architecture Application Development  Key elements of the platform work stream:\n AWS landing zone  Initial structure and pre-defined configurations for AWS accounts, networks, identity and billing frameworks and customer-selectable optional packages   Account structure Network structure Pre-defined identity and billing frameworks Pre-defined user-selectable packages  Migrating First Migrations - Build Experience  First migrated applications build confidence and experience Informs the migration plan (MRP) with the patterns and tool choices that fit your organization\u0026rsquo;s needs Provides validation and testing of the operational and security processes  Considerations:\n Identify patterns (architectures, tech stacks, etc..) in the portfolio to create a list of application groupings based on common patterns. This creates a common process for group migrations. First three to five applications should be representative of common patterns in your portfolio.  Application Migration Process  Discover  Discovering information about application  Discovery Business Information (DBI) Discovery Technical Information (DTI)     Design  Developing and documenting target state  AWS architecture Application Architecture Supporting operational components and processes   Includes information about data flow, foundational elements, monitoring design, and how the application will consume external resources   Build  Migration team is selected based on the migration strategy chosen for the application Using pre-defined methods and tools to migrate to AWS   Integrate  Migration team makes external connections for the application   Validate  Series of specific tests before cutover stage Verification, functional, performance, disaster recovery, and business continuity tests   Cutover  User acceptance test Use outlined rollback procedure in the cutover plan if the migration is not successful     Team Models  Cloud Business Office (Program Control)  Manages resources and budgets Manages and reports risk Drives communication and change management   Cloud Engineering \u0026amp; Operations  Builds and validates fundamental components Ensure development, test and prod env are scalable, automated, maintain and monitored Prepares landing zones as needed for migrations   Innovation  Repeatable solutions that will expedite migrations Larger and more complex technical issues for the migration teams   Portfolio Discovery \u0026amp; Planning  Accelerate downstream activities Executes application discovery and optimizing application backlogs Eliminate objections and minimize wasted effort    Migration Factory Teams\n Multiple teams operating concurrently on migration project Factory teams are self-sufficient and include five to six cross-functional roles  business analysts and owners, DevOps professionals, migration engineers, and developers   Migration team patterns:  Re-host migration team  High-volume, low-complexity applications that don\u0026rsquo;t require material change Leverages migration automation tools Patch and release management process   Re-platform migration team Re-factor/re-architect migration team    "
},
{
	"uri": "https://majdarbash.github.io/aws/aws-multiple-account-security/",
	"title": "AWS Multiple Account Security Strategy",
	"tags": [],
	"description": "",
	"content": "General Best Practices  Clearly define an AWS account-creation process  Who is creating an account? What is the account used for?   Define a company-wide AWS usage policy  Should include minimal security baseline requirements for the different ways they will use AWS Which services are approved for use What security or encryption features must be enabled   Create a security account structure for managing multiple accounts  Centralize security monitoring and management Manage identity and access Provide audit and compliance monitoring services   Leverage AWS APIs and scripts  Consistently apply baseline configurations across multiple AWS accounts Compliance-monitoring scripts    Implementation Considerations When to Create Multiple Accounts Multiple accounts provide highest level of security and resource isolation in AWS. Questions to consider when creating multiple accounts:\n Does the business require administrative isolation between workloads? Does the business require limited visibility and discoverability of workloads? Does the business require isolation to minimize blast radius? Does the business require strong isolation of recovery and/or auditing data?  When to Create a Security Account Structure  Do you want to manage AWS user identities in one account and federate access across to other accounts? Do you want to centrally store, secure, analyze, and report on AWS generated log data fro services such as AWS CloudTrail, AWS Config, Amazon S3, Amazon CloudFront, ELastic Load Balancing or Amazon VPC Flow Logs? Do you want to empower security and compliance organizations to apply security baselines and monitor security compliance across multiple AWS accounts? Do you want to centrally manage approved Amazon Elastic Compute Cloud (Amazon EC2), Amazon Machine Images (AMIs), or AWS Service Catalog portfolios and products?  AWS Security Account Structures Identity Account Structure\n Manage all users in a single account and enable user and group access to resources in other accounts IAM cross-account roles are used to grant access from one account to another AWS provides out-of-the-box federation capabilities from IAM, AWS Directory Service, or from existing identity stores using SAML 2.0  Logging Account Structure\n Accounts send logs and configuration information to a parent logging account Configure AWS CloudTrail and AWS Config to store configuration log data in S3 bucket owned by the parent account SCPs can be used to restrict member accounts from modifying AWS CloudTrail or AWS Config configuration settings EC2 instance can ship logs to a central account Configure CloudWatch Logs subscriptions and AWS Lambda to forward log data from one account to another Reduces the need to implement distributed log storage, protection, and analysis solution for each individual account  Publishing Account Structure\n Central management of pre-approved server images and AWS CloudFormation templates across a company Cross-account resource sharing to share AMIs and AWS Service Catalog portfolios created in a parent account  Hybrid AWS Security Account Structures Information Security Account\n Collecting and analyzing security-related data Running compliance scripts Configuring security services (CloudTrail and AWS Config) Manage IAM access across other AWS accounts Owned by InfoSec - Information Security department  Central IT Account\n Host identity repositories Federate user access Centrally manage shared AMIs, EBS snapshots and AWS Service Catalog portfolios Provide centralized DNS, logging, configuration management or software development services  "
},
{
	"uri": "https://majdarbash.github.io/random/aws-organizations/",
	"title": "AWS Organizations - Setting Up and Configuring User Accounts",
	"tags": ["aws", "devops", "solution architect"],
	"description": "",
	"content": " Setting Up Organizations on AWS Account Granting User Permissions to Access Resources Accessing Cross-Account Resources using AWS CLI Example  1. Creating AWS Accounts Hierarchy under Organization 2. Create Users and Groups in the Root Account 3. Managing Root Account User Permissions in the Child Account 4. Accessing Child Account resources using AWSCLI    Setting Up Organizations on AWS Account AWS Orgnizations provides you with an efficient way to better manage and organize your AWS accounts. You can create sub-accounts under your root account and invite existing AWS accounts to join your organization. Joining an organization account enables Consolidated Billing and better Access Management through Switch Role functionality.\nHere\u0026rsquo;s what you can do with AWS Organizations:\n Create Organization You can organize Organizations into Organization Units You can invite an existing account Enable root users to access child accounts using one set of credentials  For this, you have to:\n Use generic names and emails for root accounts Create the role with which the administrative user in root account can access the child account From now on, create users in root account and let them switch role   Granting User Permissions to Access Resources Though not compulsory, it\u0026rsquo;s recommended to create users only under the Root Account. By granting AssumeRole permissions to the users, you can allow them to assume certain roles in the child accounts.\n Create all Users in the root account Create role in each account with Principal of the parent account Give permissions to the users to assume the relevant role in the child account Create a role in the child accout which will allow Principal of the root account to access it Define the permissions allowed in the created Role within the Child Account   Accessing Cross-Account Resources using AWS CLI #/.aws/config: [profile developer.development] role_arn = arn:aws:iam::[Child-Account-ID]:role/[roleName] source_profile = [account-profile]  Example In this example we would like to create accounts with the following structure and grant users the correct permissions to each account.\nAWS Accounts:\n1. Creating AWS Accounts Hierarchy under Organization  Root (root@example.com)  Production (production@example.com) Development (development@example.com) Test (test@example.com)    While creating child accounts, AWS will by default create role named as OrganizationAccountAccessRole. This role created in the Child Account, will allow trusted entity being Root Account an AdministratorAccess to the Child Account. Meanwhile from the Root Account we can restrict who of our users is allowed to AssumeRole in the Child Account.\nTrusted Entities of OrganizationAccountAccessRole:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::[AWS-Root-Account-ID]:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } Following the same Analogy, we can create several roles in the Child Account with different permissions to the Resources in the child account. Then we can control which users from the Root Accountcan AssumeRole for which Role in the Child Account, defining User\u0026rsquo;s Cross-Account Resource permissions.\n2. Create Users and Groups in the Root Account Groups:\n Developers DevOps Testers Marketing  Allow Developers to assume \u0026ldquo;Developer\u0026rdquo; role in any of the Children Accounts. Add the following policy to the \u0026ldquo;Developers\u0026rdquo; group:\nDeveloper Group Policy\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::*:role/Developer\u0026quot; } } [profile developer.it-test-aws] role_arn = arn:aws:iam::257040594755:role/Developer source_profile = sac\nDo the same for DevOps, Testers or any other groups.\nDevOps Group Policy\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::*:role/DevOps\u0026quot; } } Tester Group Policy\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::*:role/Tester\u0026quot; } } Marketing Group Policy\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::*:role/Marketing\u0026quot; } } 3. Managing Root Account User Permissions in the Child Account At this stage, user of the Root Account can assume a role in the Child Account, using the \u0026ldquo;AssumeRole\u0026rdquo; permission granted in the policy above.\nNow it\u0026rsquo;s time to create these roles and grant permissions by assigning Policies to them. Each Child account in the current example will have the following roles:\n Developer DevOps Tester Marketing  Each of these roles will have the following Trusted Entities, based on what Root Account users can AssumeRole from the Child Account:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::[AWS-Root-Account-ID]:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, } ] } 4. Accessing Child Account resources using AWSCLI In order to access Child Account using the CLI, we need to configure a new profile for the AWS CLI where we would specify the assumed role and the source account credentials from the root account.\n# ~/.aws/config [profile root-account-profile] region = eu-west-1 output = json [profile developer.child-account-1] role_arn = arn:aws:iam::[Child-Account-Id]:role/Developer source_profile = root-account-profile # ~/.aws/credentials [root-account-profile] aws_access_key_id = [ROOT-Account-Key-Id] aws_secret_access_key = [ROOT-Account-Secret-Key]  Publish Date: 2019-11-24\n "
},
{
	"uri": "https://majdarbash.github.io/aws/aws-security-best-practices/",
	"title": "AWS Security Best Practices",
	"tags": [],
	"description": "",
	"content": " AWS Shared Responsibility Model  Infrastructure services Container services Abstracted services   AWS Trusted Advisor Define and categorize assets on AWS Design your ISMS to protect your assets on AWS Manage AWS Accounts, IAM Users, Groups, and Roles  Strategies for Using Multiple AWS Accounts Managing AWS Credentials Delegation Using IAM Roles and Temporary Security Credentials   Managing OS-level Access to Amazon EC2 Instances Secure Your Data  Resource Access Authorization Storing and Managing Encryption Keys in the Cloud Protecting Data at Rest Decommission Data and Media Securely Protect Data in Transit   Secure Your Operating Systems and Applications  Creating Custom AMIs Using Additional Application Security Practices   Secure Your Infrastructure  Using Secure Zoning and Network Segmentation Strengthening Network Security Securing Periphery Systems: User Repositories, DNS, NTP Building Threat Protection Layers Test Security Managing Metrics and Improvements Mitigating and Protecting Against DoS \u0026amp; DDoS Attacks   Manage Security Monitoring, Alerting, Audit Trail and Incident Response  Using Change Management Logs Protecting Log Information    AWS Shared Responsibility Model  IAM Service  Centrally manage users, security credentials such as passwords, access keys, and permission policies that control which AWS services and resources users can access   Regions, AZs and Endpoints  Network latency and regulatory compliance   Facilities Physical security of hardware Network infrastructure Virtualization infrastructure  Infrastructure services  EC2, EBS, ASG, VPC Services for building a cloud infrastructure similar to on-premise You control the operating system You configure and operate any identity management system that provides access to the user layer of the virtualization stack  AWS Manages:\n AWS Global Infrastructure:  Regions / Availability Zones / Edge Locations   Foundation Services  Compute / Storage / Databases / Networking    Customer Security Responsibilities:\n Customer Data Platform \u0026amp; Application Management Operating System, network \u0026amp; firewall configuration Client-side data encryption \u0026amp; data integrity authentication Server-side encryption (File system and/or data) Network traffic protection (encryption / integrity / identity) Data in transit / Data at rest Opacity layer between services from AWS and your platform, includes:  Data encryption Data integrity authentication Software-signing and data-signing Secure time-stamping    Container services  RDS, Elastic MapReduce, Elastic Beanstalk You don\u0026rsquo;t manage the underlying operating system You are responsible for managing platform-level identity and access management separate from IAM  AWS Manages:\n Platform \u0026amp; Application Management Operating System, network \u0026amp; firewall configuration AWS Global Infrastructure:  Regions / Availability Zones / Edge Locations   Foundation Services  Compute / Storage / Databases / Networking    Customer Security Responsibilities:\n Customer Data Client-side data encryption \u0026amp; data integrity authentication Network traffic protection (encryption / integrity / identity) Data in transit / Data at rest  Abstracted services  High-level storage, database, and messaging services, such as Amazon S3, Glacier, DynamoDB, SQS, SES You access the endpoints of the services using AWS APIs API manages underlying service components or the operating system on which they reside Abstracted services provide multi-tenant platform which isolates your data in secure fashion and provides for powerful integration with IAM  AWS Manages:\n Server side encryption provided by the platform Network traffic protection provided by the platform Platform \u0026amp; Application Management Operating System, network \u0026amp; firewall configuration AWS Global Infrastructure:  Regions / Availability Zones / Edge Locations   Foundation Services  Compute / Storage / Databases / Networking    Customer Security Responsibilities:\n Customer Data Client-side data encryption \u0026amp; data integrity authentication  AWS Trusted Advisor  Trusted advisor checks for compliance with security recommendations  Limited access to common administrative ports (SSH/22, Telnet/23, RDP/3389 and VNC/5500) Limited access to common database ports IAM is configured to help ensure secure access control of AWS resources MFA token is enabled to provide two-factor authentication for the root AWS account    Define and categorize assets on AWS Asset Categories:\n Essential elements, such as business information, process, and activities Components that support the essential elements, such as hardware, software, personnel, sites, and partner organizations  Assets can be quantified in financial terms such as:\n Negligible / Low / Medium / High / Very high  Design your ISMS to protect your assets on AWS Security requirements differ depending on:\n Business needs and objectives Processed employed Size and structure of the organization  Approach for building ISMS (Information Security Management System) on AWS: (ISO27001 framework can helpful with ISMS design and implementation)\n Define scope and boundaries  Which regions, AZs, instances, and AWS resources are \u0026ldquo;in scope\u0026rdquo;   Define an ISMS policy  Objectives that set the direction and principles for action regarding information security Legal, contractual, and regulatory requirements Risk management objectives for your organization How you will measure risk How management approves the plan   Select a risk assessment methodology  Business needs Information security requirements Information technology capabilities and use Legal requirements Regulatory responsibilities   Identify risks  Assets Threats to those assets Vulnerabilities that could be exploited by those threats Consequences if those vulnerabilities are exploited   Analyze and evaluation risks  Calculate the business impact, likelihood and probability, and risk levels   Address risks  Applying security controls, accepting risks, avoiding risk or transferring risks   Choose a security control framework  ISO 27002 NIST SP 800-53 COBIT (Control Objectives for Information and related Technology) CSA-CCM (Cloud Security Alliance-Cloud Control Matrix)   Get management approval  Approvals from management that acknowledge the residual risks Approvals for implementing and operating the ISMS   Statement of applicability  Which controls you choose and why Which controls are in place Which controls you plan to put in place Which controls you excluded and why    Risk Assessment Methodologies:\n OCTAVE (Operationally Critical Threat, Asset, and Vulnerability Evaluation) ISO 31000:2009 Risk Management ENISA (European Network and Information Security Agency) IRAM (Information Risk Analysis Methodology) NIST (National Institude of Standards \u0026amp; Technology) Special Publication 800-30 rev.1 Risk Management Guide  Manage AWS Accounts, IAM Users, Groups, and Roles  AWS account  Do not use root accounts for day-to-day interactions with AWS Choose several AWS accounts if required, e.g. account for each major department Create IAM users within each of the AWS accounts for the appropriate people and resources   IAM Users  IAM users can be a person, service or application that needs access to AWS resources Create individual IAM users for each individual that needs access Create fine grained permissions to resources under AWS, apply them to groups, and assign users to those groups    Strategies for Using Multiple AWS Accounts  Centralized Security Management  Single AWS account Centralize information security management and minimize overhead   Separation of production, development, and testing environments  Three AWS accounts Create an AWS account for each environment   Multiple autonomous departments  Multiple AWS accounts Create separate AWS accounts for each part of the organization Assign permissions and policies under each account   Centralized security management with multiple autonomous independent projects  Multiple AWS accounts Single AWS account for common resources, e.g. DNS, Active Directory, etc. Create separate AWS accounts per project Assign permissions and policies under each account    Consolidated billing can be used across accounts.\nManaging AWS Credentials Two types of sign-in credentials:\n Username/Password  Usernames for AWS accounts are always email addresses IAM user names allow more flexibility IAM user passwords can be forced to comply with a policy you define Multi-factor authentication (MFA)  Can be required on sign-in Can be activated for users to delete S3 objects     Access Keys  Access key id \u0026amp; secret MFA for API calls    Delegation Using IAM Roles and Temporary Security Credentials Use Cases:\n Applications running on Amazon EC2 instances that need to access AWS resources  Developers can distribute credentials to each instance and applications can then use those credentials to access resources Distributing long-term credentials is challenging to manage and a potential security risk IAM roles can be assigned to instances as a better alternative   Cross account access  Users from one account may need to access resources on another account   Identity federation  Users might have identities outside AWS, e.g. corporate directly Those users may need to work with AWS resources    Solution:\n IAM roles \u0026amp; Temporary security credentials  Defining a set of permissions to access the resources IAM users, mobile and EC2-based applications can programmatically assume a role Assuming a role returns temporary security credentials that the user or application can use for programmatic requests to AWS    Managing OS-level Access to Amazon EC2 Instances  AWS sets up initial instance access using SSH / RDP After first authentication, you might include authentication mechanisms X.509, Microsoft AD, local operating system accounts Linux Instances  AWS allows using EC2 key pair, or generating your own key pairs AWS does not store the private key cloud-init service appends the key to ~/.ssh/authorized_keys   Windows Instances  During AMI launch, ec2config service sets a new random Administrator password and encrypts it using EC2 key pair\u0026rsquo;s public key Password can be retrieved using AWS Console or command line tools, and by providing the corresponding Amazon EC2 private key to decrypt the password    Secure Your Data Resource Access Authorization Individual user\u0026rsquo;s effective permissions is the union of resource policies and the capability permissions granted directly or through group membership.\n Resource Policies  User creates resources and wants to control other users\u0026rsquo; access to the created resource   Capability Policies  Enforces company-wide access policies Can be assigned to a role Can override resource-based policy permissions by explicitly denying them    Storing and Managing Encryption Keys in the Cloud  Recommended to store keys in temper-proof storage, such as Hardware Security Modules (HSM) Amazon CloudHSM can be used to store keys Use CloudHSM for:  Database encryption Digital Rights Management (DRM) Pubic Key Infrastructure (PKI)  Authentication and authorization, document signing, and transaction processing     CloudHSM uses Luna SA HSMs from SafeNet Luna SA meets Federal Information Processing Standard (FIPS) 140-2 and Common Criteria EAL4+ standards You are responsible to manage cryptographic domain of CloudHSM  Cryptographic domain is a logical and physical security boundary that restricts access to your keys Clients on EC2 instances can be configured to allow applications to use APIs provided by CloudHSM CloudHSM can be multi-AZ with replication for HA and Storage Resilience    Protecting Data at Rest  Encrypt data on S3, EBS, RDS, etc.  Concerns related to protecting data at rest\n Accidental information disclosure  Limit number of users accessing confidential data Use IAM policies to limit access to resources Use encryption to protect confidential data Strategies  Permissions File, partition, volume or application-level encryption     Data integrity compromise  Use resource permissions to limit who can modify the data Perform data integrity checks. If you detect data compromise restore the data from backup / previous object version Strategies  Permissions Data integrity checks (MAC/HMAC/Digital Signatures/Authenticated Encryption) Backup Versioning (Amazon S3)     Accidental deletion  For Amazon S3 use MFA to require additional authentication to delete an object On compromise, restore the data from backup from a previous object version Strategies  Permissions Backup Versioning (Amazon S3) MFA Delete (Amazon S3)     System, infrastructure, hardware or software availability  In case if system failure restore your data from backup, or from replicas Will not be an issue for Multi-AZ enabled services Strategy  Backup Replication      Protecting Data at Rest on Amazon S3\n Permissions  Bucket-level / Object level permissions IAM policies   Versioning  Enable versioning to new version from which you can restore compromised objects   Replication  Replication is enabled with standard AZs   Backup  Data replication and backup as alternative to traditional backups If required, use application-level technologies to perform backups of S3 bucket   Encryption, server-side  AED-256 encryption   Encryption, client-side  Applications are responsible for encrypting / decrypting data on S3 You can use any encryption algorithm and data will be stored in encrypted format    Protecting Data at Rest on Amazon EBS\n Replication  2 copies of EBS volumes are created for redundancy in the same AZ   Backup  Snapshots capture data stored at a specific point in time   Encryption: Microsoft Windows EFS  Use EFS (Encrypted File System) on Microsoft Windows Server EFS provides transparent file and folder encryption EFS integrates with Active Directory key management facilities and PKI You can manage your own keys with EFS   Encryption: Microsoft / Windows Bitlocker  Volume encryption solutions supported from Windows Server 2008 upwards Uses 128-bit / 256-bit encryption   Encryption: Linux dm-crypt  Transparent data encryption on EBS volumes and swap space Can use Linux Unified Key Setup (LUKS) for key management   Encryption: Linux TrueCrypt  Third-party tool offering transparent encryption of data at rest on EBS Supports Windows and Linux OSs   Encryption and Integrity authentication: SafeNet ProtectV  Third-party offering, allows full disk encryption of volumes and pre-boot authentication of AMIs Provides data confidentiality and integrity authentication for data and the underlying OS    Protecting Data at Rest on Amazon RDS\n Use cryptographic functions  MySQL cryptographic functions include encryption, hashing and compression Oracle Transparent Data Encryption is supported on RDS for Oracle Enterprise Edition under the Bring Your Own License (BYOL) model: supports AES and Triple DES Microsoft Transact-SQL data protection functions include encryption, signing and hashing   Platform level encryption keys would be managed at application level SQL range queries are no longer applicable to the encrypted data Use one-way hashing functions to obfuscate personal identifiers  HMAC-SHA1 converts personal identifier into fixed-length hash value Personal identifier hashes are unique Collisions in commercial HMACs are extremely rare    Protecting Data at Rest on Amazon Glacier\n All data is protected using server-side encryption Unique encryption key is generated for each archive, uses AED-256 encryption Encryption key is encrypted with master key Master key is rotated on regular basis For additional protection, encrypt data before uploading to Amazon Glacier  Protection Data at Rest on Amazon DynamoDB\n All user data is fully encrypted at rest on DynamoDB Enhanced security encrypts all your data using encryptions keys stored in Amazon Key Management Services (AWS KMS) Additional encryption options can be implemented on application-level  Protection Data at Rest on Amazon EMR\n By default, EMR instances do not encrypt data at rest AMIs are provided by AWS and you cannot use custom AMIs AWS EMR clusters often uses S3 / DynamoDB as persistent data store Data is copied to HDFS (Hadoop Distributed File System) when cluster is launched Protection options:  Amazon S3 server-side encryption, no HDFS copy Amazon S3 client-side encryption Application-level encryption - entire file encrypted Application-level encryption - individual fields encrypted / structure preserved Hybrid - Combination of Amazon S3 server-side encryption and client-side encryption, and application-level encryption    Decommission Data and Media Securely  When deleting data storage blocks are marked as unallocated  the underlying physical media is not decommissioned   When block storage is provisioned, virtual machine manager (VMM) keeps track of which blocks the instance has written to When writing the block of storage, previous block is zeroed and overwritten with your block of data When reading, your previously stored data is returned  If the block has not previously written to, hypervisor zeros out the previous data on disk and returns a zero to the instance   When media reaches end of life, or on hardware fault, AWS destroys the data by following DoD (Department of Defense) or NIST (Guidelines for Media Sanitization) process For more controls of securely decommissioning data, data encryption at rest with customer managed keys can help you protect the decommissioned data  Delete the keys used to protect the decommissioned data, making it irrecoverable    Protect Data in Transit  Data in transit: communication over public links (e.g. Internet) Protect network traffic between clients and servers, and network traffic between servers  Concerns to communication over public links\n Accidental information disclosure  Data should be encrypted when traversing public network Solutions:  Use IPSec ESP and/or SSL/TLS     Data integrity compromise  Data integrity should not be compromised through deliberate or accidental Solutions:  Authenticate data integrity using IPSec ESP/AH, and/or SSL/TLS     Peer identity compromise / identity spoofing / man-in-the-middle  Import to authenticate the identity of the remote end Solutions:  Use IPSec with IKE with pre-shared keys X.509 certificates to authenticate remote end SSL/TLS with server certificate authentication based on CNAME or Alternative Name      Applications and Administrative Access to AWS Public Cloud Services\n HTTP/HTTPS traffic (web applications)  Use HTTPS instead of HTTP with server certificate authentication   HTTPS offload (web applications)  SSL/TLS processing requires additional CPU and memory resources from both the web server and the client Offload HTTPS processing on ELB Offload HTTPS processing on CloudFront   Remote Desktop Protocol (RDP) traffic  RDP connections use SSL/TLS connection by default Windows server should be issued a trusted X.509 certificate to protect from identity spoofing or man-in-the-middle attacks By default, RDP servers use self-signed certificates, which are not trusted, and should be avoided   Secure Shell (SSH) traffic  SSH tunneling can be used to run X-Windows on top of SSH and protect application session Use SSH version 2 using non-privileged user accounts   Database server traffic  Most databases support SSL/TLS wrappers RDS supports SSL/TLS in some cases    Protecting Data in Transit when Managing AWS Services\n AWS Management Console uses SSL/TLS to protect AWS service management traffic The client browser authenticates the identity of the console service endpoint using an X.509 certificate AWS REST APIs, accessible via SDKs or CLI also use SSL/TLS  Protecting Data in Transit to Amazon S3\n S3 is accessed over HTTPS SSL/TLS secure connection is established between client browser and console / endpoint All subsequent traffic is protected within this connection  Protecting Data in Transit to RDS\n Use SSL/TLS, when connecting to RDS over the Internet SSL/TLS provides peer authentication via server X.509 certificates, data integrity authentication and data encryption for the client-server connection SSL/TLS supported from RDS MySQL and Microsoft SQL instances Self-signed certificate can be downloaded and designated as trusted to prevent man-in-the-middle or identity-spoofing attacks Same self-signed certificate is used for all MySQL instances and another for all Microsoft SQL instances  Peer identity authentication does not provide individual instance authentication For individual server authentication via SSL/TLS, use EC2 and self-managed relational database services    Protecting Data in Transit to Amazon DynamoDB\n If connected from the same region, you can rely on AWS network security If connected across the Internet, use HTTPS  Protecting Data in Transit to Amazon EMR Communication Paths and Protection Approaches:\n Between Hadoop nodes  Communicate over TCP connections Are located in the same AZ and protected by security standards and physical infrastructure layer No additional protection is required   Between Hadoop Cluster and Amazon S3  EMR uses HTTPS for data between DDB and EC2   Between Hadoop Cluster and Amazon DynamoDB  EMR uses HTTPS for data between S3 and EC2   User or application access to Hadoop cluster  Can access EMR clusters using SSH scripts, REST or protocols such as Thrift or Avro Use SSL/TLS for Thrift, REST or Avro   Administrative access to Hadoop cluster  EMR administrators typically manage the cluster through SSH to the master node    Secure Your Operating Systems and Applications  Disable root API access keys and secret key Restrict access to instances from limited IP ranges using Security Groups Password protect the .pem file on user machines Delete keys from the authorized_keys file on your instances when someone leaves your organization or no longer requires access Rotate credentials (DB, Access Keys) Regularly run least privilege checks using IAM user Access Advisor and IAM user Last Used Access Keys Use bastion hosts to enforce control and visibility  System hardening standards include:\n Center for Internet Security (CIS) International Organization for StandardizatioN (ISO) SysADmin Audit Network Security (SANS) Institute National Institute of Standards Technology (NIST)  Creating Custom AMIs  You can create custom AMIs and publish for private / public use Published AMIs should meet your business needs and doesn\u0026rsquo;t violate the AWS Acceptable Use Policy  Clean-up Tasks before Publishing an AMI\n Disable insecure applications  Disable services and protocols that authenticate users in clear text over network, or otherwise insecurely   Minimize exposure  Disable non-essential services on startup   Protect credentials  Securely delete all AWS credentials from disk and configuration files Securely delete any third-party credentials from disk and configuration files Securely delete all additional certificates or key material from the system Ensure that software installed does not use default internal accounts and passwords   Use good governance  Ensure that the system does not violate the AWS Acceptable Use Policy (e.g. open SMTP relays or proxy servers)    Securing Linux/UNIX AMIs\n Secure services  In sshd set PublickeyAuthentication to Yes and PasswordAuthentication to No in sshd_config Generate unique SSH on instance creation. cloud-init handles this automatically.   Protect credentials  Remove and disable passwords for all user accounts so that they cannot be used to login and do not have a default password. Run passwd -l \u0026lt;username\u0026gt; for each account Securely delete all user SSH public and private key pairs   Protect data  Securely delete all shell history and system log files containing sensitive data    Securing Windows AMIs\n Protect credentials  Configure EC2 Config Service to randomly generate password for the Administrator account upon boot Make sure all user accounts have randomly generated passwords Ensure that the Guest account is disabled   Protect data  Clean the Windows event logs   Protect credentials  Make sure the AMI is not a part of a Windows domain   Minimizing exposure  Do not enable any file sharing, print spooler, RPC, and other Windows services that are not essential but are enabled by default    Bootstrapping\n Use Puppet, Chef, Capistrano, Cloud-Init and Cfn-Init Security software updates install the latest patches, service packs and critical updates beyond the patch level of the AMI Initial application patches install application level updates, beyond the current application level build as captured in the AMI Contextual data and configuration enables instances to apply configurations specific to the environment in which they are being launched-production, test or DMZ/internal, for example Register instances with remote security monitoring and management systems  Managing Patches\n You are responsible for patch management for your AMIs and live instances Implemented processes to identify new security vulnerabilities and assign risk rankings to them  Protecting Your Sustem from Malware\n Any software executed should be trusted If system has executed an untrusted program, the system on which it was executed can no longer be trusted Malicious code can change parts of the OS, install a rootkit, or establish back doors for accessing the system  Common approaches to Malware Protection\n Untrusted AMIs  Use trusted AMIs only Standard Windows and Linux AMIs provided by AWS and AMIs from trusted thrid parties   Untrusted Software  Only install software from a trusted software provider Trusted software providers often sign the software using code-signing certificates or MD5/SHA-1 signatures of their products   Untrusted software depots  Set up your own software depots of trusted software for you users to install and use Strongly discourage users from the dangerous practice of downloading and installing software from random sources on the internet   Principle of least privilege  Give users minimum privileges they need to carry out their tasks   Patching  Patch external-facing and internal systems to the latest security levels   Botnets  Infection might carry a code that creates a botnet - a network of infected hosts that can be controlled by a remote adversary   Spam  Infected systems can be used to send spam. AWS provides controls on how much email an EC2 instance can send. Avoid SMTP open relay.   Antivirus /Antispam software  Use reputable and up-to-date antivirus and antispam solution on your system   Host-based IDS software  Host-Based IDS software (e.g. open-source OSSEC software) Includes integrity checks and rootkit detection software Analyze important system files and folders and calculate checksum that reflect their trusted state    Mitigating Compromise and Abuse\n AWS works with you to detect and address suspicious and malicious activities from your AWS resources AWS uses the following mechanisms to detect abuse from customer resources:  AWS internal event monitoring External security intelligence against AWS network space Internet abuse complaints against AWS resources   Common causes of unintentional abuse activities:  Compromised resource - infected EC2 instance becoming a botnet agent Unintentional Abuse - web crawler can eb classified as DOS attacker by some Internet sites Secondary abuse - end user of application might post malware files on S3 bucket False complaints - internet users might mistake legitimate activities for abuse   AWS abuse warning issued when AWS detects an abuse Malicious, illegal or harmful activities that use your AWS resources may violate AWS Acceptable Use Polic Can lead to account suspension Best practices to respond to abuse incidents:  Never ignore AWS abuse communication  Reply to email to communicate with AWS abuse team and get help understanding the nature of the complaints Respond to abuse warnings, take action to stop the malicious activities, and prevent future re-occurrence   Follow security best practices  Consistently adopt simple defense practices Apply the latest software patches, restrict network traffic via a firewall and/or EC2 security groups Provide least privilege access to users   Mitigation to compromises  Consider any known compromise AWS resource unsafe Shutdown and rebuild the instance completely to get back to a safe state Fresh re-launch can be the first mitigation approach Carry out forensic analysis on a compromised instance to detect the root cause Isolate infected instance to prevent further damage and infection during the investigation Shutdown EBS volume and deliver it to security team to investigate Always backup key business data to be able to recover Review security control environment on the newly launched instances   Set up security communication email address  Choose a group email as a recipient for abuse-warning notifications Adjust from Personal Information page, under Configure Additional Contacts      Using Additional Application Security Practices  Always change vendor-supplied default before creating new AMIs or prior to deploying new applications, including passwords, SNMP community strings and security configuration Remove or disable unnecessary user accounts Implement a single primary function per EC2 - e.g. web server, db server, and DNS on separate servers Disable all non-essential services Disable or remove unnecessary functionality, scripts, drivers, features, EBS volumes and unnecessary web servers Use secure and encrypted protocols for communication over the plain text ones Introduce additional layers for plain text protocols, e.g. VPN Enforce security policies  Secure Your Infrastructure Amazon VPC provides isolation from other customers and Layer3 isolation from the Internet.\nOptions for protecting your applications in Amazon VPC:\n Internet-only  Encrypt application and administrative traffic using SSL/TLS or build VPN Plan routing and server placement in public and private subnets Use SGs and NACLs   IPSec over the Internet  Establish a private IPSec connection using IKEv1 and IPSec using standard AWS VPN facilities (AWS VPC VPN gateways, customer gateway and VPN connections) Establish customer-specific VPN software infrastructure in the cloud, and on-premises   AWS Direct Connect without IPSec  Using private peering links, without the Internet You might not need additional connection over private peering   AWS Direct Connect with IPSec  Use IPSec over AWS Direct Connect links for additional end-to-end protection   Hybrid  Use combination of approaches    Using Secure Zoning and Network Segmentation Build network segments using the following access control methods:\n Using Amazon VPC define an isolated network for each workload or organization entity Using SGs manage access to instances that have similar functions and security requirements Using NACLs allow stateless management of IP traffic Using host-based firewalls to control access to each instance Create a threat protection layer in traffic flow and enforce all traffic to traverse the zone Apply access control at other layers (applications and services)  Security zone requires additional controls per network segment, and they often include:\n Share Access Control - central Identity and Access Management system (IDAM) Shared Audit Logging - shared logging for event analysis and correlation, and tracking security events Shared Data Classification Shared Management Infrastructure - various components, anti-virus/antispam systems, patching systems and performance monitoring systems Shared Security (Confidentiality/Integrity Requirements) - considered in conjunction with data classification  Question to assess network segmentation and zoning requirements:\n Do I control inter-zone communication? Can I use network segmentation tools to manage communications between security zones A and B? Can I monitor inter-zone communication using an IDS/IPS/DLP/SIEM/NBAD system, depending on business requirements? Can I apply per zone access control rights? Can I manage each zone using dedicated management channel/roles? Role-Based Access Control for privileged access is a common requirement. You can use IAM to create different privilege levels. Can I apply per zone confidentiality and integrity rules? Per zone encryption, data classification, and DRM simply increase the overall security posture.  AWS features to build isolated security zones/segments on AWS per Amazon VPC access control:\n Per subnet access control Per security group access control Per instance access control (host-based) Per Amazon VPC routing block Per resource policies (S3/SNS/SMS) Per zone IAM policies Per zone log management Per zone IAM users, administrative users Per zone log feed Per zone administrative channels (roles, interfaces, management consoles) Per zone AMIs Per zone data storage resources (Amazon S3 buckets or Glacier archives) Per zone user directories Per zone applications/application control  Strengthening Network Security Best practices for AWS network security include:\n Always use SGs Augment security groups with Network ACLs Use IPSec or AWS Direct Connect for trusted connections to other sites Protect data in transit to insure the confidentiality and integrity of data Design network security in layers. Apply network security at external, DMZ, and internal layers. VPC Flow Logs provides further visibility.  Securing Periphery Systems: User Repositories, DNS, NTP  Use Amazon Route53 - Secure DNS service For Internal DNS - implement custom DNS on EC2 instance  Controls for Custom Infrastructure Components (e.g. DNS)\n Separate administrative level access Monitoring, alerting, audit trail NEtwork layer access control Latest stable software with security patches Continuous security testing (assessments) All other security control processes in place  PCI DSS (Payment Card Industry Data Security Standard) approach to time synchronization\n Verify that time-synchronization technology is implemented and kept current Obtain and review the process of acquiring, distributing and storing the correct time within the organization Verify that only designated time servers receive time signals from external sources and that time signals from external sources are based on International Atomic Time or Universal Coordinated Time (UTC) Verify that the designated central time servers peer with each other to keep accurate time, and other internal servers receive time only from central time servers Review system configurations and time-synchronization settings to verify that access to time data is restricted to only personnel who has a business need to access time data Review system configurations and time synchronization settings and processes to verify that any changes to time settings on critical systems are logged, monitored and reviewed Verify that the time servers accept time updates from specific, industry accepted external sources  Building Threat Protection Layers  Example of inline threat protection technologies:\n Third-party firewall devices installed on Amazon EC2 instances (soft blades) Unified threat management (UTM) gateways Intrusion prevention systems Data loss management gateways Anomaly detection gateways Advanced persistent threat detection gateways  Deploying threat protection layer technologies on AWS:\n Support for Multiple Layers of Load Balancers Support for Multiple IP Addresses Support for Multiple 0Elastic Network Interfaces (ENIs)  Alternatives for inline threat management layer:\n Distributed threat protection solution  Threat protection agents on individual instances in the cloud   Overlay network threat protection solution  Build an overlay network on top of Amazon VPC using GRE tunnels, vtun interfaces, or forwarding traffic on another ENI to centralized network traffic analysis and intrusion detection system, providing active or passive threat response    Test Security  External Vulnerability Assessment External Penetration Tests Internal Gray/White-box Review of Applications and Platforms  AWS Acceptable Use Policy\n outlines permitted and prohibited behavior on AWS cloud defines security violations and network abuse supports both inbound and outbound penetration testing in the cloud request a permission to conduct penetration tests  complete AWS Vulnerability Penetration Testing Request Form AWS policy does not permit testing of m1.small or t1.micro    Managing Metrics and Improvements  Monitoring and reviewing procedures and other controls Regular reviews of the effectiveness of the ISMS Measure controls effectiveness Risk assessments reviews at planned intervals Internal ISMS audits Regular management reviews Update security plans  Mitigating and Protecting Against DoS \u0026amp; DDoS Attacks  AWS Premium Support services allows you to proactively and reactively involve AWS support in the process of mitigating attacks or containing ongoing incidents in your environment on AWS DoS/DDoS on services sharing infrastructure (e.g. S3) will possibly affect multiple customers  AWS provides both mitigation and protection controls for DoS/DDoS on abstracted services from AWS   EC2 services use shared physical infrastructure  AWS does not provide mitigate or actively block network traffic affecting individual EC2 instances    Common approaches for DoS/DDoS mitigating and protection in the cloud\n Firewalls: SGs, NACLs, Host-based Firewalls Web Application Firewalls (WAF)  Deep packet inspection for web traffic Platform and application specific attacks Protocol sanity attacks Unauthorized user access   Host-based or inline IDS/IPS systems  All types of attacks   Traffic shaping/rate limiting  ICMP flooding Application request flooding   Embryonic session limits  Detect considerable deviations from the norm in the number of half-open (embryonic) TCP sessions and drop any further TCP SYN packets from specific sources TCP SYN flooding    Manage Security Monitoring, Alerting, Audit Trail and Incident Response Security monitoring questions\n What parameters should we measure? How should we measure them? What are the threshold for these parameters? How will escalation processes work? Where will data be kept?  What do I need to log?\n Actions taken by any individual with root or administrative privileges Access to all audit trails Invalid logical access attempts Use of identification and authentication mechanisms Initialization of audit logs Creation and deletion of system level objects  Log File Considerations\n Log collection  OS, application, third-party/middleware agents collect log file information   Log transport  Transfer logs to the central location in a secure, reliable, and timely fashion   Log storage  Centralize log files form multiple instances to facilitate retention policies, analysis, and correlation   Log taxonomy Log analysis/correlation Log protection/security  Privilege escalation gateway\n Centralize all access to the system via a single (clustered) gateway Automated password management for privileged access  Access control systems can rotate passwords and credentials based on given policies automatically using built-in connectors   Regularly run least privilege checks using AWS IAM and Access Advisor User authentication  Common approach is using token-based authentication for the website and acquiring click-through access to other systems allowed in the user\u0026rsquo;s profile   Tamper-proof audit trail storage of all critical activities Different sign-on credentials for shared accounts Restrict leapfrogging or remote desktop hopping by allowing access only to target systems Manage commands that can be used during sessions Provide audit trail for terminals and GUI-based sessions for compliance and security-related purposes Log everything and alert based on given threshold for the policies  Using Change Management Logs  Log all changes:  MACD - move/add/change/delete ad hoc changes unexpected changes, i.e. incidents infrastructure changes gold image/application inventory changes process and policy changes documentation changes logging faults, software or component failure  faults should generate alerts, and then you should use event analysis and correlation techniques to determine the cause of the fault and whether it should trigger a security response     Correlate and interconnect change management and log management systems Regular users should not have privileges to manage logs  Log Entry Information\n User identification information Type of event Date and timestamp Success or failure indication Origination of event Identity or name of affected data, system component, or resource  Protecting Log Information Common controls for protecting log information\n Verifying that audit trails are enabled and active for system components Ensuring that only individuals who have a job-related need can view audit trail files Confirming that current audit trail files are protected from unauthorized modifications via access control mechanisms, physical segregation and/or network segregation Ensuring that current audit trail files are promptly backed up to a centralized log server or media that is difficult to alter Verifying that logs for external-facing technologies are offloaded or copied onto a centralized internal log server or media Using file integrity monitoring or change detection software for logs by examining system settings and monitored files and results from monitoring activities Obtaining and examining security policies and procedures to verify they include procedures to review security logs at least daily and that follow-up to exceptions is required Verifying that regular log reviews are performed for all system components Ensuring that security policies and procedures include audit log retention policies and require audit log retention for a period of time, defined by the business and compliance requirements  "
},
{
	"uri": "https://majdarbash.github.io/tags/aws-advanced/",
	"title": "aws-advanced",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/aws-beginner/",
	"title": "aws-beginner",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/aws-best-practices/",
	"title": "aws-best-practices",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/backlinks/",
	"title": "backlinks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/backup/",
	"title": "backup",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/backup-and-recovery-whitepaper/",
	"title": "Backup and Recovery Approaches using AWS Whitepaper (2016)",
	"tags": ["aws", "whitepaper", "backup", "recovery"],
	"description": "",
	"content": " AWS Storage Services for Data Protection Designing a Backup and Recovery Solution Cloud-Native Infrastructure On-Premises to AWS Infrastructure Hybrid Environments Securing Backup Data in AWS  AWS Storage Services for Data Protection  Amazon S3  Single object limit of 5 TB Range of storage classes: standard, standard IA, glacier   Amazon Glacier  Extremely low-cost, cloud archive service Secure and durable storage for archiving and online backup Infrequently accessed data, retrieval times of several hours   AWS Storage Gateway  Connects on-premises software appliance with cloud-based storage   AWS Transfer Services  AWS Direct Connect, AWS Snowball, AWS Storage Gateway, Amazon S3 Transfer Acceleration to quickly transfer your data    Designing a Backup and Recovery Solution  Backup process should meet the RTO and RPO of the business, including:  File-level recovery Volume-level recovery Application-level recovery (e.g. databases) Image-level recovery    Cloud-Native Infrastructure  EBS Snapshot-Based Protection  EBS snapshots will be stored in Amazon S3, across multiple AZs First snapshot is a full copy of the volume Ongoing snapshots are incremental block-level changes only EBS snapshots can be copied between regions Consistent or Hot Backups  Best to have system in a state where it\u0026rsquo;s not performing any updates For backing up the database, put it into hot backup mode when possible For XFS filesystem, you can flash its data for a consistent backup, using xfs_freeze For file systems that don\u0026rsquo;t support the ability to freeze, unmount the volume, issue the snapshot command and remount the filesystem   Multivolume Backups  May require different considerations Data may be striped across multiple EBS volumes using a logical volume manager to increase potential throughput Snapshots should be initiated simultaneously for all volumes making up the RAID set  Snapshots should be tagged so that you can manage them collectively during a restore     Database Backup Approaches  For databases on EC2, you can use native tools for databases or create a snapshot of the volumes  AMIs can be used to be able to quickly restore the instance using aws ec2 create-image   For databases built on RAID, you can remove the burden of backups by creating a read replica of the database RDS fully automates backup and restore operations  Automated backups enable point-in-time recovery of your DB instance  Retention period of maximum 35 days for automated backup can be configured   DB snapshots are user-initiated backups that enable you to back up your DB instance and then restore to that state at any time        aws ec2 create-snapshot aws ec2 create-volume --region us-west-1b --snaposhot-id mysnapshot-id aws ec2 detach-volume --volume-id oldvolume-id --instance-id myec2instance-id aws ec2 attach-volume --volume-id newvolume-id --instance-id myec2instance-id --device /dev/sdf On-Premises to AWS Infrastructure  S3 and Glacier can be integrated with on-premise applications AWS Storage Gateway can be used if existing software does not natively support the AWS cloud Gateways act as plug-and-play devices providing standard iSCSI devices, which can be integrated into your backup or archive framework  More on Storage Gateways\u0026hellip;\nHybrid Environments  Mix of applications, some running on the cloud and others running on-premise AWS Direct Connect provides consistent latency to upload data to the cloud for the purposes of data protection and consistent performance and latency for hybrid workloads Combination of native tools, Storage Gateway, Direct Connect and VPN can be used to securely backup on-premise infrastructure to the cloud  Securing Backup Data in AWS  S3 supports encryption in transit and at rest All API endpoints are SSL encrypted through HTTPS Sever-side encryption can be chosen using KMS or SES  "
},
{
	"uri": "https://majdarbash.github.io/aws/best-practices-security-operations-reinvent/",
	"title": "Best Practices for Managing Security Operations on AWS (re:Invent 2017)",
	"tags": [],
	"description": "",
	"content": " Boundaries Consistent Controls Test often, fail early Closed-Loop Mechanisms Full Stack Practice Visibility  Netflix Usecase    Boundaries  In-VPC boundaries  SGs, NACLs AWS IAM Resource level constraints   VPC as the boundary (single account)  Equivalent to separate networks Peering, routing   AWS account as the boundary  Highest degree of segregation  By data classification Business unit Workload Functional      Considerations:\n How to aggregate logging, e.g. AWS account for logging SecOps may require a dedicated account  Consistent Controls  Setting up CodePipeline for CloudFormation templates  Allows static code analysis Allows TestDeployment in a spare environment   Consider Automating AWS account creation if required Provision stacks across AWS accounts using CloudFormation StackSets  Test often, fail early  Commit phase: source-control changes  Static code analysis: analyze the CFN templates against a set of security rules   Acceptance phase: dev environment  Dynamic analysis: run template in sandbox/acceptance test environment   Capacity/integration/staging phases: pre-prod environment  Load, performance, penetration and failover testing   Production phase: prod environment  Deploy    Closed-Loop Mechanisms  Control Monitor Fix  Spectrum of Options:  Possible use-cases:\n Control: API calls (CloudTrail) are logged  Monitor: StopTrail/Change Fix: Turn back on   Control: SSH only from bastion subnet  Monitor: Create/Change SGs, validate source port if port == 22 Fix: Change SG via Lambda   Control: All instances in patch up to date xxx  Monitor: EC2 Systems Manager + AWS Config rules Fix: Patch via Systems Manager   Control: No root access  Monitor: CloudWatch Logs + Syslog Fix: Isolate and investigate   Control: No public objects in S3  Monitor: Object level logging in CloudTrail Fix: Make object private    Full Stack  Establishing platform security Establishing network security Establishing OS security  AWS System Manager  Run command, State manager, Inventory, Maintenance Window, Patch Manager, Automation, Parameter Store, Documents     Establishing data protection  Rest: KMS, CloudHMS / Transit: VPN, ACM    Practice  S.I.R.S  Security Incident Response Simulations    Visibility Netflix Usecase  CloudTrail CloudWatch SDKs  Security Monkey\n Monitoring resources on AWS Respond to different security events on AWS  Awwwdit\n Creates a caching layer in front of API calls Caches results of the API describe calls and solves the rate limiting issue  "
},
{
	"uri": "https://majdarbash.github.io/tags/browser/",
	"title": "browser",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/caf/",
	"title": "caf",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/ci/",
	"title": "ci",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/clock/",
	"title": "clock",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/cloud-adoption-framework/",
	"title": "cloud-adoption-framework",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/commands/",
	"title": "commands",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/concept/",
	"title": "concept",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/continuous-integration/",
	"title": "continuous integration",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/crossdomain/",
	"title": "crossdomain",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/mysql-grant-privelleges/",
	"title": "CS: MySQL",
	"tags": ["mysql", "database", "terminal"],
	"description": "",
	"content": "Couple of useful sql queries we always search about for user management in mySQL:\n# creating a user in mysql CREATE USER '_username_'@'%' IDENTIFIED BY '_password_'; # give user select access on all tables in the database GRANT SELECT ON _database_.* TO '_username_'@'%'; # give user full access on all tables in the database GRANT ALL PRIVILLEGES ON _database_.* TO '_username_'@'%'; # grant special access to user for specific tables GRANT UPDATE ON _database_._tablename_ TO '_username_'@'%'; GRANT DROP, INSERT, DELETE, UPDATE ON _database_._table_ TO '_username_'@'%'; GRANT SELECT ON _database.table_ TO '_username_'@'%'; Determining the size of the database:\nSELECT table_schema \u0026quot;DB Name\u0026quot;, Round(Sum(data_length + index_length) / 1024 / 1024, 1) \u0026quot;DB Size in MB\u0026quot; FROM information_schema.tables GROUP BY table_schema; Dumping Data:\n# dumping specific database mysqldump [database] -u[username] -p[password] -h[hostname] \u0026gt; /tmp/database.sql # dumping certain tables mysqldump [database] --tables [table1] [table2] -u[username] -p[password] -h[host] \u0026gt; /tmp/database_table1_table2.sql # dumping based on query mysqldump [database] -e \u0026quot;[query]\u0026quot; -u[username] -p[password] -h[host] \u0026gt; /tmp/data.sql [query] - example: select * from users where id \u0026gt; 10 Other options for dumping: \u0026ndash;no-data \u0026ndash;skip-comments \u0026ndash;compact \u0026ndash;add-drop-database \u0026ndash;no-create-db\nPublish Date: 2014-09-10\n "
},
{
	"uri": "https://majdarbash.github.io/random/some-useful-commands-for-ubuntu-terminal/",
	"title": "CS: Ubuntu Commands",
	"tags": ["ubuntu", "terminal", "shell", "commands"],
	"description": "",
	"content": " # Checking the current version of installed ubuntu OS: lsb _release -a # checking disk space usage: df -h # checking disk space usage in a directory du -hs * # changing the password of the user: passwd [username ] # listing the cronjobs: crontab -l # editing cronjobs: crontab -e # tailing logs tail -f [filePath ] # tailing last 100 lines of the log file tail -f -n 100 [filePath ] # displaying contents of the file cat [filePath ] more [filePath ] less [filePath ] # showing only lines which have the string \u0026quot;testString\u0026quot; grep \u0026quot;testString\u0026quot; [file ] # grep with OR grep \u0026quot;pattern1 |pattern2\u0026quot; [file ] # extended grep (below does the same like grep with OR, however escaping characters is not required here) grep -e \u0026quot;pattern1|pattern2\u0026quot; [file ] # egrep (same like grep -e, which is extended grep) egrep 'pattern1|pattern2' [file ] # show location in terminal shell prompt (you can add this to ~/.bashrc) export PS1='$(whoami)@$(hostname):$(pwd)' Find where inodes are being used: # navigate to a specific directory cd / find . -printf \u0026quot;%h n\u0026quot; | cut -d/ -f-2 | sort | uniq -c | sort -rn FInd where the storage space is used # navigate to a specific directory cd / du -hs * mv /path/subfolder/{.,}* /path/ # copy files securely via ssh [remote -\u0026gt; local ] (-r for recursive is applicable, -P for port) scp -i test.pem ubuntu@hostname:/remote/location/path /destination/path # copy files securely [local -\u0026gt; remote ] scp -i test.pem /destination/path ubuntu@hostname:/remote/location/path TAR Compress tar -czvf /tmp/destination.tar.gz -C [directory ] [context _directory ] Example:\ntar -czvf /tmp/destination.tar.gz -C /tmp/where _to _compress . Decompress / Extract tar -xzvf /tmp/destination.tar.gz -C /tmp/where _to _extract . Move all files including hidden to parent directory mv /path/subfolder/{.,}* /path/ JSON Manipulations # converting json to ini string echo $json _var | jq -r \u0026quot;to _entries|map( \u0026quot; (.key)= (.value|tostring) \u0026quot;)|. [ ]\u0026quot; How to use screen command  Start a new screen  screen  Detach from the screen  Ctrl + a + d  Attach to the screen  screen -rd  List Screens  screen -ls  Open the next screen to the current  Ctrl + a + n  Publish Date: 2014-10-04\n Execute commands remotely using SSH If you are accessing your SSH using public key authentication:\nsudo ssh -i key.pem ubuntu@host \u0026lsquo;command to be executed remotely\u0026rsquo; ;\nIf you are accessing your SSH using username and password:\nsudo ssh -u root -p ubuntu@host \u0026lsquo;command to be executed remotely\u0026rsquo; ;\n"
},
{
	"uri": "https://majdarbash.github.io/tags/data-fixtures/",
	"title": "data fixtures",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/data-science/",
	"title": "data-science",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/database/",
	"title": "database",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/databases/",
	"title": "Databases",
	"tags": [],
	"description": "",
	"content": "Relational Database on AWS: (RDS) used for OLTP (Online Transaction Processing)  Supported Types  SQL Server Oracle MySQL Server PostgreSQL Aurora MariaDB   Run on virtual machines You cannot log in to these operating systems however Patching of the RDS Operating System and DB is Amazon\u0026rsquo;s responsibility RDS is NOT Serverless Aurora Serverless is Serverless  Read-Replicas vs Muti-AZ  Multi-AZ  For Disaster Recovery AWS handles cross-region replication AWS does an automatic failover of the instance Available for SQL Server, Oracle, MySQL Server, PostgreSQL, MariaDB You can force a failover from one AZ to another by rebooting the RDS instance.   Read Replicas  Improves Performance Up to 5 copies of Read-Replicas All read traffic can be going to read replica Available for MySQL Server, PostgreSQL, MariaDB, Aurora You can create read replicas of read replica as well, however, the latency will be greater then. Read replicas can be promoted to be their own databases - this will break the replication. Read replica can be in a second region. Automated backups should be turned on in order to be able to create read replicas.    RDS Backups\n Automated Backups  Allow you to recover your database to any point in time within a \u0026ldquo;retention period\u0026rdquo;. Retention period can be between 1 and 35 days. Enabled by default Backups stored in S3 Free storage space equal to the size of your database On recovery, RDS will choose the most recent daily backup (full snapshot) and then apply transaction logs relevant to the restore \u0026ldquo;point in time\u0026rdquo;. Backups are taken within the defined window. During the backup window, storage I/O may be suspended and latency may be elevated. Deleted when RDS instance is deleted   Snapshots  Done manually Stored even after deletion of RDS instance   Restoring snapshots or automated backups will create a new RDS instance with a new DNS endpoint.  Encryption:\n Encryption at rest is supported. Encryption is done using AWS Key Management Service (KMS). Automated backups, snapshots and read replicas of encrypted storage will be encrypted as well. Is available for all RDS supported types  DynamoDB (NoSQL database, Non-Relational Databases)  Terms:  Collection Document Key Value Pairs   Fast and flexible NoSQL database service Single-digit millisecond latency at any scale Fully managed database Great fit for mobile, web, gaming, ad-tech, IoT and many other applications Stored on SSD storage Spread across 3 geographically distinct data centers Eventual Consistent Reads (Default) Strongly Consistent Reads (1-second rule)  Allocated resources:\n Read Capacity Unit (for an item up to 4 KB)  One strongly consistent read per second Two eventual consistent reads per second   Write Capacity Unit (for an item up to 1KB)  One write per second    AWS Redshift Data Warehousing\n Business Intelligence Tools Pulling very large and complex data sets. Usually used by management to do queries on data OLTP vs OLAP\n(Online Transaction Processing vs Online Analytics Processing) Data Warehousing databases use a different type of architecture both from a database perspective and infrastructure layer.  Amazon Redshift (OLAP)\n Amazon\u0026rsquo;s Data Warehouse Solution Fast and powerful, fully managed, petabyte-scale data warehouse service in the cloud Configuration  Single Node (160GB) Multi-Node  Leader Node (manages client connections and receives queries) Compute Node (store data and perform queries and computations). Up to 128 Compute Nodes.     Advanced Compression  Columnar data stores can be compressed much more than row-based data stores because similar data is stored sequentially on disk. Does not require indexes or materialized views, so uses less space than traditional relational database systems. When loading data into an empty table, Amazon Redshift automatically samples your data and selects the most appropriate compression scheme.   Massive Parallel Processing (MPP)  Distributes data and query load across all nodes. Makes it easy to add odes to your data warehouse and enables you to maintain fast query performance as your data warehouse grows.   Backups  By default 1 day retention period, up to 35 days. Always attempts to maintain at least three copies of your data (the original and replica on the compute nodes and a backup in Amazon S3) Can also asynchronously replicate your snapshots to S3 in another region for disaster recovery.   Pricing  Compute Node Hours You will not be charged for leader node hours. Backup charges Data transfer (only within a VPC, not outside it)   Security Considerations  Encrypted in transit using SSL Encrypted at rest using AES-256 encryption Redshift takes care of key management  Manage your key through HSM (Hardware Security Module) AWS Key Management Service     Availability  Available in 1AZ Can restore snapshots to the new availability zone    Aurora  Amazon\u0026rsquo;s own proprietary database that is compatible with MySQL and PostgreSQL 2 copies of your data are contained in each AZ with a minimum of 3 AZs. 6 copies of your data. You can share Aurora Snapshots with other AWS accounts. 2 types of replicas available  Aurora Replicas MySQL replicas   Automated failover is only available with Aurora Replicas. You can do migrations from MySQL to Aurora only by creating an Aurora read replica and promoting it. Provides up to 5 times better performance than MySQL. Start with 10GB, Scales in 10GB increments to 64TB (Storage Autoscaling) Compute resources can scale up to 32vCPU and 244GB of Memory Designed to transparently handle the loss of up to two copies of data without affecting database write availability and up to three copies without affecting read availability. Automated backups are always enabled and they do not impact database performance. Taking snapshots does not impact performance.  ElasticCache Web service that makes it easy to deploy, operate and scale an in-memory cache in the cloud. Improves the performance of web applications. Retrieves information from fast, managed, in-memory caches.\n Memcached  Can be scaled horizontally   Redis  Multi-AZ You can do backups and restores of Redis    FAQs  By default customers are allowed to have up to a total of 40 Amazon RDS DB instances. Database limits per RDS instance are imposed based on software limitation. Maintenance windows - defines the time range when DB instance modifications, database engine version upgrades, and software patching occurs, in the event they are requested or required. Auto Minor Version Upgrade setting will automatically schedule the minor version upgrade in the next maintenance window. When a major version is deprecated in RDS a minimum 6 month period will be granted to upgrade to a supporter major version. Billing components include: DB instance hours, Storage, I/O requests per month, Provisioned IOPS per month, Backup Storage, Data transfer. Amazon RDS supported storage types are: Provisioned IOPS (SSD) Storage (OLTP workloads) and General Purpose (SSD) Storage (for moderate I/O requirements) RDS Automated Backups - allows point in time recovery using transaction logs. Snapshots - are user-initiated backups - backing up your DB instance. RDS master user account is a native database user account which you can use to connect to your DB Instance. You can encrypt connections between your application and the DB instance using SSL/TLS. Amazon RDS generates an SSL/TLS certificate for each DB Instance. Once an encrypted connection is established, data transferred between the DB Instance and your application will be encrypted during transfer. Amazon RDS supports encryption at rest for all database engines, using keys you manage using AWS Key Management Service (KMS). By default, Amazon RDS chooses the optimal configuration parameters for your DB Instance taking into account the instance class and storage capacity. When you create or modify your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different Availability Zone. When you run a DB instance as a Multi-AZ deployment, the “primary” serves database writes and reads. In addition, Amazon RDS provisions and maintains a “standby” behind the scenes, which is an up-to-date replica of the primary. The standby is “promoted” in failover scenarios. After failover, the standby becomes the primary and accepts your database operations. You may observe elevated latencies relative to a standard DB instance deployment in a single Availability Zone as a result of the synchronous data replication performed on your behalf. Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. RDS initiates failover in these events: Loss of availability in primary Availability Zone, Loss of network connectivity to primary, Compute unit failure on primary, Storage failure on primary. AWS will emit a DB instance event in case of failover. You can alsouse RDS Event Notifications to be notified of specific events. You can initiate a forced failover when rebooting your instance. Automatic Backups must be enabled on instance to be able to create read replicas. Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle allow you to create up to 5 read replicas for a given source DB instance. Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle allow you to enable Multi-AZ configuration on read replicas to support disaster recovery and minimize downtime from engine upgrades. Amazon Aurora, Amazon RDS for MySQL and MariaDB: You can create a second-tier read replica from an existing first-tier read replica. By creating a second-tier read replica, you may be able to move some of the replication load from the master database instance to a first-tier Read Replica. Please note that a second-tier Read Replica may lag further behind the master because of additional replication latency introduced as transactions are replicated from the master to the first tier replica and then to the second-tier replica.  "
},
{
	"uri": "https://majdarbash.github.io/tags/ddos/",
	"title": "ddos",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/deep-dive-on-aws-rds-reinvent/",
	"title": "Deep Dive on Amazon Relational Database Service (reInvent 2017)",
	"tags": [],
	"description": "",
	"content": " Why use Amazon RDS? Which instance type should I choose? Which storage type should I choose? How do I decide between GP2 and IO1? How do I scale my database instance? What happens during a Multi-AZ failover? Why would I use Read Replicas? How does Amazon RDS manage backups? When should I use Multi-AZ as opposed to Read Replicas? How do I secure my Amazon RDS database? How do I monitor my Amazon RDS database? How do you maintain my database? How am I charged for Amazon RDS? How do I understand my bill?  Why use Amazon RDS?  Lower TCO  Get more leverage from your teams Focus on the things that differentiate you   Built-in high availability and cross region replication across multiple data centers Even a small startup can leverage multiple data centers to design highly available apps with over 99.95% availability  Which instance type should I choose?  T2 Family  Burstable instances Moderate networking performance Good for smaller or variable workloads Monitor CPU credit metrics in Amazon CloudWatch T2.micro is eligible for free tier   M3/M4 Family  General-purpose instances High-performance networking Good for running CPU intensive workloads   R3/R4 Family  Memory-optimized instances High-performance networking Good for query intensive workloads or high connection counts    Which storage type should I choose?  General purpose (GP2)  SSD storage Maximum of 16 TB! Leverages Amazon EBS Elastic Volumes IOPS determined by volume size Minimum of 100 IOPS (below 33.33GiB) Bursts to 3,000 IOPS (applicable below 1.3 TB) Baseline of 10,000 IOPS (at 3.3 TB and above) Affordable performance   Provisioned IOPS (IO1)  SSD storage Maximum of 16 TB! Leverages Amazon EBS Elastic Volumes Maximum of 40K IOPS (20K on SQL Server) Delivers within 10% of the IOPS performance 99.9% of the time High performance and consistency   Magnetic  Magnetic storage Maximum of 1TB Supported for legacy databases    How do I decide between GP2 and IO1?  GP2 is a great choice, but be aware of burst credits on volumes \u0026lt; 1TB  Hitting credit-depletion results in IOPS drop - latency and queue depth metrics will spike until credits are replenished Monitor BurstBalance to see percent of burst-bucket I/O credits available Monitor read/write IOPS to see if average IOPS is greater than the baseline   Think of GP2 burst rate and PIOPS stated as maximum I/O rates  How do I scale my database instance?  Scale compute/memory vertically up or down  Handle higher load to grow over time Lower usage to control costs New host is attached to existing storage with minimal downtime   Scale up Amazon ECS storage (up to 16TB!)  Amazon ECS engines now support Elastic Volumes for fast scaling (now including SQL Server) No downtime for storage scaling Initial scaling operation may take longer, because storage is reconfigured on older instances Can re-provision IOPS on the fly    What happens during a Multi-AZ failover?  Each host manages set of Amazon EBS volumes with a full copy of the data Instances are monitored by an external observer to maintain consensus over quorum Failover initiated by automation or through the Amazon RDS API Redirection to the new primary instance is provided through DNS (watch for TTLs)  Why would I use Read Replicas?  Relieve pressure on your source database with additional read capacity Bring data close to your applications in different regions Promote a Read Replica to a master for faster recovery in the event of disaster Upgrade a Read Replica to a new engine version Supported for MySQL, MariaDB, and PostgreSQL  How does Amazon RDS manage backups?  Two options - automated backups and manual snapshots Backups leverage Amazon EBS snapshots stored in S3 Transaction logs are stored every 5 minutes in Amazon S3 to support point-in-time recovery (PITR) No performance penalty for backups Snapshots can be copied across regions or shared with other accounts  When to use Automated vs Manual backups?\n Automated  Specify backup retention window per instance (7-day default) Kept until outside of window (35-day maximum) or instance is deleted Support PITR Good for disaster recovery   Manual  Manually created through AWS console, AWS CLI, or Amazon RDS API Kept until you delete them Restores to saved snapshot Use for checkpoint before making large changes, non-production/test environments, final copy before deleting a database    Restoring Backups\n Restoring creates an entirely new database instance New volumes are hydrated from Amazon S3  While the volume is usable immediately, full performance requires the volume to warm up until fully instantiated Migrate to a DB instance class with high I/O capacity Maximize I/O during restore process    When should I use Multi-AZ as opposed to Read Replicas?  Multi-AZ  Synchronous replication - highly durable Only primary instance is active at any point in time Backups can be taken from secondary Always in two Availability Zones within a Region Database engine version upgrades happen on primary Automate failover when a problem is detected   Read Replicas  Asynchronous replication - highly scalable All replicas are active and can be used for read scaling No backups configured by default Can be within an AZ, cross-AZ or cross-region Database engine version upgrades independently from source instance Can be manually promoted to a standalone database    How do I secure my Amazon RDS database?  Designed to be secure by default: patches, updates, etc\u0026hellip; NEtwork isolation with VPC AWS IAM based resource-level permission controls Encryption at rest using AWS KMS (all engines) or Oracle/Microsoft TDE  No performance penalty for encryption data Encryption cannot be removed from DB instances If source is encrypted, Read Replicas must be encrypted Add encryption to an unencrypted DB instance by encryption a snapshot copy   Use SSL protection for data in transit Do not use AWS root credentials to manage RDS resources - create IAM user for everyone, including yourself Can use AWS Multi-Factor Authentication (MFA) to provide extra level of protection  How do I monitor my Amazon RDS database?  Amazon CloudWatch Metrics  CPU/Storage/Memory Swap Usage I/O (read and write) Latency Throughput Replica lag   Amazon CloudWatch Alarms Enhanced monitoring for RDS  Access to over 50 CPU, memory, file system and disk I/O metrics Low as 1-second intervals   Integration with third-party monitoring tools Amazon RDS Performance Insights  Measures DB Load - Average Active Sessions (AAS) Identifies database bottlenecks (TOP SQL) Identifies source of bottlenecks Enables problem discovery Adjustable time frame (hour, day, week and longer)   Subscribe to SNS notifications on events  How do you maintain my database?  Any maintenance that cases downtime will be scheduled in your maintenance window Operating system or Amazon RDS software patches are usually performed without restarting databases Database engine upgrades require downtime  Minor version upgrades - automate or manually applied Major version upgrades - manually applied Version deprecations - three to six-month notification before scheduled upgrades View upcoming maintenance events in your AWS Personal Health Dashboard    How am I charged for Amazon RDS?  Database instance (instance hours) Database storage (GB-mo) Backup storage  No charge for backup storage up to 100% of total database storage   Data transfer (GB-mo)  Uses AWS regional data-transfer pricing    How do I understand my bill?  Amazon RDS charges are grouped by region Instances are grouped by engine Storage and backup charges are cross-engine Use AWS Cost Explorer for graphical comparison Use the AWS Cost \u0026amp; Usage Report for billing details  Must be enabled for account Stored in your Amazon S3 bucket    Saving Money\n Use Reserved Instances Stop database when not in use  "
},
{
	"uri": "https://majdarbash.github.io/aws/deep-dive-on-aws-s3-reinvent/",
	"title": "Deep Dive on Amazon S3 &amp; Amazon Glacier Storage Management (reInvent 2017)",
	"tags": [],
	"description": "",
	"content": " Storage Management on S3  User Permission Management By Tagging S3 Inventory Storage Class Analysis Object-Level Logging Cross-Region Replication (CRR) Automate with Trigger-Based Workflow Amazon S3 event notifications Default Encryption Amazon Macie   AlertLogic Use Case on AWS S3  S3 Object Management Tags with Lifecycle Expiration Policies Tags with Lifecycle Transition Policies Demonstrate Scale of Storage Solution (AWS re:Invent 2017)    Storage Management on S3  Organize  Object Tagging   Monitor and Analyze  S3 Inventory Amazon CloudWatch Storage Class Analysis AWS CloudTrail   Act  Cross Region replications Event Notification Lifecycle Policy   Security Management  AWS KMS AWS IAM Bucket Permissions Check Encryption Status in S3 Inventory Default Encryption Trusted advisor Amazon Macie    User Permission Management By Tagging { \u0026#34;version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::Project-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: {\u0026#34;StringEquals\u0026#34;: {\u0026#34;s3:RequestObjectTag/Project\u0026#34;: \u0026#34;x\u0026#34;}} } ] } S3 Inventory  Generates a CSV / ORC file based of all objects in S3 bucket with respect to filter criteria. Triggers business workflows and applications such as secondary index, garbage collection, data auditing and offline analytics.  Features:\n Save time Daily or Weekly delivery Delivery notification Delivery to S3 bucket Same set of metadata as the LIST API Can add size, last modified date, storage class, etag or replication status Object-level Encryption Status Encrypt Inventory with SSE-S3 or SSE-KMS CSV or ORC output format Query with Athena, Redshift Spectrum or any Hive tools  S3 Inventory can be queried with Amazon Athena:\nCREATE EXTERNAL TABLE my_inventory_table( `bucket` string, `key` string, `version_id` string, `is_latest` boolean, `is_delete_marker` boolean, `size` bigint, `last_modified_date` timestamp, `e_tag` string, `storage_class` string, `is_multipart_uploaded` boolean, `replication_status` string, `encryption_status` string ) PARTITIONED BY (dt string) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.ql.io.orc.OrcSerde\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.SymLinkTextInputFormat\u0026#39; LOCATION \u0026#39;s3://bucketname/inventory/output_destination/hive\u0026#39; Storage Class Analysis  Data-driven storage management for S3 Daily Storage Class Analysis Export Analysis data to your S3 Bucket Filter by Bucket, Prefix, or Object Tags  Process:\n Monitors access patterns to understand your storage usage After 30 days, recommends when to move objects to Standard - Infrequent Access Export file includes a daily report of storage, retrieved bytes, and GETs by object age  Object-Level Logging  Allows Logging CloudTrail for Read / Write Events on the Objects  Cross-Region Replication (CRR) Use cases:\n Compliance Lower latency Security  Features:\n Ownership overwrite for cross-account CRR Support SSE-KMS Encrypted objects Choose any S3 Storage Class as target Choose any AWS region as target Bi-directional replication Lifecycle Policy  Automate with Trigger-Based Workflow Amazon S3 event notifications  Notifications when objects are created via Put, Post, Copy, Multipart Upload, or Delete Filter on prefixes and suffixes Trigger workflow with Amazon SNS, Amazon SQS, and Amazon Lambda functions  Default Encryption  Automatically encrypts all objects written to your Amazon S3 bucket Choose SSE-S3 or SSE-KMS Makes it easy to satisfy compliance needs  Amazon Macie  Security service that uses machine learning to automatically discover, classify and protect sensitive data in AWS Recognizes sensitive data Continously monitors data access Provides dashboards and alerts  AlertLogic Use Case on AWS S3 S3 Object Management  S3 Object Keys use hash prefix for performance: logmsgs-001:/X-OGA/11543.2016-03/... S3 Objects written with two Tags  Customer identitfier (cid=1234567890) Date (date=2017-06)   AWS KMS used to generate data encryptionkeys  Customer Master Key (CMK) for each data type with automatic rotation enabeld Data Keys generated per-customer/per-month    Tags with Lifecycle Expiration Policies  Per Customer Expiration Rule Uses cid and date tags as filter Indepdendent of object create time  \u0026lt;Rule\u0026gt; \u0026lt;ID\u0026gt;expiration-12345\u0026lt;/ID\u0026gt; \u0026lt;Status\u0026gt;Enabled\u0026lt;/Status\u0026gt; \u0026lt;Filter\u0026gt; \u0026lt;And\u0026gt; \u0026lt;Tag\u0026gt; \u0026lt;Name\u0026gt;cid\u0026lt;/Name\u0026gt; \u0026lt;Value\u0026gt;12345\u0026lt;/Value\u0026gt; \u0026lt;/Tag\u0026gt; \u0026lt;Tag\u0026gt; \u0026lt;Name\u0026gt;date\u0026lt;/Name\u0026gt; \u0026lt;Value\u0026gt;2015-09\u0026lt;/Value\u0026gt; \u0026lt;/Tag\u0026gt; \u0026lt;/And\u0026gt; \u0026lt;/Filter\u0026gt; \u0026lt;Expiration\u0026gt; \u0026lt;!-- Depends entirely on the tag values --\u0026gt; \u0026lt;Days\u0026gt;0\u0026lt;/Days\u0026gt; \u0026lt;/Expiration\u0026gt; \u0026lt;/Rule\u0026gt; Tags with Lifecycle Transition Policies  One Transition Rule per month Uses date tag as filter  \u0026lt;Rule\u0026gt; \u0026lt;ID\u0026gt;transition-ia-3months\u0026lt;/ID\u0026gt; \u0026lt;Status\u0026gt;Enabled\u0026lt;/Status\u0026gt; \u0026lt;Filter\u0026gt; \u0026lt;And\u0026gt; \u0026lt;Tag\u0026gt; \u0026lt;Name\u0026gt;date\u0026lt;/Name\u0026gt; \u0026lt;Value\u0026gt;2016-07\u0026lt;/Value\u0026gt; \u0026lt;/Tag\u0026gt; \u0026lt;/And\u0026gt; \u0026lt;/Filter\u0026gt; \u0026lt;Transition\u0026gt; \u0026lt;StorageClass\u0026gt;STANDARD_IA\u0026lt;/StorageClass\u0026gt; \u0026lt;/Transition\u0026gt; \u0026lt;/Rule\u0026gt; Demonstrate Scale of Storage Solution (AWS re:Invent 2017)  Scaled wrokload 100x successfully  140PB/month of customer data 30k writes/second sustained Write latency 200ms at 95th percentile Read latency 125ms at 95th percentile   Limited only by resources driving traffic  "
},
{
	"uri": "https://majdarbash.github.io/software-architecture-and-design/design-patterns/",
	"title": "Design Patterns",
	"tags": [],
	"description": "",
	"content": " Factory Adapter Active Record Observer Decorator Repository Service Locator Singleton Strategy Facade  Factory In this pattern, the Factory class is responsible for instantiating the defined models. If in future you would like to exchange the model or change the way the model is instantiated\nExample:\nclass Book { private $isbn; private $title; private $author;\npublic function \\_\\_construct($isbn, $title, $author) { $this-\u0026gt;isbn = $isbn; $this-\u0026gt;title = $title; $this-\u0026gt;author = $author; }  }\nclass BookFactory { public static function create($isbn, $title, $author) { return new Book($isbn, $title, $author); } }\nAdapter The adapter pattern adjusts the interface of one class to match that of another.\nDrawback:\n You\u0026rsquo;re just hiding a bad design  class NotificationAdapter { protected $username = \u0026lsquo;'; protected $password = \u0026lsquo;';\npublic function __construct($username, $password) { $this-\u0026gt;username = $username; $this-\u0026gt;password = $password; }\npublic function send($to, $from, $body, $subject = \u0026lsquo;') { if (\u0026rsquo;\u0026rsquo; == $subject) { return $this-\u0026gt;sendSMS($to, $from, $body); } else { return $this-\u0026gt;sendEmail($to, from, $body, $subject); } }\nprotected function sendSMS($to, $from, $body) { echo \u0026lsquo;Sending SMS Implementation \u0026hellip;'; }\nprotected function sendEmail($to, $from, $body, $subject) { echo \u0026lsquo;Sending Email Implementation \u0026hellip;'; } }\n$notificationAdapter = new NotificationAdapter();\n$notificationAdapter-\u0026gt;send(\u0026lsquo;test@test.com\u0026rsquo;, \u0026lsquo;me@test.com\u0026rsquo;, \u0026lsquo;hello!', \u0026lsquo;test message\u0026rsquo;);\nActive Record Database table or view will be mapped into an object.\nDrawbacks:\n* objects are tightly coupled to the database schema\n* objects are tightly coupled to the database itself\nExample:\nclass User { protected $connection = null;\npublic function __construct() { $this-\u0026gt;connection = new PDO(\u0026ldquo;mysql:host=localhost;dbname=development\u0026rdquo;, \u0026lsquo;root\u0026rsquo;, \u0026lsquo;root\u0026rsquo;); }\npublic function load($id) { $sql = \u0026lsquo;SELECT * FROM users WHERE user_id=\u0026rsquo; . (int)$id; $result = $this-\u0026gt;connection-\u0026gt;query($sql); $row = $result-\u0026gt;fetch(PDO::FETCH_ASSOC);\n foreach ($row as $column =\u0026gt; $value) { $this-\u0026gt;column = $value; }  } }\n$user = new User(); $user-\u0026gt;load(2);\nprint_r($user);\nObserver In this design pattern one object (subject) notifies its dependent objects (observers) about any state changes.\nDecorator Decorator allows you to add specific behaviors to the instances of a class instead of attaching them to the object itself.\nDrawbacks:\n testing can be hard  Repository Provides a more object-oriented view of the persistence layer. It encapsulates the set of objects persisted in a data store and the operations performed over them. Supports the objective of clean separation between the domain and data mapping layers.\nExample:\nclass Order { /** * @var int */ private $id; /** * @var string */ private $name; /** * @var DateTime */ private $date;\npublic function __construct(int $id, string $name, DateTime $date) { $this-\u0026gt;id = $id; $this-\u0026gt;name = $name; $this-\u0026gt;date = $date; }\npublic static function fromState(array $state): Order { return new self($state[\u0026lsquo;id\u0026rsquo;], $state[\u0026lsquo;name\u0026rsquo;], $state[\u0026lsquo;date\u0026rsquo;]); }\n/** * @return int */ public function getId(): int { return $this-\u0026gt;id; }\n/** * @return string */ public function getName(): string { return $this-\u0026gt;name; }\n/** * @return DateTime */ public function getDate(): DateTime { return $this-\u0026gt;date; } }\ninterface Persistence { public function generateId();\npublic function persist(array $data);\npublic function retrieve(int $id);\npublic function delete(int $id); }\nclass MemoryPersistence implements Persistence {\nprivate $data = []; public $lastId = 0;\npublic function generateId() { $this-\u0026gt;lastId++; return $this-\u0026gt;lastId; }\npublic function persist(array $data) { $this-\u0026gt;data[$this-\u0026gt;lastId] = $data; }\npublic function retrieve(int $id) { if (!isset($data[$id])) { throw new OutOfBoundsException(\u0026lsquo;Cannot find the requested record!'); } return $this-\u0026gt;data[$id]; }\npublic function delete(int $id) { if (!isset($data[$id])) { throw new OutOfBoundsException(\u0026lsquo;Cannot find the requested record!'); } unset($this-\u0026gt;data[$id]); }\n}\nclass OrderRepository { /** * @var Persistence */ private $persistence;\n/** * OrderRepository constructor. */ public function __construct(Persistence $persistence) { $this-\u0026gt;persistence = $persistence; }\npublic function generateId(): integer { return $this-\u0026gt;persistence-\u0026gt;generateId(); }\npublic function findById(integer $id) { $arrayData = $this-\u0026gt;persistence-\u0026gt;retrieve($id); return Order::fromState($arrayData); }\npublic function save(Order $order) { $this-\u0026gt;persistence-\u0026gt;persist([ \u0026lsquo;id\u0026rsquo; =\u0026gt; $order-\u0026gt;getId(), \u0026lsquo;name\u0026rsquo; =\u0026gt; $order-\u0026gt;getName(), \u0026lsquo;date\u0026rsquo; =\u0026gt; $order-\u0026gt;getDate(), ]); } }\n$orderRepository = new OrderRepository(new MemoryPersistence());\n$order = Order::fromState([ \u0026lsquo;id'=\u0026gt;12, \u0026lsquo;name'=\u0026gt;'Flowers Order\u0026rsquo;, \u0026lsquo;date'=\u0026gt;new DateTime() ]);\n$orderRepository-\u0026gt;save($order); $orderInPersistence = $orderRepository-\u0026gt;findById(12);\nprint_r($order); print_r($orderInPersistence);\nService Locator This is considered as an anti-pattern.\nWith service locators services are usually defined globally throughout the app and are instantiated to be used as dependency wherever required. According to symfony docs: \u0026ldquo;Service locators are a design pattern that encapsulates the processes involved in obtaining a service using a central registry known as service locator\u0026rdquo;.\nSingleton Singleton pattern means allowing access to a single instance of defined class. For example, Mailer class could be reused several times throughout your application to send emails. It doesn\u0026rsquo;t have to be instantiated several times and you should be able to access the same instance once you need to use the Mailer class. The same applies to the database class. You would like to use existing database connection instead of constructing and creating a spare connection to your database every time you want to execute a query.\nThough singleton pattern guarantee that class has only one instance and can be easily accessed a lot of developers consider it as anti-pattern. Singleton pattern may introduce global state in application, unnecessary restrictions and make your code easier to maintain and reuse. Writing tests can be also more complicated.\nStrategy Another name for this pattern is Policy pattern. In this pattern families of Algorithms are grouped together. This pattern is useful in particularly when several algorithms can perform the same operation and we would like the application to pick up the appropriate algorithm based on specific attributes.\nInterfaces serve by providing contracts which must be obeyed by any new concrete implementation.\nExample:\nIn this example there are 2 ways to ship an item: cargo or freight. Based on user\u0026rsquo;s choice of shipping type the order price will vary.\ninterface ShippingStrategy { public function getPrice(); }\nclass ShipByFreight implements ShippingStrategy { public function getPrice() { return 1000; } }\nclass ShipByCargo implements ShippingStrategy { public function getPrice() { return 2000; } }\nclass Order {\nprivate $itemCode; private $itemName; private $shippingType; /\\*\\* \\* Order constructor. \\*/ public function \\_\\_construct($itemCode, $itemName, $shippingType) { $this-\u0026gt;itemCode = $itemCode; $this-\u0026gt;itemName = $itemName; $this-\u0026gt;shippingType = $shippingType; } public function getPrice() { if ($this-\u0026gt;shippingType == 'cargo') { $shipping = new ShipByCargo(); } elseif ($this-\u0026gt;shippingType == 'freight') { $shipping = new ShipByFreight(); } else { throw new Exception('Undefined shipping type: ' . $this-\u0026gt;shippingType); } return $shipping-\u0026gt;getPrice(); }  }\n$order = new Order(\u0026lsquo;10029NA\u0026rsquo;, \u0026lsquo;iPhone X\u0026rsquo;, \u0026lsquo;cargo\u0026rsquo;);\necho $order-\u0026gt;getPrice();\nFacade This pattern belongs to the structural patterns. It specifies how the code should be structured to be organized and maintainable.\nRepetitive code throughout the system can be contained in facade and called with proper arguments to perform specific action.\nExample:\nclass FriendshipFacade { private $notificationService; private $userService; private $friendshipService;\npublic function __construct( NotificationService $notificationService, UserService $userService, FriendshipService $friendshipService ) { $this-\u0026gt;notificationService = $notificationService; $this-\u0026gt;userService = $userService; $this-\u0026gt;friendshipService = $friendshipService; }\npublic function add($userId1, $userId2) { $user1 = $this-\u0026gt;userService-\u0026gt;get($userId1); $user2 = $this-\u0026gt;userService-\u0026gt;get($userId2);\n $this-\u0026gt;friendshipService-\u0026gt;add($user1, $user2); $this-\u0026gt;notificationService-\u0026gt;notify($user1, 'You have a new friend: ' . $user2-\u0026gt;name); $this-\u0026gt;notificationService-\u0026gt;notify($user2, 'You have a new friend: ' . $user1-\u0026gt;name);  } }\nclass UserController { public function addFriend() { $friendshipFacade = new FriendshipFacade(); $friendshipFacade-\u0026gt;add($userId1, $userId2); } }\n"
},
{
	"uri": "https://majdarbash.github.io/tags/design-patterns/",
	"title": "design patterns",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/devops/",
	"title": "devops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/doctrine/",
	"title": "doctrine",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/ec2/",
	"title": "EC2",
	"tags": [],
	"description": "",
	"content": " EC2 Instances  Security Groups   EBS Volumes  5 Types of EBS Storage   AMI\u0026rsquo;s CloudWatch AWS CLI IAM Roles Instance Metadata EFS (Elastic File System) FAQ Notes  Overview  Compute-Optimized Instances General-Purpose Instances High Memory Instances Previous Generation Instances Memory Optimized Instances Storage Optimized instances   Storage Networking and Security Management Billing Platform    Amazon EC2 is a web service that provides resizable compute capacity in the cloud. Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change.\nEC2 Pricing Models   On Demand\nAllows you to pay a fixed rate by the hour with no commitment.\n  no up-front payment required\n  applications with short term, spiky or unpredictable workloads that cannot be interrupted\n  applications being developed or tested on Amazon EC2 for the first time\n  Reserved\nProvides you with a capacity reservation and offer a significant discount on the hourly charge for an instance. Contract Terms are 1 Year or 3 Year Terms.\n applications with steady state or predictable usage applications that require reserved capacity users able to make upfront payments to reduce their total computing costs even further Types of Reserved Pricing  Standard Reserved Instances\nUp to 75% off on-demand instances. The more you pay up front and the longer the contract, the great the discount. Convertible Reserved Instances\nThese offer up to 54% off on demand capability to change the attributes of the Reserved Instance as long as the exchange results in the creation of Reserved Instances of equal or greater value. Scheduled Reserved Instances      Spot\nEnables you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flexible start and end times.\n Applications that have flexible start and end times Applications that are only feasible at very low compute prices Users with urgent computing needs for large amounts of additional capacity If the Spot instance is terminated by EC2 you will not be charged for a partial hour of usage. However, if you terminate the instance yourself, you will be charged for any hour in which instance ran.    Dedicated Hosts Pricing\n Regulatory requirements that may not support multi-tenant virtualization Great for licensing which does not support multi-tenancy or cloud deployments Can be purchased On-Demand (hourly.)    EC2 Instance Types  F1 - FPGA, Field Programmable Gate Array I3 - IOPS, High Speed Storage G3 - Graphics, Graphics Intensive H1 - High Disk Throughput T3 - Cheap general purpose (think T2 Micro) D2 - Density, Dense Storage R5 - RAM, Memory Optimized M5 - Main choice for general purpose apps C5 - Compute, Compute optimized P3 - Graphics (think Pics), Graphics / General Purpose GPU X1 - Extreme Memory, Memory Optimized Z1D - Extreme Memory and CPU, High compute capacity and a high memory footprint A1 - Arm-based workloads U-6tb1 - Bare Metal  EC2 Instances  Termination protection is turned off by default On an EBS-backed instance, the default action is for the root EBS volume to be deleted when the instance is terminated EBS Root Volumes of the default AMI\u0026rsquo;s cannot be encrypted. You can only use a third-party tool to encrypt the root volume. Additional volumes can be encrypted upon launching an instance.  Security Groups   All Inbound traffic is blocked by default\n  All Outbound traffic is allowed\n  If you add / remove a security group to EC2 instance or modify the rules in security group, this change will take place immediately.\n  You can have any number of EC2 instances within a security group\n  You can have multiple security groups attached to EC2 Instances\n  Security Groups are STATEFUL\nIf you create an inbound rule allowing traffic in, that traffic is automatically allowed back out again\n  You cannot block specific IP addresses using Security Groups, instead, use Network Access Control Lists\n  You can specifically allow rules, but not deny rules.\n  Status Checks\n System Status Checks Instance Status Checks    Detailed Monitoring\n  EBS Volumes 5 Types of EBS Storage   General Purpose (SSD)\n API Name: gp2 Up to 16000 IOPS Applications: Most Workloads Storage: 1GB - 16TB    Provisioned IOPS (SSD)\n API Name: io1 Up to 64000 IOPS Highest performance SSD volume designed for mission-critical applications Applications: Databases Storage: 4GB-16TB    Throughput Optimized (HDD)\n API Name: st1 Up to 500 IOPS Low-cost HDD volume for frequently accessed throughput intensive workloads Applications: Big Data \u0026amp; Data Warehouses Storage: 500GB - 16TB    Cold Hard Disk Drive (HDD)\n API Name: sc1 Up to 250 IOPS Lowest cost HDD volume designed for less frequently accessed workloads Applications: File Servers Storage: 500GB - 16TB    Magnetic (HDD)\n API Name: Standard 40-200 IOPS Previous generation HDD Applications: Workloads where data is infrequently accessed Storage: 1GB - 1TB    By default, upon EC2 instance termination:\n root EBS volume will be automatically terminated additional volumes attached to the instance will continue to persist    SSD is good for random access, HDD is better for sequential access\n  EBS Encryption\n Root device cannot be encrypted on the first launch  To encrypt the root volume:  Launch the instance with the root volume Create a snapshot of unencrypted root device volume Create a copy of the Snapshot and select the encrypt option Create an AMI from the encrypted Snapshot Use AMI to launch new encrypted instances\n(only specific instance types will be supported)     Secondary volumes can be encrypted Snapshots of encrypted volumes are encrypted automatically Volume restored from encrypted snapshots are encrypted automatically You can share snapshots, but only if they are unencrypted These snapshots can be shared with other AWS accounts or made public    Volumes exist on EBS\n  Snapshots exist on S3\n  Snapshots are a point in time copies of Volumes\n  Snapshots are incremental - this means that only the locks that have changed since your last snapshot are moved to S3\n  If you take snapshot for the first time it may take some time\n  You can take a snapshot while instance is running, however it\u0026rsquo;s better to stop the instance when taking snapshot from the root Volume to assure consistency\n  You can create AMI\u0026rsquo;s from both Volumes and Snapshots\n  EBS volume sizes can be changed on the fly\n  Volumes will ALWAYS be in the same availability zone as the EC2 instance\n  Moving EC2 volume from one AZ to another\n take snapshot of the volume create AMI from the snapshot launch EC2 instance in a new AZ using the AMI    Moving EC2 volume from one region to another\n take a snapshot of the volume create AMI from the snapshot copy AMI from one region to another use AMI to launch the new EC2 instance in the new region    AMI\u0026rsquo;s AMI can be selected based on:\n Region Operating System Architecture (32-bit / 64-bit) Launch Permissions Storage for the Root Device (Root Device Volume)  Instance Store (Ephemeral Storage)  The root device for an instance launched from the AMI is an instance store volume created from a template stored in Amazon S3 Instance store volumes will not be shown in volumes section Instance store instances cannot be stopped: they can only be terminated or rebooted If the underlying host fails, you will loose your data Cannot keep root volume upon instance termination Data is lost when the instance is restarted These instances are free, you will get charged for the instance usage Instance size is determined by the instance type Temporary block-type storage Disks are physically attached to the hardware (SSD / HDD) These volumes are available to specific instance types   EBS Backed Volumes  Persistent block storage volume The root device for an instance launched from the AMI is an Amazon EBS volume created from an Amazon EBS snapshot Can be attached to any EC2 instance type Can be scaled up and down based on your requirements      CloudWatch  CloudWatch monitors performance CloudWatch monitors most of AWS as well as your applications that run on AWS CloudWatch with EC2 will monitor events every 5 minutes by default You can have 1-minute intervals by turning on Detailed Monitoring You can create CloudWatch alarms with trigger notifications Host Level Metrics  CPU Network Disk Status Check   Alarms\nAllows you to set Alarms that notify you when particular thresholds are hit Dashboards\nCreates awesome dashboards to see what is happening in your AWS environment Logs\nHelps you aggregate, monitor and store your log data Events\nHelp you to respond to state changes in your AWS resources  AWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls.\nAWS CLI  You can interact with AWS from anywhere in the world just by using the command line (CLI) You will need to set up access in IAM  IAM Roles  Roles are more secure than storing your access key and secret access key on individual EC2 instances Roles are easier to manage Roles can be assigned to an EC2 instance after it is created using both the console \u0026amp; command line Roles are universal - you can use them in any region  Instance Metadata  Metadata is used to get information about an instance (such as public ip) Examples  Show the bootstrap script\ncurl http://169.254.169.254/latest/user-data Show the options available\ncurl http://169.254.169.254/latest/meta-data Show the private and public v4 IPs\ncurl http://169.254.169.254/latest/meta-data/local-ipv4\ncurl http://169.254.169.254/latest/meta-data/public-ipv4    EFS (Elastic File System)  File storage service for Amazon EC2 instances Storage capacity is elastic, growing and shrinking automatically as you add and remove files It\u0026rsquo;s a great way to share files among EC2 instances Can be mounted to thousands of instances at the same time Supports the Network File System version 4 (NFSv4) protocol EFS can be mounted using  EFS mount helper EFS mount helper with TLS NFS   You only pay for the storage used Can scale up to petabytes Can support thousands of concurrent NFS connections Data is stored across multiple AZ\u0026rsquo;s within a region Read After Write Consistency  FAQ Notes Overview  Longer EC2, EBS and Storage Gateway resource IDs allow uninterrupted creation of new resources The speed of launching an instance depends on the number of factors, including the size of your AMI, the number of instances you are launching, and how recently you have launched that AMI. Images launched for the first time may take slightly longer to boot. Instance Limits: you are limited to running a total of 20 On-Demand instances across the instance family, purchasing 20 Reserved Instances and requesting spot instances based on your dynamic Spot limit per region. There are limitations of the number of emails that can be sent out from EC2. These limits can be removed when filling the request. EC2 supports a variety of operating systems including Amazon Linux, Ubuntu, Windows Server, Red Hat Enterprise Linux, SUSE Linux Enterprise Server, Fedora, Debian, CentOS, Gentoo Linux, Oracle Linux, and FreeBSD. All AWS underlying hardware in EC2 uses ECC memory (Error-correcting code memory) Service Level Agreement (SLA)  Guarantees a Monthly Uptime Percentage of at least 99.99% for EC2 and EBS within a Region SLA credit will be provided if the region you are operating in has a Monthly Uptime Percentage of less than 99.95% during any monthly billing cycle.   Accelerated Computing Instances\nUse hardware accelerators or co-processors to perform some functions, such as floating point number calculation and graphics processing, more efficiently than is possible in software running on CPUs. EC2 provides 3 types of accelerated computing instances: GPU compute instances, GPU graphics instances, FPGA programmable hardware compute instances. GPU Instances  workloads with massive parallelism good for Graphics processing good for applications where the throughput of a pipeline is more important than the latency of the individual operations   G3 Instances  G3 provided a high-performance platform for applications using DirectX or OpenGL G3 instances support DirectX 12, OpenGL 4.5, CUDA 8, and OpenCL 1.2   P3 Instances  P3 are the next-generation of EC2 general-purpose GPU computing instances P3 instances support CUDA 9 and OpenCL P3 have new features like Streaming Multiprocessor (SM) architecture for machine learning (ML)/deep learning (DL) performance optimization, second-generation NVIDIA NVLink high-speed GPU interconnect and highly tuned HBM2 memory for higher-efficiency NVIDIA Tesla V100 accelerator adds features that improve programmability - advances will supercharge HPC, data center, supercomputer, and deep learning systems and applications. P3 Instances benefit Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL) and High-Performance Computing (HPC) applications. Users includes data scientists, data architects, data analysts, scientific researchers, ML engineers, IT managers and software developers Use-cases for P3 instances: autonomous vehicle platforms, speech, image and text recognition systems, intelligent video analytics, molecular simulations, drug discovery, disease diagnosis, weather forecasting, big data analytics, financial modeling, robotics, factory automation, real-time language translation, online search optimizations, and personalized user recommendations P3 Instances will support VPC only.   P2 instances  P2 instances support CUDA 8 and OpenCL 1.2   NVIDIA drivers for P3 and G3 instances  Can be obtained from AWS Marketplace by using AMIs with the pre-installed drivers Alternatively, you can use HVM AMIs and install the drivers yourself   EC2 F1 Instance  The instance with programmable hardware that can be used for application acceleration High performance, easy to access FPGA for developing and deploying custom hardware accelerations   Amazon FPGA Image (AFI) - the design that you create to program your FPGA  Compute-Optimized Instances  C5 Instances offer a 25% price/performance benefit over C4 instances  General-Purpose Instances  A1 Instances  Using Graviton Processors ARM ecosystem Applications based on interpreted or run-time compiled languages should run without modifications. Other applications may require to be recompiled to run on ARM instead of x86. arm64 AMIs should be used x86 AMIs are not compatible EBS volumes are supported, through Non-Volatile Memory Express (NVMe) interface. Blkfront interface is not supported Supports ENA based Enhanced Networking - up to 10Gbps of network bandwidth Support AWS Nitro System - a combination of dedicated hardware and Nitro hypervisor   M5 Instances  Good choice for running development and test environments, web, mobile and gaming applications, analytics applications Provides instances with a higher memory footprint Customers using M4 Instances should move to M5 Delivers up to 14% improvement in price/performance compared to M4 EBS Volumes is the only storage supported Supports ENA Enhanced Networking with up to 25 Gbps of network bandwidth when launched within a placement group Up to 27 EBS Volumes can be used Uses lightweight Nitro Hypervisor, based on core KVM technology   Fixed performance instances are C, M and R Burstable performance instance (T2)  Bursting CPU Credits CPU Credit Balance and other related metrics can be monitored from CloudWatch When running low on credits t2 instance provides a baseline CPU performance of 10% of a physical CPU core. The stopped instance does not retain the previously earned credit balance    High Memory Instances  6 TB, 9 TB, or 12 TB of memory in a single instance Useful for running large in-memory databases Uses ENA-based Enhanced Networking Delivers high networking throughput and low-latency with 25 Gbps bandwidth Certified by SAP for running Business Suite on HANA Instance Types: u-6tb1.metal, u-9tb1.metal and u-12tb1.metal 448 logical processors, a total of 224 CPU cores EBS Volumes supported, EBS-optimized instances by default These instances are bare metal instances, they do not run on a hypervisorAva Available on EC2 Dedicated Hosts on a 3-year Reservation You can launch, stop/start, and terminate instances on your EC2 Dedicated Hosts using AWS CLI/SDK.  Previous Generation Instances  M1, C1, CC2 and HS1  Memory Optimized Instances  Large memory size for memory intensive applications X1 instances are ideal for in-memory databases X1e instances offer twice the memory per vCPU compared to the X1 instances X1e instances - 32 GB of memory per vCPU, from 4 vCPUs to 128 vCPUs X1 instances - 65 GB of memory per vCPU Intel E7 processors SSD based instance store and EBS (Instances are EBS-Optimized by default)  Storage Optimized instances  Dense-storage instance - for workloads with high sequential read and write access to very large data sets HDD-storage instances d2.8xlarge - up to 3.5 GBps read and 3.1 GBps write disk throughput with a 2MiB block size, h1.16xlarge - up to 1.15 GBps read and write. H1 and D2 instances are EBS-optimized by default  Storage  Attaching multiple instances to one volume is not supported at this time Though EBS snapshots reside in S3, they cannot be accessed using S3 API, and can be accessed using EC2 APIs Snapshots can be done while volume is attached an in use You can find the snapshots shared with you by using the \u0026ldquo;Private Snapshots\u0026rdquo;, while public using \u0026ldquo;Public Snapshots\u0026rdquo; from the viewing dropdown in the console  Networking and Security  Elastic Fabric Adapter (EFA) - uses a custom-built operating system bypass technique to enhance the performance of inter-instance communications which is critical to scaling HPC applications. All accounts are limited to 5 Elastic IP addresses per region. You will be charged a small hourly rate for each unassigned IP address. Enhanced networking provides significantly improved performance, consistency of performance and scalability.  can be enabled when launching an HVM AMI with the appropriate drivers    Management  Amazon CloudWatch receives and aggregates data at 1-minute intervals CloudWatch metrics for terminated resources are available for 2 weeks Instance Hibernation  useful when the instance takes a long time to bootstrap can be considered as pre-warmed instances RAM data is persisted to the root EBS volume during the hibernation You do not incur instance usage fees while an instance is hibernating Hibernation should be enabled when launching an instance Hibernated instances are in ‘Stopped’ state EBS volume data is persisted as in hibernate state as in stopped state RAM data is encrypted when moved to EBS Instances can be hibernated for a maximum of 60 days Hibernation is supported for specific instance types   VM Import/Export  Enables Importing Virtual Machine (VM) images in order to create Amazon EC2 instances You can export EC2 instances as well to create Virtual Machines Import / Export commands are not available in the management console. You have to use EC2 CLI and API    Billing  Billing starts when the instance transitions to the Running state Reserved instance is associated with a spefic region, which is fixed for the duration of the reservation\u0026rsquo;s term EC2 Fleet lets you provision compute capacity across difference instance types, AZs and On-Demand, Reserved and Spot Instances with a single API call Reserved Instance Marketplace - provides AWS customers the flexibility to sell their EC2 RIs to other businesses and organizations. Spot Instances  spare EC2 capacity can save you up to 90% off on-Demand prices AWS can interrupt them with a 2-minute notification    Platform  Amazon Time Sync Service - available on 169.254.169.123. NTP clients can be configured Cluster Compute Instances - provide similar functionality to other Amazon EC2 instances but have been specifically engineered to provide high-performance networking High Memory Cluster Instances provide customers with large amounts of memory and CPU capabilities per instance in addition to high network capabilities EC2 Compute Unit  represents CPU resource is used to provide a consistent amount of CPU capacity no matter what the actual underlying hardware    "
},
{
	"uri": "https://majdarbash.github.io/tags/encore/",
	"title": "encore",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/error/",
	"title": "error",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/event-emitter/",
	"title": "event emitter",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/event-driven/",
	"title": "event-driven",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/event-driven-architectures/",
	"title": "Event-Driven Architecture",
	"tags": [],
	"description": "",
	"content": "Structures:\n Command Bus  Command Command Handler   Event Store Event Bus  Event Event Handler    Event-Driven Systems:\n Event Notification\nWill help decouple received from the sender, however it makes it difficult to understand overall behavior of the system. Event-carried State Transfer\nDecouples and reduces load from supplier by each system keeping a copy of the data it requires in its storage. Due to the fact that data is replicated, higher availability will lead to Eventual Consistency. Event Sourcing\nApplication state is can be reproduced through replaying the Event log. Starting from customer\u0026rsquo;s input, all events are stored in the Event Store and can be replayed at later stage. In addition to Auditing, this allows easy Debugging through replaying the events and monitoring system\u0026rsquo;s behavior. Snapshots will accelerate the application state restoration process as only the Events after snapshot date will have to be replayed.\nIn this aspect we can benefit from Memory Image: application state does not need to be stored on persistent storage and can stored in-memory.\nEvent sourcing does not mean Asynchronous event handling, as you can still use it in Monolith applications. Application versioning adds extra complexity to handle the replay functionality of past Events. CQRS (Command Query Responsibility Segregation)\nSeparating writing systems from the reading systems. In this context Command Model (write) and Query Model (read) are separate components. You can scale read and write components independently and have a different representation of application data in each of the systems. However CQRS adds an extra complexity so should be used in caution where applicable.  Publish Date: 2019-06-16\n "
},
{
	"uri": "https://majdarbash.github.io/tags/filter/",
	"title": "filter",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/fixtures/",
	"title": "fixtures",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/flash-messages/",
	"title": "flash messages",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/for-loop/",
	"title": "for loop",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/form/",
	"title": "form",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/form-class/",
	"title": "form class",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/formbuilder/",
	"title": "formBuilder",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/ftp-server/",
	"title": "ftp server",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/fuzzy-text-search/",
	"title": "fuzzy text search",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/gaming-patterns/",
	"title": "gaming-patterns",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/genetics-algorithm/",
	"title": "genetics algorithm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/getting-started-with-amazon-aurora-whitepaper/",
	"title": "Getting Started with Amazon Aurora Whitepaper (2016)",
	"tags": ["aws", "whitepaper", "aurora", "mysql"],
	"description": "",
	"content": " Amazon Aurora Amazon Aurora Architecture Monitoring Migrating to Amazon Aurora  Amazon Aurora  Fully managed, cloud-native database service Scalability, reliability, performance Cost-effective Highly durable  Database volumes are divided into 10GB segments Each segment is replicated six ways across 3 AZs   Fault-tolerant  Transparently handles loss of 2 out of 6 copies without losing write availability or three out of six copies without losing read availability   Self-healing  Automatically replaces or repairs failed disks and nodes   Storage autoscaling  Volume grows in increments of 10GB up to a maximum of 64TB   Continuous backup  No impact on performance, 99.99999999999% (11 nines) durability   High performance  Delivers up to five times the throughput of standard MySQL   Read replicas  Each cluster may have up to 15 read replicas across AZs Scale read operations Act as failover targets Replication lag is very low (typically in tens of milliseconds)   Instant crash recovery  Log-structured storage that doesn\u0026rsquo;t require crash recovery replay of database redo logs   Survivable buffer cache  Database buffer cache is isolated from the database process Cache survives a database restart   Highly secure  Runs in a VPC by default, uses SSL to secure data in transit Support encryption at rest    Amazon Aurora Architecture Cluster consists of:\n Primary instance  Supports read-write workloads   Cluster volume  SSD virtual database storage volume that spans across multiple AZs Each AZ having 2 copies of the cluster data The primary instance and Amazon Aurora Replicas share te same cluster volume   Aurora Replica:  Can be additionally created, up to 15 replicas Distributes read workload When located in separate AZ can increase DB availability    Features:\n Self-Healing, fault-tolerant design  Using volume spanning across multi AZs DB volume is split into 10GB segments, spread widely across the cluster (isolates the blast radius of disk failures)   Automatic, continuous backups  Continuously backs up data to Amazon S3 Automatic backup retention up to 35 days Database restore point-in-time up to last 5 minutes   High performance  Modified database engine Log-structure storage SSD-based virtualized layer purpose-built for database workloads Tests on r3.8xlarge delivers over 500,000 SELECTs/second and 100,000 updates/second Data write operations are acknowledge as soon as they are committed by four out of six storage nodes Storage nodes acknowledge the write operations as soon as the log records are persisted to disk   Autoscaling storage  Don\u0026rsquo;t have to provision the space Storage grows automatically Delivers consistent low-latency I/O Hotspots are managed to move data around to ensure consistent performance of the storage of the layer   Low-latency read replicas  Lower costs, due to sharing the same underlying storage as the primary instance No need to replay logs at the replica nodes Less processing power to serve read requests Reduced replica lag time   Failure testing  Fault injection queries enables you to schedule a simulated occurrence of failure events  Crash of the master instance or an Aurora Replica Failure of an Aurora Replica Disk failure Disk congestion   Example: ALTER SYSTEM CRASH [INSTANCE | DISPATCHER | NODE]   Multiple Failover Targets  On instance failure, Aurora will automatically failover to any of up to 15 Replicas Recommended to place at least one replica in an alternate AZ Failover happens with no data loss, and log replay is not required  Replicas and the primary instance share the same storage     Instance Crash Recovery  Immediate recover from crash No need to replay the redo log from the last database checkpoint Restart time is reduced to less than 60 seconds   Survivable Caches  Cache is isolated from the database Cache remains warm after database restart   Security  Based Amazon VPC SGs and NACLs can be leverage to control access to your instances in each subnet Supports SSL connections from application, using SSL (AES-256) Supports encryption of data at rest, using AES-256 with hardware acceleration support Encryption keys managed using AWS KMS    Monitoring  Default CloudWatch metrics are collected from hypervisor Enhanced monitoring of up to 1 seconds granularity  Metrics are collected from lightweight agent installed the instance Supports up to 50 metrics related to CPU usage, network, storage, and memory    Migrating to Amazon Aurora  MySQL on RDS database snapshot can be to Aurora DB cluster MYSQL on EC2 can be migrated by piping mysqldump directly to Amazon Aurora AWS Database Migration Service (AWS DMS)  Offer minimal downtime or service interruption The source database remains fully operational during the migration AWS DMS continuously captures the changes from the source database and applies them to the target Manages all the complexities of the migration process, e.g. compression, parallel transfer for faster data transfer Low-cost and simple to use - pay for compute resources during the migration process Supports Oracle to Oracle, Oracle to Aurora, SQL Server to MySQL  For heterogenous migrations use AWS Schema Conversion Tool      "
},
{
	"uri": "https://majdarbash.github.io/tags/github/",
	"title": "github",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csap/glossary/",
	"title": "Glossary",
	"tags": [],
	"description": "",
	"content": " Fault Tolerance High Availability Storage Gateway AWS Trusted Advisor  "
},
{
	"uri": "https://majdarbash.github.io/tags/google/",
	"title": "google",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/ha-arhictecture-high-availability/",
	"title": "HA Arhictecture: High Availability",
	"tags": [],
	"description": "",
	"content": "ELB, ALB (Elastic Load Balancer, Application Load Balancer)  a physical/virtual device designed to balance network load across multiple web servers at least 2 public Subnets are required when provisioning a load balancer ELBs do not have a pre-defined IPv4 addresses; you resolve to them using a DNS name types of load balancers  Application Load Balancers  Best suited for HTTP / HTTPS traffic Operates at Layer 7 Application-aware Advanced request routing, sending specified requests to specific web servers Register target groups Load balancing rules are supported   Network Load Balancers  Load balancing TCP traffic Operates at Layer 4 Capable of handling millions of requests per seconds with ultra-low latencies   Classic Load Balancers  Legacy / previous generation load balancers HTTP / HTTPS / TCP traffic Operates at layer 4, supports Layer-7 specific features, such as X-Forwarded and sticky sessions Errors: 504 - Gateway timeout - ec2 instances are not responding Registers instances     Health Checks  Checks the instance health by talking to it Instances are reported as InService, OutofService LBs have their own DNS name. You are never given an IP address Connection Draining - the number of seconds to allow traffic to be flowing (300 seconds by default)    Auto-Scaling  Auto-Scaling Group  subnets should be assigned - instances will be distributed on these subnets Scaling Policy  Target tracking scaling policy Simple scaling policy Scaling policy with steps   Scaling warm-up time\nthe time required to warm up an instance - during this time instance won\u0026rsquo;t contribute to the metrics When deleting the auto-scaling group, the instances beneath it will be deleted as well.   Launch Configuration  IP Address Type  Default public IP assignment (as per subnet) Assign public IP to every instance Do not assign a public IP to an instance      HA Architecture  You should plan for failure Netflix (Simian Army Projects)\nInjecting failure into production systems\n(https://medium.com/netflix-techblog/the-netflix-simian-army-16e57fbab116) More than one AZ should be used (2 AZs at least) Always Design for failure Use Multiple AZ\u0026rsquo;s and Multiple Regions wherever you can In RSD Read Replica - creates a replica of AWS for performance purposes and not for HA Multi-AZ configuration means replication across different AZs supporting seamless failover Scaling out - adding ec2 instances using ASGs Scaling up - increasing resources of ec2 instances (modifying instance type) S3 storage classes  Highly Available  Standard S3 Standard S3 Infrequently access   Non-HA  Reduced redundancy storage S3 single AZ      HA Wordpress Site  2 x Cloud Front distributions  Cloud Front distribution for static media content Cloud Front distribution for the main site   2 x S3 Buckets  S3 Bucket for media S3 Bucket for code synchronization   RDS MySQL/Aurora with Mutli-AZ enabled Writer Node  LAMP installed CRON commands  Sync/var/www/html/wp-content/uploads to media bucket\naws s3 sync \u0026ndash;delete /var/www/html/wp-content/uploads s3://bucket-for-media-name/ Sync /var/www/html to code bucket\naws s3 sync \u0026ndash;delete /var/www/html s3://bucket-for-code-name/     Reader Node(s)  LAMP installed Scales in 3 AZs using ASG, min size is 2 instances Traffic received by Cloud Front distribution for the main site  Route 53 domain record points at Cloud Front distrbution ALB is defined as an origin ALB forwards traffic to Target Groups   On user-data instances will pull the code content from S3 bucket-for-code Periodically instances will run sync commands to get any updates done to the code from the Writer Node CRON command  aws s3 sync \u0026ndash;delete s3://bucket-for-code-name/ /var/www/html   .htaccess rewrite rule is added to serve the wp-content/uploads files from Cloud Front distribution serving the static media bucket from bucket-for-media-name   Simulating failure  Terminate an EC2 instance RDS, Multi-az  You may reboot with failover to simulate the failure This may still take your site offline for a couple of minutes      Cloud Formation  Is a way of completely scripting your cloud environment From AWS website:  \u0026ldquo;AWS CloudFormation provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This file serves as the single source of truth for your cloud environment.\u0026rdquo;   https://aws.amazon.com/quickstart/  Built by solution architects and partners Cloud Formation templates to help you build your production or test environment quickly and starting using it immediately    "
},
{
	"uri": "https://majdarbash.github.io/random/accessing-multiple-github-accounts-from-one-computer/",
	"title": "How to accessing multiple Git accounts from the same computer using different private keys?",
	"tags": ["github", "account"],
	"description": "",
	"content": "In order to access multiple github accounts from one computer you need to :\n generate ssh keys add ssh keys to your github account create a configuration file to manage the separate keys update stored identities  Step 1 - Generating SSH keys:\ncd ~/.ssh ssh-keygen -t rsa -C \u0026quot;[email associated with your github account #1]\u0026quot; # save it as id_rsa_git_account1 when prompted ssh-keygen -t rsa -C \u0026quot;[email associated with your github account #2]\u0026quot; # save it as id_rsa_git_account2 when prompted As the result of executing these commands, the following files will be generated:\n id_rsa_git_account1 id_rsa_git_account1.pub id_rsa_git_account2 id_rsa_git_account2.pub  Step 2 - Adding ssh keys to your github account\nUsing this command you can copy the key to your clipboard:\npbcopy \u0026lt; ~/.ssh/id_rsa_git_account1.pub Now as the key is in your account, all you have to do is to open your github account and:\n Click on Account Settings Click \u0026ldquo;SSH Keys\u0026rdquo; -\u0026gt; \u0026ldquo;Add SSH Key\u0026rdquo; Paste your key into key field and add a relevant title Click \u0026ldquo;Add key\u0026rdquo; then enter your Github password to confirm.  Step 3 - Create a configuration file to manage the separate keys\n Navigate to ~/.ssh/ touch config Enter the configuration to the config file:  # githubAaccount1 Host account1 HostName github.com User git IdentityFile ~/.ssh/id_rsa_git_account1# github Account2 Host account2 HostName github.com User git IdentityFile ~/.ssh/id_rsa_git_account2 Step 4 - Update Stored Identities:\nssh-add -D Add new keys:\nssh-add id_rsa_git_account1 ssh-add id_rsa_git_account2 Test to make sure new keys are stored\nssh-add -l Testing Push / Pull\ntouch readme.md $ git ini $ git add . $ git commit -am \u0026quot;first commit\u0026quot; $ git remote add origin git@account1:githubPersonal/test-personal.git $ git push origin master  Publish Date: 2015-04-08\n "
},
{
	"uri": "https://majdarbash.github.io/random/backing-mysql-database/",
	"title": "How to Backup MySQL database using Percona&#39;s innobackupex",
	"tags": [],
	"description": "",
	"content": "Backup using innobackupex This script backups your database and uploads it to Amazon S3:\n#!/bin/bash\ntimestamp=`date \u0026ldquo;+%Y-%m-%d_%H-%M-%S\u0026rdquo;`\ninnobackupex \u0026ndash;user=[username] \u0026ndash;password=[password] \u0026ndash;parallel=3 \u0026ndash;safe-slave-backup \u0026ndash;slave-info \u0026ndash;no-timestamp /data/backup/${timestamp}_slave innobackupex \u0026ndash;user=[username] \u0026ndash;password=[password] \u0026ndash;apply-log /data/backup/${timestamp}_slave/\ntar czf /data/backup/${timestamp}_slave.tgz /data/backup/${timestamp}_slave\n/usr/local/bin/aws \u0026ndash;region [region] s3 cp /data/backup/${timestamp}_slave.tgz s3://[bucket]/[backup-path]/`date \u0026ldquo;+%Y\u0026rdquo;`/`date \u0026ldquo;+%m\u0026rdquo;`/`date \u0026ldquo;+%d\u0026rdquo;`/\nrm -rf /data/backup/${timestamp}_slave rm -f /data/backup/${timestamp}_slave.tgz\nYou can easily write this script in backup.sh and schedule a cronjob using crontab -e for performing this backup on the daily basis\nIn order to restore the backup downloaded you have to:\n# extracting backup contents to BACKUP-DIR tar -xzvf /data/backup/${timestamp}_slave.tgz innobackupex \u0026ndash;copy-back /path/to/BACKUP-DIR\ninnobackupex: Finished copying back files. 111225 01:08:13 innobackupex: completed OK! You may need to change ownership of files from: chown -R mysql:mysql /var/lib/mysql\nDownloading innobackupex on Mac:\ncd /tmp curl -O https://www.percona.com/downloads/XtraBackup/XtraBackup-0.8/MacOS/xtrabackup-0.8-macos.tar.gz tar -xzf xtrabackup-*-macos.x86_64.tar.gz cd xtrabackup*\nchecking the files downloaded ls cp * /usr/local/bin\nNow you can call the backup perl script, using innobackupex-1.5.1 command\n# will restore the backup to MySQL datadir (determined by value is mysql my.cnf) innobackupex \u0026ndash;copy-back [path/to/Backup-dir]\nchange the ownership in data-dir to make sure it\u0026rsquo;s correctly set up chown -R mysql:mysql /var/lib/mysql\nspecify the configuration file location innobackupex \u0026ndash;copy-back \u0026ndash;defaults-file=/etc/my.cnf \u0026ndash;[path/to/backup-dir]\nBackup using mysqldump # dumping the database mysqldump -u[user] -p[password] -h[hostname] [database] \u0026gt; /tmp/backup.sql\nrestoring the database mysql -u[user] -p[password] -h[hostname] [database] \u0026lt; /tmp/backup.sql\n"
},
{
	"uri": "https://majdarbash.github.io/random/find-backlinks-for-your-site/",
	"title": "How to find the backlinks to your site which are indexed by Google?",
	"tags": ["seo", "google", "backlinks"],
	"description": "",
	"content": "I was trying to find backlinks for my site on google search. The best way to do this was to do a search, using the following parameters:\n[your Keywords or your Sitename] -site:yoursitename.com\nBasically this will located all the words which match the search terms, excluding your own site -\u0026gt; which are actually the backlinks to your site.\nPublish Date: 2014-10-19\n "
},
{
	"uri": "https://majdarbash.github.io/random/levenshtein-distance-in-mysql-measuring-distance-between-the-strings/",
	"title": "How to measure distance between strings? What is Levenstein Distance?",
	"tags": ["mysql", "database", "levenshtein distance", "fuzzy text search"],
	"description": "",
	"content": "Levenshtein distance function can be used to measure distance between the strings. Smaller distance will mean that string 1 is closer to string 2. This function can be used to implement \u0026ldquo;fuzzy\u0026rdquo; text search in mySQL where you can search for records and order by levenshtein distance in ascending order. In this case the closest matches (more relevant results) will appear on top and then the less relevant ones.\nDELIMITER ;; DROP FUNCTION IF EXISTS `levenshtein`;; CREATE FUNCTION `levenshtein`(s1 VARCHAR(255), s2 VARCHAR(255)) RETURNS INT(11) DETERMINISTIC BEGIN DECLARE s1_len, s2_len, i, j, c, c_temp, cost INT; DECLARE s1_char CHAR; DECLARE cv0, cv1 VARBINARY(256); SET s1_len = CHAR_LENGTH(s1), s2_len = CHAR_LENGTH(s2), cv1 = 0x00, j = 1, i = 1, c = 0; IF s1 = s2 THEN RETURN 0; ELSEIF s1_len = 0 THEN RETURN s2_len; ELSEIF s2_len = 0 THEN RETURN s1_len; ELSE WHILE j \u0026lt;= s2_len DO SET cv1 = CONCAT(cv1, UNHEX(HEX(j))), j = j + 1; END WHILE; WHILE i \u0026lt;= s1_len DO SET s1_char = SUBSTRING(s1, i, 1), c = i, cv0 = UNHEX(HEX(i)), j = 1; WHILE j \u0026lt;= s2_len DO SET c = c + 1; IF s1_char = SUBSTRING(s2, j, 1) THEN SET cost = 0; ELSE SET cost = 1; END IF; SET c_temp = CONV(HEX(SUBSTRING(cv1, j, 1)), 16, 10) + cost; IF c c_temp THEN SET c = c_temp; END IF; SET c_temp = CONV(HEX(SUBSTRING(cv1, j + 1, 1)), 16, 10) + 1; IF c c_temp THEN SET c = c_temp; END IF; SET cv0 = CONCAT(cv0, UNHEX(HEX(c))), j = j + 1; END WHILE; SET cv1 = cv0, i = i + 1; END WHILE; END IF; RETURN c; END;; DELIMITER ;  Publish Date: 2014-09-12\n "
},
{
	"uri": "https://majdarbash.github.io/random/apache-configuration-logging-traffic-from-x-forwarded-for-header/",
	"title": "How to properly log ClientIP on Apache2 behind Load Balancer?",
	"tags": ["apache2", "log", "X-Forwarded-For"],
	"description": "",
	"content": "This case will help you to log to apache server any traffic coming from the load balancer, proxy or any IDS.\nLogFormat \u0026quot;%h %l %u %t \u0026quot;%r\u0026quot; %\u0026gt;s %b \u0026quot;%{Referer}i\u0026quot; \u0026quot;%{User-Agent}i\u0026quot;\u0026quot; combined LogFormat \u0026quot;%{X-Forwarded-For}i %l %u %t \u0026quot;%r\u0026quot; %\u0026gt;s %b \u0026quot;%{Referer}i\u0026quot; \u0026quot;%{User-Agent}i\u0026quot;\u0026quot; proxy SetEnvIf X-Forwarded-For \u0026quot;^.*..*..*..*\u0026quot; forwarded CustomLog ${APACHE_LOG_DIR}/access_log combined env=!forwarded CustomLog ${APACHE_LOG_DIR}/access_log proxy env=forwarded Where are Apache2 Logs? The default location of apache log files on ubuntu server apache2 installation is:\n/var/www/apache2/access.log in order to view the log file contents you can use any of the following commands:\ndisplays the whole contents of the log file\ncat /var/www/apache2/access.log shows the contents from the end of access.log file and wait to display any additional logs appended to the end of the file\ntail -f /var/www/apache2/access.log displays the last 100 mb of the apache2 log file\ntail -c 104857600 /var/log/apache2/access.log If you would like to extract the last 100 mb of the log file to some other file, you can achieve this by executing the command:\ntail -c 204857600 /var/log/apache2/access_sacoffice_log \u0026gt; log.log After this, you can download the log file and check the logs ! :)\nPublish Date: 2014-11-18\n "
},
{
	"uri": "https://majdarbash.github.io/random/password-protect-directory-apache-configurations/",
	"title": "How to protect directory with a password in Apache2?",
	"tags": ["ubuntu server", "web server", "apache", "password protect", "security"],
	"description": "",
	"content": "A usual scenario encountered by every web server is administrator is to password protect specific directory and all it\u0026rsquo;s resources. We need to add some lines in the configuration file where our web server / virtual host is defined. This is how we do it:\nCreating the password file: (the system will ask you to enter the password which will be saved in encrypted format)\nsudo htpasswd -c [filename] [username] Apache BASIC HTTP Authentication using the created password file\nAuthUserFile [the path to the password file] AuthType Basic AuthName \u0026quot;RESTRICTED ACCESS\u0026quot; Require valid-user  Publish Date: 2014-10-27\n "
},
{
	"uri": "https://majdarbash.github.io/random/running-apache-2-proxy-mod_proxy/",
	"title": "How to run Apache2 as proxy using modproxy module?",
	"tags": [],
	"description": "",
	"content": "One other way to fix CORS issue and prevent your API endpoints to be exposed is to configure Apache2 web server to proxy certain requests, thus making it serve the requests to different domain through the apache installation of the current domain. The other application I can think about would be to hide the web service endpoint.\nIn order to make apache act as a \u0026ldquo;proxy\u0026rdquo; you need to make sure that proxy_http extension is there. In any case, please execute the commands below to install apache proxy module, assuming you are running your apache2 installation on Ubuntu Server Edition.\nsudo a2enmod proxy sudo service apache2 restart a2enmod proxy_http sudo a2enmod proxy_http sudo service apache2 restart\nAfter enabling the required proxy_http module, you need to add the following line to the .htaccess file:\nRewriteRule /path/to/request/to/be/routed/(.*) http://remote/server/mapped/url/$1 [P]\n"
},
{
	"uri": "https://majdarbash.github.io/random/setting-squid-proxy-on-ubuntu-server/",
	"title": "How to Setup Squid proxy on Ubuntu Server?",
	"tags": ["ubuntu", "squid", "proxy"],
	"description": "",
	"content": "In order to install a proxy service on Ubuntu Server edition, please execute the following commands:\nInstalling Squid:\nsudo aptitude -y install squid3\nEditing the configuration file:\nsudo vim /etc/squid3/squid.conf // add a line in order to enable global access: http_access allow all\nRestart the squid to reflect the newly changed configuration settings\ninitctl restart squid3\nAs the squid is running now, you can access the proxy by setting the default port as 3128 and using the host ip address.\nIn order to be able to access squid make sure to open the port 3128 externally for the server you have installed the squid on.\n"
},
{
	"uri": "https://majdarbash.github.io/random/docker-network-conflicts/",
	"title": "How to solve Network IP conflict when running Docker?",
	"tags": [],
	"description": "",
	"content": "Sometimes docker container networks created will conflict with the network configuration you have.\nTo fix it, I\u0026rsquo;m using the following configuration on my local machine:\n# /etc/docker/daemon.json { \u0026quot;bip\u0026quot;: \u0026quot;192.168.199.1/28\u0026quot;, \u0026quot;dns\u0026quot;: [\u0026quot;8.8.8.8\u0026quot;, \u0026quot;8.8.4.4\u0026quot;] } This configuration will limit the IP addresses of the docker containers to 192.168.199.1 CIDR block. In addition you may want to troubleshoot. This may help you identify any potential conflict between your network / DNS server addresses and your existing networks / bridges created by docker.\n# list the interfaces can help you identify what networks do you have ifconfig # will display DNS servers used and other information of the used network interface nmcli device show [interface name] # investigate the routing table to make sure that the destination interfaces are correct # and none of the DNS servers used will get blocked by a network / bridge route -n # display the bridge networks using bridge administrator tool sudo apt-get install bridge-utils brctl show # delete any docker networks which may be conflicting with the current settings docker network prune I have done these steps several times and found it helpful to solve my issues.\nHope this helps !\nPublish Date: 2019-07-17\n "
},
{
	"uri": "https://majdarbash.github.io/random/transferring-wordpress-one-domain-another/",
	"title": "How to transfer a Wordpress site from one domain to another?",
	"tags": ["database", "wordpress", "migration"],
	"description": "",
	"content": "Explanation Other than uploading your wordpress sites to the new domain, you will need to do some changes in your database. Therefore, when you want to move the wordpress website to a new domain, you will find out that existing database records are still pointing at the old domain. So how do we properly migrate our wordpress site to the new domain name?\nI have seen a lot of developers going straight to the database and updating records to a new domain. Some just write update queries to change the domain. Usually they end up with corrupt wordpress site - which may not be functioning. If you are lucky the site may be still working but you may experience other issues with the website later on.\nSo what makes changing the wordpress domain so complex? And is it really such a big deal? Not really - the answer lies in serialised strings in the database. Because you cannot manually modify serialized strings, you will need a script to do this for you.\nAfter searching I found out some useful script that will help you replace all occurrences of the old domain to the new one.\nAction Points Follow these steps for proper migration of your wordpress site:\n Before starting I prefer to backup the existing database. You can do this from the control panel, or phpmyadmin / other database access interfaces - if something goes wrong you can always restore your wordpress site database. Download the script from this link:\n[download-attachment id=\u0026quot;323\u0026rdquo; title=\u0026quot;Search-Replace-DB-master.zip\u0026rdquo;] Extract the contents to where you have uploaded your website. Access the url which will lead you to installed script location Carefully choose the old site name in the strings to replace, including http:// and www in case it was used in the old wordpress configuration. For example: http://www.[oldsitename].com Submit the new site name with the full url : http://www.[newsitename].com Submit the database credentials Dry run the script to check what changes will be made after the script is running\n(You can click on specific change to see the differences which will be made in the database script) Click \u0026ldquo;Update\u0026rdquo; and confirm to update the database record Enjoy your properly migrated wordpress site.  \n"
},
{
	"uri": "https://majdarbash.github.io/random/htaccess-how-to-redirect-non-www-url-to-www/",
	"title": "htaccess : how to redirect non-www url to www",
	"tags": [],
	"description": "",
	"content": "Place the following configuration in your .htaccess file:\nRewriteEngine On RewriteCond %{HTTP_HOST} !^www\\. RewriteCond %{HTTP_HOST} !=localhost RewriteRule ^(.*)$ http://www.%{HTTP_HOST}/$1 [R=301,L]\nThis line is placed to make sure that redirection does not occur on your localhost during the development process. You can remove this line if you want to place the .htaccess configuration directly on your production server. Otherwise you can leave it and change \u0026ldquo;localhost\u0026rdquo; to any other local web server you are running.\nRewriteCond %{HTTP_HOST} != localhost\nRedirects from .htaccess file # permanent redirect (301) Redirect 301 /path new_Url\ntemporary redirect (302) Redirect 302 /path new_Url\n"
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/iam/",
	"title": "IAM",
	"tags": [],
	"description": "",
	"content": "IAM (Identity Access Management)\n Centralized control of your AWS account Shared Access to your AWS account Granular Permissions Identity Federation (including Active Directory, Facebook, etc\u0026hellip;) Multifactor Authentication Temporary access to users/devices and services Password rotation policy PCI DSS Compliance  Models:\n Users Groups Policies Roles  AWS provides IAM Pre-defined Roles per Job Function, otherwise custom roles and policies can be defined.\n IAM is universal. It does not apply to regions. The root account is the account created when first setting up AWS account - It has complete Admin access. Users have no permissions by default. New Users are assigned Access Key ID \u0026amp; Secret Access Keys when programmatic access is enabled. Access Key Id and Secret Access Key will be displayed once. If lost, they will have to be regenerated. MFA should be set up on the root account.  "
},
{
	"uri": "https://majdarbash.github.io/aws/iam-policies-on-aws-reinvent/",
	"title": "IAM Policy Master on AWS (re:Invent 2018)",
	"tags": [],
	"description": "",
	"content": " Policy Types and use cases Policy resolution within an account Policy resolution across accounts Example use cases  1. Set permission guardrails across accounts 2. Control creation of resources to regions 3. Enable developers to create roles safely 4. Use tags to scale permission management 5. Utilize PrincipalTags in IAM policies    Policy Types and use cases  AWS Organizations  Use: Guardrails to disable service access on the principals of the account Service control policies (SCP)   AWS Identity and Access Management  Use: Grant granular permissions on IAM principals (users and roles) and control the maximum permissions they can set Permission policies Permission boundaries  Scale and delegate permissions management to developers safely Control the maximum permissions employees can grant     AWS Security Token Service (STS)  Use: Reduce general shared permissions further Scoped-down policies   Specific AWS Resources  Use: Cross-account access and to control access from the resource Resource-based policies   VPC Endpoints  Use: Controls access to the service with a VPC endpoint Endpoint policies    Policy resolution within an account Final Policy = [SCP] AND ([IAM Policies] OR [Resource-based Policies]) where: [IAM Policies] = ?[Permission Boundary] AND [Permission Policy (managed / inline)] AND [Scope-down Policy]\nPolicy resolution across accounts Final Policy = [SCP] AND ([IAM Policies] AND [Resource-based Policies]) where: [IAM Policies] = ?[Permission Boundary] AND [Permission Policy (managed / inline)] AND [Scope-down Policy]\nExample use cases 1. Set permission guardrails across accounts { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;DenyUnapprovedAction\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34; : [ \u0026#34;ds:*\u0026#34;, \u0026#34;iam:createUser\u0026#34;, \u0026#34;cloudtrail:stopLogging\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } 2. Control creation of resources to regions { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34; : [ \u0026#34;secretsmanager:*\u0026#34;, \u0026#34;lambda:*\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:RequestedRegion\u0026#34;: [\u0026#34;us-west-1\u0026#34;, \u0026#34;us-west-2\u0026#34;] } } }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34; : [ \u0026#34;ec2:RunInstances\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:ec2:*:*::subnet/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::key-pair/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::instance/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::snapshot/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::launch-template/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::volume/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::security-group/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::placement-group/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::network-interface/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::image/*\u0026#34;, ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:RequestedRegion\u0026#34;: [\u0026#34;us-west-1\u0026#34;, \u0026#34;us-west-2\u0026#34;] } } }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:Describe\u0026#34;, \u0026#34;ec2:Get\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListAllBuckets\u0026#34;, \u0026#34;iam:list\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 3. Enable developers to create roles safely Enable your developers to create IAM roles and pass to EC2 and Lambda, but ensure they cannot exceed their own permissions\nSolution (using Permission Boundaries):\n Allow create managed policies Allow create role, but only with specific permission boundary Allow attach managed policies, but only to roles with specific permission boundary Allow passRole for these roles using a naming requirement  In this example, the role created should have unicorns prefix\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreatePolicyVersion\u0026#34;, \u0026#34;iam:DeletePolicyVersion\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::1234567890:policy/unicorns-*\u0026#34; } { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::1234567890:role/unicorns-*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;iam:PermissionsBoundary\u0026#34;: \u0026#34;arn:aws:iam:1234567890:policy/region-restriction\u0026#34; } } } 4. Use tags to scale permission management Enable developers working on Dorky project and the sneaky project to manage their own resources without also managing the other project\u0026rsquo;s.\nSolution:\nUse:\n RequestTag - condition to require tag value during create actions ResourceTag - control access to resources based on a tag that exists on resource   Allow users to create tags when creating resources, but require specific tags when users create resources  arn:aws:ec2:*:*::instance/* Resource was removed from first policy and expanded in the one below\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34; : [ \u0026#34;ec2:RunInstances\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:ec2:*:*::subnet/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::key-pair/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::snapshot/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::launch-template/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::volume/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::security-group/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::placement-group/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::network-interface/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*::image/*\u0026#34;, ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:RequestedRegion\u0026#34;: [\u0026#34;us-west-1\u0026#34;, \u0026#34;us-west-2\u0026#34;] } } } { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34; : [ \u0026#34;ec2:CreateTags\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ec2:CreateAction\u0026#34;: \u0026#34;RunInstances\u0026#34; } } } { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34; : [ \u0026#34;ec2:RunInstances\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:ec2:*:*:instance/*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;ForAllValues:StringEquals\u0026#34;: { \u0026#34;aws:TagKeys\u0026#34;: [\u0026#34;project\u0026#34;, \u0026#34;name\u0026#34;] }, \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:RequestTag/project\u0026#34;: [\u0026#34;dorky\u0026#34;], \u0026#34;aws:RequestedRegion\u0026#34;: [\u0026#34;us-west-1\u0026#34;, \u0026#34;us-west-2\u0026#34;] } } } 2. Control which existing resources and values developers can tag\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34; : [ \u0026#34;ec2:createTags\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ec2:ResourceTag/project\u0026#34;: [\u0026#34;dorky\u0026#34;] }, \u0026#34;ForAllValues:StringEquals\u0026#34;: { \u0026#34;aws:TagKeys\u0026#34;: [\u0026#34;project\u0026#34;, \u0026#34;name\u0026#34;] }, \u0026#34;StringEqualsIfExists\u0026#34;: { \u0026#34;aws:RequestTag/project\u0026#34;: [\u0026#34;dorky\u0026#34;] } } } 3. Control resources users can manage based on tag values\nUsers with dorky tag can start/stop instances:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34; : [ \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ec2:ResourceTag/project\u0026#34;: [\u0026#34;dorky\u0026#34;] } } } 5. Utilize PrincipalTags in IAM policies Policies can be adjusted to have a single general policy and tagging users. This can be achieved using ${aws:PrincipalTag/project}.\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34; : [ \u0026#34;ec2:RunInstances\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:ec2:*:*:instance/*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;ForAllValues:StringEquals\u0026#34;: { \u0026#34;aws:TagKeys\u0026#34;: [\u0026#34;project\u0026#34;, \u0026#34;name\u0026#34;] }, \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:RequestTag/project\u0026#34;: [\u0026#34;${aws:PrincipalTag/project}\u0026#34;], \u0026#34;aws:RequestedRegion\u0026#34;: [\u0026#34;us-west-1\u0026#34;, \u0026#34;us-west-2\u0026#34;] } } } "
},
{
	"uri": "https://majdarbash.github.io/aws/microservices-on-aws-whitepaper/",
	"title": "Implementing Microservices on AWS (2019)",
	"tags": ["aws", "microservices", "whitepaper"],
	"description": "",
	"content": " Simple Microservices Architecture on AWS Reducing Operational Complexity Distributed Systems Components  Service Discovery Distributed Data Management Asynchronous Communication and Lightweight Messaging Distributed Monitoring    Microservices are a combination of various successful and proven concepts such as:\n Agile software development Service-oriented architectures API-first design Continuous Integration/Continuous Delivery (CI/CD)  Serverless operational model tenets:\n No infrastructure to provision or manage Automatically scaling by unit of consumption \u0026ldquo;Pay for value\u0026rdquo; billing model Built-in availability and fault tolerance  Simple Microservices Architecture on AWS User Interface\n Often use JavaScript, single-page application Communicates with a REST / RESTful API Static web content can be served using Amazon S3 / Amazon CloudFront CDN reduces client/server latency  Microservices\n APIs serve as an entry point for applications logic Typically RESTful web services API Traffic management, request filtering, routing, caching, authentication, and authorization  Microservices Implementation\n AWS Lambda AWS Elastic Container Service (ECS)  AWS Fargate (Docker containers) - run serverless containers   AWS Elastic Kubernetes Service (EKS)  Private Links\n Allow you to privately connect your VPC to supported AWS services  Data Store\n Amazon ElastiCache Amazon Aurora, Amazon RDS Amazon DynamoDB  DynamoDB Accelerator (DAX)    Reducing Operational Complexity Amazon API Gateway:\n Create your APIs programmatically by importing swagger definitions, using AWS API or AWS Management Console Serves as a front door for any web application running on EC2, ECS, Lambda, or any on-premises environment Fully managed API service Integrated with CloudFront Supports Custom Domains Serverless, eliminates operational complexity Tightly integrated with Lambda Can be deployed using AWS CloudFormation  AWS Serverless Application Model (AWS SAM) is a convenient way to define serverless applications    Distributed Systems Components Service Discovery DNS-Based Service Discovery\n Amazon ECS creates and manages a registry of service names using Route 53 Auto Naming API Names are automatically mapped to a set of DNS records Services can communicate with each other using service discovery names Use unified service discovery for services managed by Kubernetes Use AWS CloudMap for extended capabilities, e.g. service registry for IPs, URLs, ARNs, with faster change propagation and the ability to use attributes to narrow down the set of discovered resources  Service Discovery using Third-Party Software\n HashiCorp Consul etcd Netflix Eureka  Service Meshes\n Additional layer for handling inter-service communication Responsible for monitoring and controlling traffic in microservice architectures AWS App Mesh  Provides application-level networking Make it easy for your services to communicate Standardizes how your services communicate Gives you end-to-end visibility and ensuring high availability for you applications Can be used with Fargate, ECS, EKS    Distributed Data Management  Each microservice component should have its own data persistence layer Distributed microservices architectures inherently trade off consistency for performance Need to embrace eventual consistency Distributed Saga pattern helps avoid partial executions by orchestrating compensating transactions that undo the changes that were made by the preceding transactions AWS Step Functions can help in implementing Saga execution coordinator Event Sourcing pattern can be used when state changes affect more than a single microservice  Represent and persist every application change as an event record State can be reconstructed for any point in time Enables decoupling different parts of an application by using a publish/subscribe pattern Frequently used in conjunction with CQRS      Asynchronous Communication and Lightweight Messaging REST-based Communication\n Over HTTP/S protocol Stateless communication, uniform interfaces, and standard methods Use API Gateway GET, POST, PUT request methods Application can be deployed to different stages and can be versioned  Asynchronous Messaging and Event Passing\n Messaging passing pattern via queue Possible combination of Amazon SQS and Amazon SNS  SNS can push a message to multiple subscribers, through push mechanism   Amazon MQ can be another solution  Protocols supported: JMS, NMS, AMQP, STOMP, MQTT, and WebSocket Managed service for ActiveMQ    Orchestration and State Management\n Avoid adding orchestration code into the service directly - will introduce tighter coupling and make it harder to replace individual services Use Step Functions to build applications from individual components that each perform a discrete function  Distributed Monitoring  Use Amazon CloudWatch to collection logs and build metrics for monitoring Popular choice for Amazon EKS - Prometheus, often used in combination with Grafana to visualize the collected metrics EC2 instance have a daemon to send logs to EC2 AWS ECS includes support for awslogs log driver that allow centralization of container logs to CloudWatch Logs AWS KES forwards logs using FluentD to a centralized logging CloudWatch Logs, combined for higher level reporting using Elasticsearch and Kibana Amazon Athena can be used to run ad hoc queries against centralized log files in Amazon S3  Distributed Tracing\n AWS X-Ray allows tracing all events and messages based on specific unique identifier, attached to all request related to specific event chain Works with EC2, ECS, Lambda and Elastic Beanstalk Supports applications written in Java, Node.js and .NET  Log Analysis on AWS\n Amazon CloudWatch Insights Amazon ES + Kibana CloudWatch logs can stream logs to Amazon ES through a CloudWatch Logs subscription Amazon Redshift together with Amazon Quicksight Logs can be streamed from CloudWatch Logs to Amazon Redshift using Kinesis Data Firehose  Chattiness\n Communication overhead increases because microservices have to talk to each other REST over HTTP is a lightweight communication protocol You might consider consolidating services that send many messages back and forth to reduce chattiness  Protocols\n HTTP for communication Messages can be encoded using JSON or YAML or efficient binary formats like Avro or Protocol buffers  Caching\n Amazon ElastiCache to reduce volume of calls to other microservices by caching results locally API Gateway providers a built-in caching layer to reduce the load on the backend servers Reduce the load on persistence layer Find the right balance between a good cache hit rate and the timeliness/consistency of data  Auditing\n Ensure visibility of user actions on each service Being able to get a good overall view across all services at an organizational level AWS CloudTrail - tracking changes in microservices made in AWS Cloud  Can be logged and sent to either CloudWatch Logs in real time or to Amazon S3   CloudWatch Events with CloudTrail allows to generate events based on certain logs  Resource INventory and Change Management\n AWS Config rules allow a company to define security policies with specific rules to automatically detect, track, and alert you to policy violations Violations can be detected in AWS Config and remediated by triggering Amazon SNS for example  "
},
{
	"uri": "https://majdarbash.github.io/software-architecture-and-design/important-concepts-every-dev-should-know/",
	"title": "Important Concepts Every Developer should know",
	"tags": ["software development", "design patterns", "programming"],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/quickbooks-php-integration/",
	"title": "Initial steps towards successful integration Quickbooks &amp; PHP?",
	"tags": [],
	"description": "",
	"content": "Integration: PHP and Quickbooks using v3-php-sdk-2.0.5\nThis was a bit of headache. In this article I explain how to configure your quickbooks php sdk, issues faced, fixes until successful integration.\nSome useful references to accomplish php / quickbook integration using the php sdk.\n- download php sdk\nhttp://bit.ly/IppPhpSdkV3\n- PHP SDK for QuickBooks V3 documentation and getting started\nhttps://developer.intuit.com/docs/0100_accounting/0500_developer_kits/0210_ipp_php_sdk_for_quickbooks_v3\n*** Trying to Integrate\nAfter download the v3-php-sdk-2.0.5 in order to accomplish the quickbooks integration, i have faced some serious problems to be able to integrate. After the installation I have set some settings in App.Config and directly accessed _Samples folder and decided to run the safest CustomerQuery.php - assuming it doesn\u0026rsquo;t cause any modification to our existing records :).\nThe first issue encountered was this:\nFatal error: Uncaught exception \u0026lsquo;UnexpectedValueException\u0026rsquo; with message \u0026lsquo;DirectoryIterator::__construct(/var/folders/bn/gqvb2w4x7n3573t34xd1sdn00000gq/T): failed to open dir: Permission denied\u0026rsquo; in /var/www/test/v3-php-sdk-2.0.5/Core/LogRequestsToDisk.php:65 Stack trace: #0 /var/www/test/v3-php-sdk-2.0.5/Core/LogRequestsToDisk.php(65): DirectoryIterator-\u0026gt;__construct('/var/folders/bn\u0026hellip;') #1 /Applications/XAMPP/xamppfiles/htdocs/test/v3-php-sdk-2.0.5/Core/RestCalls/SyncRestHandler.php(227): LogRequestsToDisk-\u0026gt;LogPlatformRequests(\u0026lsquo;SELECT * FROM C\u0026hellip;', \u0026lsquo;https://sandbox\u0026hellip;', Array, true) #2 /var/www/test/v3-php-sdk-2.0.5/DataService/DataService.php(572): SyncRestHandler-\u0026gt;GetResponse(Object(RequestParameters), \u0026lsquo;SELECT * FROM C\u0026hellip;', NULL) #3 /var/www/test/v3-php-sdk-2.0.5/_Samples/CustomerQuery.php(37): DataService-\u0026gt;Query(\u0026lsquo;SELECT * FROM C\u0026hellip;') #4 {main} thrown in /var/www/test/v3-php-sdk-2.0.5/Core/LogRequestsToDisk.php on line 65\nAfter tracing the code i found out that the situation is caused by the following setting in sdk.config:\nSo basically I thought, let\u0026rsquo;s turn the logging off. The result was still the same after mofifying the configuration:\nAfter tracing the code I discovered that a fix should be applied to overcome this problem.\n*** Applied FIX:\n/Core/LogRequestsToDisk.php / line 57:\nif ($this-\u0026gt;EnableServiceRequestsLogging)\nreplaced by\nif ($this-\u0026gt;EnableServiceRequestsLogging == \u0026lsquo;true\u0026rsquo;)\nDescription:\nEnableServiceRequestsLogging was interpreted as string when using false value and on casting to bool the string word \u0026lsquo;false\u0026rsquo; was considered to be true: (bool)\u0026lsquo;false\u0026rsquo; \u0026ndash;\u0026gt; true\nAfter applying this fix we are ready to go and start setting up our configuration and the environment.\n*** Quickbooks developer account - Creating an App\nThe first thing you want to do is to create an app on quickbooks developer account by connecting to which you will be integrating with your quickbooks online account. In order to achieve this, access:\nhttps://developer.intuit.com\nFrom the top right menu, click \u0026ldquo;My Apps\u0026rdquo; and then create a new App.\nFor this tutorial choose the \u0026ldquo;Just start coding\u0026rdquo; option on the right and then check \u0026ldquo;Accounting\u0026rdquo; out of Quickbooks API. So far you have an app and some keys for development. Now you need to click on the \u0026ldquo;Keys\u0026rdquo; tab and you will see the App Token , OAuth Consumer Key and OAuth Consumer Secret.\nBasically the idea here is to generate AccessToken and AccessTokenSecret using the existing (App Token , OAuth Consumer Key and OAuth Consumer Secret). This will mean that your application with (App Token) will be granted access to your account data and services and will be authenticated by AccessToken and AccessTokenSecret, that you will use to connect from your application to the SDK.\nIntegration scenario\nPHP CODE + SDK \u0026ndash;(connected to)\u0026ndash;\u0026gt; Developer Application \u0026ndash;(connected to)\u0026ndash;\u0026gt; Quickbooks APIs\n*** Getting authentication tokens\nIn order to generate access token for your app to your whole account, you need to do this using the url:\nhttps://appcenter.intuit.com/Playground/OAuth\nfollowing step-by step instructions, starting from the already defined App Token , OAuth Consumer Key and OAuth Consumer Secret, we will continue until we get AccessToken and AccessTokenSecret. After this point we have all the authentication tokens we need in order to configure our SDK.\n*** Configuration\nwe need 2 configuration files:\nApp.Config - located at the path of the index file you are working on\nsdk.config - located at the root path of the SDK\nApp.Config should containg the settings:\n- RealmID (your company id, can include the sandbox company id)\n- AccessToken\n- AccessTokenSecret\n- ConsumerKey\n- ConsumerSecret\nsdk.config should be adjusted:\nAs we are integrating with quickbooks, we are concerned about \u0026ldquo;qbo\u0026rdquo; attribute of \u0026ldquo;baseUrl\u0026rdquo;.\nIn case we are on development environment and would like to use the actual company keys, we would need to use the default url:\nhttps://quickbooks.api.intuit.com/\nIn case we are connecting to the sandbox company this url has to be changed to:\nhttps://sandbox-quickbooks.api.intuit.com/\nSample App.Config file:\nSample sdk.config file:\nIf you have any ideas or suggestions or other experiences about PHP - Quickbooks SDK integration please comment below. :)\nHappy integration !\n"
},
{
	"uri": "https://majdarbash.github.io/tags/installation/",
	"title": "installation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/ubuntu-vsftpd-installation/",
	"title": "Installing VsFTPd on Ubuntu Server",
	"tags": ["ubuntu", "vsftpd", "ftp server", "ubuntu server"],
	"description": "",
	"content": "A small tutorial on how to install vsftpd and setup a separate user.\nPlease follow these steps to install vsftpd service on your ubuntu machine.\n# adding a new local user useradd -g ubuntu ftpuser # change the password of the new user passwd ftpuser # checking the user information id ftpuser # checking the user group groups ftpuser # changing the group of the user usermod -g ubuntu ftpuser # installing and configuring vsftpd sudo apt-get install vsftpd sudo vim /etc/vsftpd.conf # anonymous_enable=NO # local_enable=YES # write_enable=YES # chroot_local_user=NO§ # to display the hidden files using the ftp client: # hide_file=none # force_dot_files=YES # local_umask=000 sudo service vsftpd restart # pam_service_name=ftp # Enabling passive ports: # pasv_enable=YES # pasv_min_port=64000 # pasv_max_port=64321 # port_enable=YES # pasv_address=[your public ip] # pasv_addr_resolve=YES after this, make sure that the port range 64000 - 64321 is enabled in your amazon ec2 security groups or iptables\n# configuring ftp user home directory: sudo mkdir /home/ftpuser chown ftpuser:ubuntu /home/ftpuser  Connect using filezilla Use active ftp connection  Publish Date: 2014-09-10\n "
},
{
	"uri": "https://majdarbash.github.io/aws/scalable-gaming-patterns/",
	"title": "Introduction to Scalable Gaming Patterns on AWS (2017)",
	"tags": ["aws", "aws-advanced", "gaming-patterns"],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/hybrid-app-development-testing-on-google-chrome/",
	"title": "Ionic app testing - Cross domain issues",
	"tags": ["ionicframework", "browser", "testing", "crossdomain"],
	"description": "",
	"content": "I have encountered this issue several times while I was trying to run my hybrid mobile app on google chrome browser. In particular, an app based on ionic framework was used. Basically, I was building the HTML part of the hybrid app, using cordova and phonegap. The app is accessing the web service using jquery calls: $.ajax. The issue was that in order to test the app on my google chrome browser, I had to bypass the cross domain request restriction. I tried several extensions, however the ultimate way way just to run google chrome, while disabling the security switch. For whoever needs it:\n/Applications/Google Chrome.app/Contents/MacOS/Google Chrome --disable-web-security The same can be applied to the safari browser, using:\n/Applications/Safari.app/Contents/MacOS/Safari --disable-web-security  Publish Date: 2014-10-14\n "
},
{
	"uri": "https://majdarbash.github.io/tags/ionicframework/",
	"title": "ionicframework",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/js/",
	"title": "js",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/installing-apache2-mysql-php-on-ubuntu/",
	"title": "LAMP Installation on Ubuntu Server",
	"tags": ["ubuntu", "mysql", "apache2", "php", "installation"],
	"description": "",
	"content": "Found it useful to include these command I\u0026rsquo;m always using whenever I need to create an ec2 instance:\nsudo apt-get update sudo apt-get install apache2 sudo apt-get install libapache2-mod-php5 sudo apt-get install mysql-server libapache2-mod-auth-mysql php5-mysql # installing php / curl sudo apt-get install php5-curl other php modules sudo apt-get update sudo apt-cache search php5  Publish Date: 2014-10-04\n "
},
{
	"uri": "https://majdarbash.github.io/tags/levenshtein-distance/",
	"title": "levenshtein distance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/log/",
	"title": "log",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/machine-learning/",
	"title": "machine learning",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/microservices/",
	"title": "microservices",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/migrating-applications-to-aws/",
	"title": "Migrating Applications Running Relational Databases to AWS: Best Practices Guide (2019)",
	"tags": ["aws", "migration", "whitepaper", "relational-database"],
	"description": "",
	"content": " Migration Steps and Tools  Development Environment Setup Prerequisites Step 1: Migration Assessment Step 2: Schema Conversion Step 3: Conversion of Embedded SQL and Application Code Step 4: Data Migration Step 5: Testing Converted Code Step 6: Data Replication Step 7: Deployment to AWS and Go Live   Best Practices  Migration Steps and Tools  Migration assessment analysis Schema conversion to target database platform SQL statement and application code conversion Data migration Testing of converted database and application code Setting up replication and failover scenarios for data migration to the target platform Setting up monitoring for a new production environment and go live with the target environment  Tools to automate migration steps:\n AWS Schema Conversion Tool (AWS SCT) AWS Database Migration Service (AWS DMS)  Development Environment Setup Prerequisites  Most desirable for development env to mirror the production env Setup the AWS SCT on a server in the development environment Procure an Amazon RDS instance to serve as the migration target  Step 1: Migration Assessment Database Migration Assessment Report\n Identifies schema objects (tables, views, stored procedures, triggers, etc.) in the source database and the actions that are required to convert them (Action Items) to the target database (including fully automated conversion, small changes like selection of data types or attributes of tables, and rewrites of significant portions of the stored procedure) Recommends the best target engine, based on the source database and the features used Recommends other AWS services that can substitute for missing features Recommends unique features available in Amazon RDS that can save the customer licensing and other costs Recommends re-architecting for the cloud, for example sharding a large database into multiple Amazon RDS instances, such as sharding by customer or tenant, sharding by geography, or sharing by partition key  Report Sections:\n Executive summary  Key migration metrics, help you choose the best target engine   Conversion statistics graph  Visualizes the schema objects and number of conversion issues / complexity required in the migration project  Objects automatically converted Objects with simple actions (less than 1 hour) Objects with medium-complexity actions (1-4 hours) Objects with significant actions (4 hours or more)     Conversion action items  Detailed list with recommendations and references in the database code    Step 2: Schema Conversion Translation of DDL for the source database to the target database syntax\n Convert the schema Apply the schema to the target database  AWS SCT\n Automatically creates DDL scripts for objects that can be converted automatically Highlights objects that require manual intervention to convert to the target platform Supported languages:  Standard objects Application code in triggers PLSQL, T-SQL Simpler procedural languages of MySQL and PostgreSQL   Schema Mapping Rules  Create custom schema transformations and mapping rules during the conversion Standardize naming conventions Can be exported to be used by AWS DMS during Data Migration step Supported transformations:  Rename Add prefix Add suffix Remove prefix Remove suffix Replace prefix Replace suffix Convert uppercase (not available for columns) Convert lowercase (not available for columns) Move to (tables only) Change data type (columns only)      Step 3: Conversion of Embedded SQL and Application Code  Run an assessment report to understand the level of effort required to convert the application code to the target platform Analyze the code to extract embedded SQL statements Allow the AWS SCT to automatically convert as much code as possible Work through the remaining conversion Action Items manually Save code changes  AWS SCT to convert application code:\n Extract SQL statements from the surrounding application code Convert SQL statements  Embedded SQL conversion process:\n Analyze the selected code folder to extract embedded SQL Convert the SQL to the target script. If the AWS SCT is able to convert the script automatically, it appears in the lower right pane. Any manual conversion code can also be entered here. Apply the converted SQL to the source code base, swapping out the original snippet for the newly converted snippet. Save the changes to the source code. A backup of the original source code is saved to your AWS SCT working directory with an extension of .old. Click the green checkmark to the right of Parsed SQL Script to validate the Target SQL script against the target database.  Step 4: Data Migration AWS DMS process:\n Set up a replication instance (for the source DB) Define connections for the source and target databases Define data replication tasks  Supported Data Migrations:\n Full load of existing data Full load of existing data, followed by continuous replication of data changes to the target  Step 5: Testing Converted Code  Thoroughly test the migrated application Ensure correct functional behavior on the new platform Perform automated tests on the converted database Examine data rows affected by the tested Analyze data independently from application functionality Accounts for 45% of the overall migration effort  Exercising critical functionality in the application Verifying that converted SQL objects are functioning as intended    Step 6: Data Replication AWS DMS Change Data Capture (CDC) process:\n downtime for full load migration is not required implements ongoing replication from the source database to the target database no installation required neither on the source nor the target databases  CDC supported ongoing replication:\n Migrate existing data and replicate ongoing changes  Creating the target schema Migrating existing data and caching changes to existing data as it is migrated Applying those cached data changes until the database reaches a steady state Applying current data changes to the target as soon as they are received by replication instance   Replicate data changes only  Replicate data changes from specific point in time (in case schema and initial load is already completed)    Replication Sources:\n MS SQL Server  Replication - must be enabled on the source server and distribution database Transaction logs - source database should be in Full or Bulk Recovery Mode to enable transaction log backups   Oracle  BinaryReader or LogMiner ARCHIVELOG Supplemental Logging   PostgreSQL  Write-Ahead Logging (WAL) Primary Key   MySQL  Binary Logging Automatic backups   SAP ASE (Sybase)  Replication   MongoDB  Oplog   IBM Db2 LUW  LOGARCHMETH1 and LOGARCHMETH2 should be set to ON    Step 7: Deployment to AWS and Go Live  Enabled Validation option in the Task Settings of AWS DMS  AWS DMS validates the data migration by comparing the data in the source and target databases   Post-Deployment Monitoring  Table statistics pane Migration task metrics pane Amazon CloudWatch Logs  Enable CloudWatch for DMS Task in the Task Settings      Best Practices  Schema Conversion Best Practices  Save the Database Migration Assessment Report (CSV/PDF) Apply DDL in the following order to avoid dependency errors:  Sequences Tables Views Procedures   Configure the AWS SCT with the memory performance settings you need Apply the additional schema that AWS SCT creates to the target database aw_[source platform]_ext Use source code version control to track changes to target objects (both database and application code)   Application Code Conversion Best Practices  After running the initial application assessment report, save it as CSV/PDF   Data Migration Best Practices  Choose a replication instance class large enough to support your database size and transactional load  By default DMS loads 8 tables at a time. Performance can be increased on a larger instance.   On the target database, disable what isn\u0026rsquo;t needed  Disable unnecessary triggers, validation, foreign keys, secondary, jobs, backups and logging indexes on the target databases, if possible. Tables in the source database that do not participate in common transactions can be allocated to different tasks Monitor performance of the source system to ensure it is able to handle the load of the database migration tasks Enable logging using Amazon CloudWatch Logs - helps to troubleshoot DMS errors Optimize loading BLOBs, XML, or other binary data using Task Settings     Data Replication Best Practices  Achieve best performance by not applying indexes or foreign keys to the target database during the initial load - better write performance on the target database Apply indexes and foreign keys to the target database before the application is ready to go live For ongoing replication (such as for high availability), enable the Multi-AZ option of the replication instance - provides high availability and failover support for the replication instance. Use the AWS API or AWS CLI for more advanced AWS DMS task settings - more granular control over data replication tasks and additional settings that are not currently available in the AWS Management Console Disable backups on the target database during the full load for better performance. Enable them during cutover. Wait until cutover to make your target RDS instance Multi-AZ for better performance.   Testing Best Practices  Have a test environment where full regression tests of the original application can be conducted. In the absence of automated testing, run \u0026ldquo;smoke\u0026rdquo; tests on the old and new applications, comparing data values and UI functionality to ensure like behavior. Apply standard practices for database-drive software testing regardless of the migration process. Have sample test data that is used only for testing. Know your data logic and apply it to your test plans. Helps you produce comprehensive test data to cover mission-critical application functionality. Test using a dataset similar in size to the production dataset to expose performance bottlenecks, such as missing or non-performant indexes.   Deployment and Go Live Best Practices  Have a rollback plan in place Test the deployment on a staging or pre-production environment to ensure that all needed objects, libraries, code, etc., are included in the deployment and created in the correct order of dependency Verify that AWS DMS has reached a steady state and all existing data has been replicated to the new server before cutting off access to the old application in preparation for the cutover Verify that database maintenance jobs are in place, such as backups and index maintenance. Turn on Multi-AZ, if required Verify that monitoring is in place AWS provides several services to make deployments easier and trouble-free, such as CloudFormation, OpsWOrks and CodeDeploy.   Post-Deployment Monitoring Best Practices  Create CloudWatch Logs alarms and notifications to monitor for unusual database activity, and send alerts to notify production staff if the AWS instance is not performing well. Monitor logs and exception reports for unusual activity and errors. Determine if there are additional platform-specific metrics to capture and monitor. Monitor instance health.    "
},
{
	"uri": "https://majdarbash.github.io/tags/migration/",
	"title": "migration",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/multi-tenant-saas-storage/",
	"title": "Multi-Tenant SaaS Storage Strategies",
	"tags": [],
	"description": "",
	"content": " SaaS Paritioning Models  Silo Model Pool Model Bridge Model   Migration and Multitenancy Security Considerations Management and Monitoring Tiered Storage Models Developer Experience Linked Account Silo Model Multitenancy on DynamoDB  Silo Model Bridge Model Pool Model   Mutlitenancy on RDS  Silo Model Bridge Model Pool Model Single Instance Limits   Mutlitenancy on Amazon Redshift  Silo Model Bridge Model Pool Model   Agility  The goal is to find the best intersection of your storage and tenant partitioning needs. Consider how the strategy impacts your ability to build, deliver and deploy versions in zero downtime environment. Assess the regulatory, business and legacy dimensions of a given environment.\nSaaS Paritioning Models  Silo  Separate database for each tenant Addresses concerns on sharing infrastructure with other tenants Great for migration from existing solution to multi-tenant solution   Bridge  Single database, multiple schemas   Pool  Shared database, single schema Requires introduction of partitioning key to scope and control access to tenant data Fits with continuous delivery and agility goals that are essential to Saas providers     Silo Model Pros\n Compliance alignment No cross-tenant impacts Tenant-level tuning Tenant-level availability  Cons\n Compromised agility Centralized management Deployment complexity  Automating creation and configuring database on per-tenant basis adds a layer of complexity and a potential point of failure in your SaaS environment.   Cost  Pool Model Pros\n Agility Cost optimization Centralized management Simplified deployment  Cons\n Cross-tenant impacts Compliance challenges All or nothing availability  Bridge Model  Hybrid model combining pros and cons of both Silo and Pool model extremes.  Hybrid Silo/Pool Storage\n One possible solution is to build a solution that fully supports pooled storage as your foundation. Then you can carve out a separate database for those tenants that demand a siloed storage solution.   Migration and Multitenancy  Minimize invasive changes Favor data changes which have backward compatibility with earlier changes  Silo/Bridge Models\n Data can be migrated on tenant-by-tenant basis Allows careful migration of each SaaS tenant without exposing all tenants to possibility of migration error Introduces complexity into overall orchestration of your deployment lifecycle  Pool Model\n Easier migration process, all tenants are migrated at once Any migration error would impact all tenants  Security Considerations  Robust security strategy to ensure that tenant data is effectively protected from unauthorized access Adopting common security patterns supported by AWS  encrypt data at rest utilize IAM policies to limit access to tenant data  works great with Silo and Bridge model to limit database access in Pool model responsibility shifts to authorization models of your application\u0026rsquo;s services     Research on how isolation is achieved on each of the used AWS Services  Management and Monitoring  Building effective metrics and dashboard for aggregating storage trends  With siloed storage, data should be collected from each isolated database and aggregated in an aggregate model   Tenant-centric Views of Activity  represents the ability to drill down into tenant-centric storage activity Silo models align more naturaly with constructing this view Pool models will require some tenant-filtering mechanism   Policies and Alarms  More moving parts on a tenant-by-tenant basis will affect the complexity and manageability of your storage monitoring strategy Overall goal of the policies to set proactive rules to anticipate and react to health events    Tiered Storage Models  It\u0026rsquo;s not uncommon to find a spectrum of different storage solutions in use across the set of microservices that make up your application Storage can be used as another way to tier the SaaS solution Each tier can leverage a separate storage strategy, offering varying levels of performance and SLAs  Developer Experience  Developers typically introduce layers of frameworks that centralize and abstract away horizontal aspects of their applications Centralize and standardize policies and tenant resolution strategies Data access layer would inject tenant context into data access requests  Linked Account Silo Model  Need to provision separate Linked Account for each tenant Entire infrastructure of a tenant is isolated from other tenants Linked approach relies on Consolidated Billing More complex provisioning process Automate creation of each Linked Account and adjust any limits as needed AWS has constraints on the number of Linked Accounts - won\u0026rsquo;t be a good strategy for creating a large number of SaaS tenants  Multitenancy on DynamoDB  Schema-less nature of DDB makes migration easy  Silo Model  No notion of database instance, all tables are created globally in the region Requires grouping tables belonging to a single tenante, e.g. prefix by tenant identifier Access to the tables is controlled through IAM policies Provisioning process should automate generation of tables and IAM policies Tuning can be done on tenant-by-tenant basis  RCU and WCU, set on table level Amazon CloudWatch Metrics that are captured on table level   Number of tables can drastically grow in DDB with each microservice introducing new set of tables for each particular tenant Another approach to be considered is to have a single table for all data per tenant  Simplifies provisioning, management and migration profile of your solution     Bridge Model  Relaxing some isolation requirements through eliminating the introduction of any table-level IAM policies Removing IAM policies could simplify your provisioning scheme  Pool Model  For evenly distributed data across tenant performance optimization can be achieved by simply relying on underlying partioning scheme For SaaS environments which don\u0026rsquo;t have uniform multi-tenant data distribution you need to introduce a mechanism to better control the distribution of your data  One way would be to introduce shards per tenant and make it a parition key Gives us control on how much data a shard should contain and make the distribution of data uniform across partitions Tenants with large data footprint will be given more shards Introduces level of indirection that has to be addressed in data access layers (tenant-shard resolution)    Introducing a tenant lookup table:  Mutlitenancy on RDS Silo Model  Creating separate instances for each tenant Typically satisfies the compliance needs of customers without the overhead of provisioning entirely separate account   Bridge Model  Leverage a single instance for all tenants Create separate representation for each tenant Requires provisioning and runtime resolution for each tenant Requires adopting policies to limit schema changes Some RDS containers limit the number of database/schemas that you can create for an instance   Pool Model  Tenant data is stored in a single RDS instance Tenants share custom tables Tenant identifier is used to access each tenant\u0026rsquo;s data   Single Instance Limits  Storage Amount Limits  MySQL, MariaDB, Oracle, PostgreSQL - 6TB SQL Server - 4TB Aurora - 64TB   Consider sharding tenants data and distributing accross multiple instances  Mutlitenancy on Amazon Redshift  Focuses on building high-performance clusters to house large-scale data warehouses Places some limits on the constructs that you can create for each cluster  60 databases per cluster, 250 schemas per db, etc\u0026hellip;    Silo Model  Requires provisioning separate clusters for each tenant Access can be controlled and restricted using IAM policies and database priveleges Ability to create tuned experience per tenant Per tenant provisioning process adds extra complexity to your deployment footprint  Bridge Model  Create separate schemas for each tenant You will run into 256 limit with Redshift Redshift security grants all access to databases inside the cluster  SaaS application will be responsible for enforcing finer-grained access controls   The isolation profile of this solution is likely unacceptbale by customers  Pool Model  All tenants share databases and tables Overall management, monitoring and agility are improved by using a single Redshift Cluster Upper limit of 500 concurrent connections can be a bottleneck  SaaS developer defines an effective strategy to manage the connection, e.g. implementing client-based caching    Agility Storage technology and isolation model directly impacts your ability to easily deploy new features. Underlying storage model must accomodate the required changes without requiring downtime. The storage model picked today might not be a good fit for tomorrow.\n"
},
{
	"uri": "https://majdarbash.github.io/tags/mysql/",
	"title": "mysql",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/neural-networks/",
	"title": "neural networks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/nodejs/",
	"title": "nodejs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/node-js-npm-package-manager/",
	"title": "NodeJS 8: NPM - Package Manager",
	"tags": [],
	"description": "",
	"content": "NPM - Node Package Manager : npm makes it easy for JavaScript developers to share and reuse code, and it makes it easy to update the code that you\u0026rsquo;re sharing.\nThe official documention is available at : http://docs.npmjs.com\nhttps://www.npmjs.org - centralized repository of public modules\nNPM modes of operation global / local - Node modules can be installed locally or globally. Global installation of modules makes them available throughout any NodeJS Application you are writing on the same system. Local installation includes the module files into the same working directory and makes them available to your application (in the working directory). Default mode of operation is local -\u0026gt; doesn\u0026rsquo;t make system wide changes - always use the default local mode if you are in a doubt\nNPM - The Global Mode All packages will be installed into /usr/local/lib/node_modules, uses –g flag for Global mode. For example:\nnpm install -g sax\nin any script file, you can then use sax module by using var sax = require(\u0026lsquo;sax\u0026rsquo;);\nNPM - The Local Mode default mode, downloads the modules into node_modules directory of your current working directory. For example:\nnpm install sax\nNPM - Installing specific version of the module npm install sax@0.2.5 // will install a 0.2.5 version npm install sax@0.2.x // installs the latest release of the 0.2 branch npm install sax@\u0026ldquo;\u0026lt;0.3\u0026rdquo; // latest version before 0.3 npm install sax@\u0026ldquo;\u0026gt;=0.1.0 \u0026lt;0.3.1\u0026rdquo; // more complicated requirements\nNPM - Uninstalling a Module npm uninstall // removes a module locally npm -g uninstall // removes a module globally\nNPM - Updating a Module npm update // updates an installed module, if the package was not installed the command will install it npm update -g // update a globally installed module\nPackage.json Basically this can be considered as an alternative to composer.\npackage.json can be use this one to define the dependencies, which will be then installed by executing the following command in your working directory:\nnpm install\nexample of using package.json:\n{ \u0026ldquo;name\u0026rdquo; : \u0026ldquo;MyApp\u0026rdquo;, \u0026ldquo;version\u0026rdquo; : \u0026ldquo;1.0.0\u0026rdquo;, \u0026ldquo;dependencies\u0026rdquo; : { \u0026ldquo;sax\u0026rdquo; : \u0026ldquo;0.3.x\u0026rdquo;, \u0026ldquo;nano\u0026rdquo; : \u0026ldquo;*\u0026quot;, \u0026ldquo;request\u0026rdquo; : \u0026ldquo;\u0026gt;0.2.0\u0026rdquo; } }\n"
},
{
	"uri": "https://majdarbash.github.io/random/node-js-notes/",
	"title": "NodeJS 8: Overview",
	"tags": ["nodejs", "event-driven", "event emitter", "asynchronous programming", "concept"],
	"description": "",
	"content": "Introduction Node.js is an open source, cross-platform runtime environment for server-side and networking applications. Node.js is becoming more and more popular as NodeJS applications are written in JavaScript, and can be run within the Node.js runtime on any operating system.\nThe main features of Node JS area:\nEvent-driven / asynchronous programming\nInstead of retuning values for the functions and determining the program-flow, you basically define functions that are called by the system when an interesting event occurs (event callbacks).\nEvents are executed by an event loop - it\u0026rsquo;s a continuous loop which performs event detection and event triggering.\nEvent-driven programming is a programming style whereby the flow is determined by the occurrence of events. Programmers register callbacks to be used as event handlers for events they are interested in, and the system invokes these handlers when those events occur. This model of programming has some advantages over the traditional blocking paradigm where, to scale, you have to use multiple processes or threads.\nJavaScript is a powerful language, which is well suited for this style of programming, mainly because it has first-class functions and closures.\nCore Concept Defining feature of Node: Event-driven / asynchronous programming\nHere, instead of using the return value, you define functions that are called by the system when interesting event occurs (event callbacks). this is accompanied by an event loop - it\u0026rsquo;s a continuous loop which performs event detection and event triggering.\n\u0026ldquo;Event-driven programming is a programming style whereby the flow is determined by the occurrence of events. Programmers register callbacks to be used as event handlers for events they are interested in, and the system invokes these handlers when those events occur. This model of programming has some advantages over the traditional blocking paradigm where, to scale, you have to use multiple processes or threads.\nJavaScript is a powerful language, which is well suited for this style of programming, mainly because it has first-class functions and closures.\u0026rdquo;\nprints the message to the console console.log(\u0026lsquo;Hello World!');\nNPM (Node package manager) https://www.npmjs.org - centralized repository of public modules\nNPM modes of operation: global / local\nDefault - local, i.e. doesn\u0026rsquo;t make system wide changes - always use the default local mode if you are in a doubt.\nNPM - The Global Mode All packages will be installed into /usr/local/lib/node_modules, uses –g flag for Global mode.\n# insalling sax module globally sudo npm install -g sax\nNow in any script file, you can then use sax module by using var sax = require(\u0026lsquo;sax\u0026rsquo;);\nNPM - The Local Mode Default mode, downloads the modules into node_modules directory of your current working directory.\n# installing sax module using local mode sudo npm install sax\nNPM - Installing specific version of the module # install 0.2.5 version of sax module sudo npm install sax@0.2.5 # installs the latest release of the 0.2 branch sudo npm install sax@0.2.x\nlatest version before 0.3 sudo npm install sax@\u0026ldquo;\u0026lt;0.3\u0026rdquo;\n=0.1.0 \u0026lt;0.3.1\\\u0026rdquo; - more complicated requirement\u0026rdquo;}\u0026rdquo; data-sheets-userformat=\u0026rdquo;{\u0026ldquo;2\u0026rdquo;:641,\u0026ldquo;3\u0026rdquo;:{\u0026ldquo;1\u0026rdquo;:0},\u0026ldquo;10\u0026rdquo;:1,\u0026ldquo;12\u0026rdquo;:0}\u0026ldquo;\u0026gt;# you can have more complicated requirements sudo npm install sax@\u0026ldquo;\u0026gt;=0.1.0 \u0026lt;0.3.1\u0026rdquo;\nNPM - Uninstalling a Module # remove a module locally sudo npm uninstall # remove a module globally sudo npm -g uninstall\nNPM - Updating a Module # updates an installed module\nif the package was not installed the command will install it npm update # update a globally installed module \u0026ldquo;}\u0026rdquo; data-sheets-userformat=\u0026rdquo;{\u0026ldquo;2\u0026rdquo;:641,\u0026ldquo;3\u0026rdquo;:{\u0026ldquo;1\u0026rdquo;:0},\u0026ldquo;10\u0026rdquo;:1,\u0026ldquo;12\u0026rdquo;:0}\u0026ldquo;\u0026gt;npm update -g\nPackage.json You can use package.json one to define the dependencies and packages which you want to install. Later, you can use the \u0026ldquo;npm install\u0026rdquo; command which will read the contents of the package.json file and install accordingly.\nExample of package.json file:\n{ \u0026ldquo;name\u0026rdquo; : \u0026ldquo;MyApp\u0026rdquo;, \u0026ldquo;version\u0026rdquo; : \u0026ldquo;1.0.0\u0026rdquo;, \u0026ldquo;dependencies\u0026rdquo; : { \u0026ldquo;sax\u0026rdquo; : \u0026ldquo;0.3.x\u0026rdquo;, \u0026ldquo;nano\u0026rdquo; : \u0026ldquo;*\u0026quot;, \u0026ldquo;request\u0026rdquo; : \u0026ldquo;\u0026gt;0.2.0\u0026rdquo; } }\nLoading Modules You can load module by referencing: file path / name.\n# including the module in your code, which exposes it\u0026rsquo;s public API for use var module = require(\u0026lsquo;module_name\u0026rsquo;);\nExporting a Module # exports the instance type Circle function Circle(){ } module.exports = Circle;\n# exports function printA function printA(){} module.exports.printA = printA;\nLoading Core Modules # loads the core http module var http = require(\u0026lsquo;http\u0026rsquo;)\nUsing Buffers # creates a binary buffer from the utf-8 encoded string var buf = new Buffer(\u0026lsquo;Hello World!');\n# creates a binary buffer from base 64 encoded string # accepted encodings: base64, utf8, ascii var buf = new Buffer(\u0026lsquo;8b76fde713ce\u0026rsquo;, \u0026lsquo;base64\u0026rsquo;); # creating a 1024-byte buffer var buf = new Buffer(1024);\n# accessing an element within the buffer array of bytes console.log(buf[10]);\n# manipulating contents of the buffer (setting the byte at the position 12 to 125) buf[12] = 125\n# slicing a buffer - obtaing the bytes from the position 8 to 20 of the original buffer buf.slice(8, 20)\ncopying a buffer buf from bytes 5 to 10, to the buf2 starting from position byte 0 of the buf2 var buf2 = new Buffer(10); buf.copy(buf2, 0, 5, 10);\nbuf.copy(buf2, targetStart, sourceStart, sourceEnd);\nDecoding a Buffer # converting the buffer into a utf-8 encoded string var string = buf.toString();\n# converting the buffer into a base64 encoded string var b64Str = buf.toString(\u0026lsquo;base64\u0026rsquo;);\nEvent Emitter Pattern Event emitters allow programmers to subscribe to events they are interested in. Asynchronous programming uses the continuation-passing style (CPS) - where style takes as an argument an explicit continuation - a function of one argument.\nExample of callback after finishing reading the file:\nvar fs = require(\u0026lsquo;fs\u0026rsquo;); fs.readFile('/etc/passwd\u0026rsquo;, function(err, fileContent) { if (err) { throw err; } console.log(\u0026lsquo;file content\u0026rsquo;, fileContent.toString()); });\nResponse object is an event emitter here, and it can emit the data and end events:\nvar req = http.request(options, function(response) { response.on(\u0026ldquo;data\u0026rdquo;, function(data) { console.log(\u0026ldquo;some data from the response\u0026rdquo;, data); }); response.on(\u0026ldquo;end\u0026rdquo;, function() { console.log(\u0026ldquo;response ended\u0026rdquo;); }); }); req.end();\nUse CPS when you want to regain control after the requested operation completes and use the event emitter pattern when an event can happen multiple times.\nEvent Emitter API Any object that implements the event emitter pattern will implement a set of methods.\n.addListener and .on — To add an event listener to an event type\n.once - To attach an event listener to a given event type that will be called at most once\n.removeEventListener - To remove a specific event listener of a given event\n.removeAllEventListeners - To remove all event listeners of a given event type\nInheriting from Node Event Emitter Here we say that MyClass inherits from EventEmitter - i.e the method of MyClass will be able to emit events.\nutil = require(\u0026lsquo;util\u0026rsquo;); var EventEmitter = require(\u0026lsquo;events\u0026rsquo;).EventEmitter;\n// Here is the MyClass constructor: var MyClass = function() { }\nutil.inherits(MyClass, EventEmitter);\nMyClass.prototype.someMethod = function() { this.emit(\u0026ldquo;custom event\u0026rdquo;, \u0026ldquo;argument 1\u0026rdquo;, \u0026ldquo;argument 2\u0026rdquo;); };\nvar myInstance = new MyClass(); myInstance.on(\u0026lsquo;custom event\u0026rsquo;, function(str1, str2) { console.log(\u0026lsquo;got a custom event with the str1 %s and str2 %s!', str1, str2); });\n"
},
{
	"uri": "https://majdarbash.github.io/aws-csap/notes/",
	"title": "Notes",
	"tags": [],
	"description": "",
	"content": "Datastore\n Files stored in S3 can be served over Bittorrent to decrease costs File Gateway (Storage Gateway) can expose S3 bucket files in the office through NFS http://registry.opendata.aws contains publically open databases AWS Glue allows you to extract data from S3 Bucket to a Table that can be queried using AWS Athena Graph databases are best a storing complex relationship data and AWS Neptune is a graph database. While other options might be able to work, none would work as well as a true graph database and we can run such a database like SAP HANA or Neo4j on EC2. Secondary Indexes and DynamoDB Accelerator (DAX) - in-memory cache in front of DDB can accelerate DynamoDB performance Gateway Stored Volume Mode, or Volume Gateway Stored Mode as its also called, would be a way to maintain a full local copy of the data and have it replicated asynchronously to S3. Amazon ElastiCache offers a fully managed Memcached and Redis service. Although the name only suggests caching functionality, the Redis service in particular can offer a number of operations such as Pub/Sub, Sorted Sets and an In-Memory Data Store. However, Elasticache is only a key-value store and cannot therefore store relational data. A global secondary index can be used to speed up queries against non-primary key items. A local secondary index, on the other hand, must retain the partition key of the table. Hash key is another term for partition key. If you make a HEAD or GET request for the S3 key name before creating the object, S3 provides eventual consistency for read-after-write. As a result, we will get a 404 Not Found error until the upload is fully replicated. However, this replication usually only takes a few seconds and we might get the metadata after all. The ACID consistency model is Atomic, Consistent, Isolated and Durable.  Networking\n Only two components allow VPC to Internet communication using IPv6 addresses and those are \u0026ldquo;Internet Gateways\u0026rdquo; (inbound) and \u0026ldquo;Egress-Only Internet Gateways\u0026rdquo; (outbound). \u0026ldquo;NAT Instances\u0026rdquo; and \u0026ldquo;NAT Gateways\u0026rdquo; explicitly do not support IPv6 traffic and a \u0026ldquo;Direct Connection\u0026rdquo; carries data between a Data Centre and an AWS VPC, but does not travel over the Internet. You can use DHCP Option Sets to configure which DNS is issued via DHCP to instances. This can be any DNS address. So long as its reachable from the VPC, instances can use it to resolve.  Security\n By default, CloudTrail will log all regions and store them in a single S3 location. It can however be configured to only log specific regions. OAuth 2.0 provides authorization only. Service Control Policy is the best way to implement restriction on OU level for allowed regions.  ACLs and Resource-based policies apply to assets and not users or groups. Identity-based policies using the aws:RequestedRegion condition key could do the job but since we are trying to control at the OU level, an SCP would require less management and localized care. We can use a DENY with StringNotEqualsIfExists conditional against aws:ReqeustedRegion for allowed regions.   DDoS Layer 7 Attacks: The challenge with layer 7 detection is telling an attack from normal user traffic. CloudFront in conjunction with AWS WAF can be an effective way to create DDoS resilience at Layer 7. Network Load Balancers are NACLs are Layer 4 solutions, and would have no visibility of Layer 7 DDoS. CloudTrail and GuardDuty are focused on the security of the AWS account, and would not be suitable in isolation for securing at Layer 7 Further information: https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf  Business Continuity\n Via Aurora Global Database, an Aurora PostgreSQL database does support automatic failover to a secondary region. AWS does not recommend the use of RAID on EBS as it greatly affects the IOPS RAID0 offers no drive fault-tolerance. RAID1, also known as mirroring, requires 2x the required volume space. RAID5 requires 3 volumes at a minimum. Elasticache for Redis supports multi-AZ failover Recovery Point Objective will define the potential for data loss during a disaster. This can inform an expectation of manual data re-entry for BC planners. Redshift currently only supports single-AZ deployments but you can run multiple clusters in different AZs. Both spread placement groups and horizontal scaling spread risk across more resources. These are reasonable approaches if hardware failure is a concern. RAID0, sometimes known as striping, provides the highest write performance of these options because writes are distributed across disks and no parity is required.  Deployment and Operations\n CloudFormation Stack Policy should have \u0026ldquo;Allow\u0026rdquo; statement to whitelist what actions can be done Once applied, stack policy can be updated only using the CLI Continuous Deliver differs from Continuous Deployment in that Delivery still includes a manual check before release to production. A Canary Release is a way to introduce a new version of an application into production with limited exposure. AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. A Disposable Upgrade is one were a new release is deployed on new instances while instances containing the old version are terminated. AWS EKS runs the Kubernetes management platform for you on AWS across multiple AZs. Because its Kubernetes conformant, you can use third-party add-ons. Service Discovery makes it easy for containers within an ECS cluster to discover and connect with each other, using Route 53 endpoints. Task Definitions define the resource utilisation and configuration of tasks, using JSON templates. Task Scheduling allows you to run batch processing jobs run on a schedule. File Storage is not a component of ECS. Storage within ECS is handled by EBS volumes attached to the underlying EC2 instances and not by ECS itself. OpsWorks is a global service but when creating a stack you must specify a region and it will not allow you to clone to another region. AWS CodeDeploy does not provide Scaling or Provisioning of the deployment. Elastic Beanstalk, CloudFormation and OpsWorks can do this.  Cost Management\n The primary value proposition around cost for AWS is that it creates the opportunity for agility using a pay-as-you-go model. Traditional CapEx models make it difficult to quickly test new ideas. You should first get a solid understanding of current costs. It may turn out that a move to the cloud is not warranted even with financial evidence so the other activities would be waste. Bulk buys are almost always cheaper than on-demand, so RIs can be a good proxy. Managed services will be more cost-effective than just mimicking a pure on-prem server farm. Additionally, soft costs like agility or maintenance should be accounted for in the model. Tagging can be directly used for all of these purposes except Purchasing. However, indirectly, I could configure a CloudWatch event to trigger some action when a tag changes. That action might be a call to an API that places an order with a vendor. Right sizing is using the lowest cost resource that still meets the technical specifications of a specific workload. CloudWatch and Trusted Advisor are the most direct tools for this. Dedicated Hosts reserve capacity because you are paying for the whole physical server that cannot be allocated to anyone else. Dedicated Instances are available as on-demand, reserved and spot instances. Costs will most certainly increase during a migration given items like training, dual environments, lease penalties, consulting and planning. AWS calls this period the migration bubble. Regional RIs are not specific to an AZ and can be consumed across a region. Zonal RIs can be modified for use in another AZ using the console of ModifyReserveInstances API. Consolidated Billing is a feature of AWS Organizations. Once enabled and configured, you will receive a bill containing the costs and charges for all of the AWS accounts within the Organization. Although each of the individual AWS accounts are combined into a single bill, they can still be tracked individually and the cost data can be downloaded in a separate file. Using Consolidated Billing may ultimately reduce the amount you pay, as you may qualify for Volume Discounts. There is no charge for using Consolidated Billing. Consolidated Billing allows you to potentially realize lower prices on some services with tiered pricing. A buffering pattern is useful in smoothing demand. We can do this with SQS using FIFO to satisfy the in order requirement. If we solely use a spot fleet, we might be outbid and not have available instances. So, we can use a RI instead.  Areas of Focus:\n Fault Tolerance, High Availability, Disaster Recovery AWS Organizations, Security Compliance Policy AWS Support Plans AWS Trusted Advisor Direct Connect, VPN (Gateway)  "
},
{
	"uri": "https://majdarbash.github.io/tags/notice/",
	"title": "notice",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/osi-model/",
	"title": "OSI Model",
	"tags": [],
	"description": "",
	"content": " 7. Application 6. Presentation 5. Session 4. Transport 3. Network 2. Data Link 1. Physical  Overview:\n Stands for Open Systems Interconnection model 7-Layer Model Standardizes the communication functions of a telecommunication or computing system Conceptual model enables diverse communication systems to communicate using standard protocols  7. Application  Protocol Data Unit (PDU): Data Human-computer interaction layer, where applications can access the network services Examples: Web browsers (Chrome, Safari, IE), Apps: Skype, Zoom, Telnet, FTP  Functions of the layer:\n File transfer, access, and management (FTAM) Mail services Directory services  6. Presentation  Protocol Data Unit (PDU): Data Translates of application format to network format, and network format to application format Ensures that data is in usable format  Functions of the layer:\n Encryption Compression Encoding  5. Session  Protocol Data Unit (PDU): Data Maintains connections and is responsible for controlling ports and sessions  Functions of the layer:\n Dialog control Synchronization  4. Transport  Protocol Data Unit (PDU): Segment, Datagram Main responsibility is to transfer the data completely Transmits data using transmission protocols: TCP and UDP  Transmission Control Protocol (TCP)\n Standard protocol used for communication over the internet Establishes and maintains a connection between hosts Data is broken into segments, segments arrive at destination using different routes and are reordered Receipt acknowledgement  User Datagram Protocol (UDP)\n Receiver does not send any acknowledgement when the packet is received Sender does not wait for any acknowledgement Faster transmission than TCP, mostly used for Media streaming Unreliable due to potential packet loss  Functions of the layer:\n Service-point addressing - transmitting data from one computer to another, and then to the correct process - via port Segmentation and reassembly Connection control  Connection-oriented service  Connection oriented protocol makes a connection Checks whether message is received or not Sends again if an error occurs   Connection-less service  Does not guarantees a message delivery     Flow Control Error Control  3. Network  Protocol Data Unit (PDU): Packet Decides which physical path the data will take Determines the best path to move data from source to the destination based on the network conditions, the priority of service, and other factors Routers are the layer 3 devices, they are specified in this layer and used to provide the routing services within an internetwork  Functions of the layer:\n Internetworking Addressing Routing Packetizing  Network layer receives the packets from upper layers and converts them into packets This process is achieved by Internet Protocol (IP)    2. Data Link  Protocol Data Unit (PDU): Frame Defines the format of data on the network  Functions of the layer:\n Framing  Adds header and trailer to each frame   Physical Addressing Flow Control  Technique through which the constant data rate is maintained on both the sides so that no data get corrupted   Error Control  Calculated value CRC (Cyclic Redundancy Check) that is placed to the Data link layer\u0026rsquo;s trailer   Access Control  1. Physical  Protocol Data Unit (PDU): Bit, Symbol Transmits raw bit stream over the physical medium  Functions of the layer:\n Line Configuration Data Transmission  Defines transmission mode between the two devices on the network, i.e. simplex, half-duplex or full-duplex mode   Topology Signals  Type of the signal used for transmitting the information    "
},
{
	"uri": "https://majdarbash.github.io/tags/overview/",
	"title": "overview",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/password-protect/",
	"title": "password protect",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/elasticache-at-scale/",
	"title": "Performance at Scale with Amazon ElastiCache",
	"tags": [],
	"description": "",
	"content": " Memcached vs Redis ElastiCache for Memcached  Caching Design Patterns Consistent Caching (Sharding) Lazy Caching Write On Through Expiration Date The Thundering Herd Cache (Almost) Everything   Elastic Cache for Redis  Distributing Reads and Writes Mutli-AZ with Auto Failover Sharding with Redis Advanced Datasets with Redis   Monitoring and Tuning  Monitoring Cache Efficiency Watching for Hotspots Memory Optimization Redis Backup and Restore Cluster Scaling and Auto Discovery    Overview\n ElastiCache deploys one or more cache clusters for your application ElastiCache automates resources provisioning, failure detection and recovery, and software patching Supports Redis and Memcached engines  Alternatives to ElastiCache\n Amazon CloudFront - cache images, web pages, static data at the edge Amazon RDS Read Replicas - distributing data to remote apps On-host caching - this approach lacks of efficiency - cannot reuse existing cache entries and maintain consistency in validation of the cache keys across all hosts  Memcached vs Redis  Due to replication and persistence features of Redis, Redis is managed as relational database Memcached is designed as pure caching solution with no persistence - is managed as pool of nodes that can grow and shrink, similar to Amazon EC2 Auto Scaling Group  Important questions to consider impacting the choice of the caching engine:\nMemcached:\n Object caching as a primary goal? Offload database? Simplest caching model? Large cached nodes, multi-threaded performance with utilization of multiple cores Scale cache horizontally? Atomically increment / decrement counters?  Redis\n More advanced types, e.g. lists, hashes, bit arrays, HyperLogLogs and sets? Sorting and ranking datasets in memory? Pub/Sub capabilities in your application? Persistence of the key store? Run in multiple AZs with failover? Geospatial support? Encryption and compliance standards? PCI DSS, HIPAA, FedRAMP?  ElastiCache for Memcached  Considerably cheaper to add an in-memory cache then to scale up to a larger database cluster Easier to distribute an in-memory cache horizontally in comparison to relational database Choose the same AZs for ElastiCache as your application servers  Specify Preferred Zones option during cache cluster creation Spread Nodes Across Zones tells ElastiCache to distribute nodes to AZs evenly   Expect slightly higher latency for cross-zone AZ requests  Cache Node Size\n M5 or R5 families support the latest generation CPUs and networking capabilities Delivers up to 25Gbps of aggregate networking bandwidth with enhanced networking and over 600 GiB of memory M5.large single node can be a good starting point Track resource utilization through CloudWatch metrics Estimate the memory requirements by calculating the size consumed per cache item x number of items you want to cache  Security Groups and VPC\n ElastiCache supports security groups Advised to launch in a private subnet with no public connectivity Memcache doesn\u0026rsquo;t have any serious authentication or encryption capabilities Create a security group for ElastiCache cluster and allow traffic from \u0026ldquo;application tier\u0026rdquo; security group Test connectivity from an application instance to your cache cluster in VPC, using netcat:  nc -z w5 [cache endpoint] 11211 # will return 0 if connection was successful (the exist code of last command) echo $? Caching Design Patterns Some questions you need to think of:\n Is it safe to use a cached value? Is caching effective for that data? Is the data structured well for caching?  Problem Overview Objective: Splitting cache keys across multiple nodes to make use of multiple ElastiCache nodes\n Naive approach is to randomly distribute cache keys  Based on this approach hash key is generated from random CRC32 Node corresponding to hash key % (modulo) number of nodes will contain the key In the event of scaling, you will have to remap some keys, i.e. old count / new count If Scaling from 9 to 10 nodes, you will have to remap 90% of your keys Bad approach as scaling the nodes introduces more load on the database    Consistent Caching (Sharding)  Consistent Hashing  Alternative approach to spreading cache keys across your cache nodes Creating internal ring with a pre-allocated number of partitions that can hold hash keys There\u0026rsquo;s mathematical calculation involved to preallocate a set of random integers and assign cache nodes to the random integers In this case you find the closest integer in the ring for a given cache key and use the associated cache node Many Client Libraries support consistent hashing  Make sure that consistent hashing is enabled in the client library  For example in PHP: $memcached-\u0026gt;setOption(Memcached::OPT_LIBKETAMA_COMPATIBLE, true);   If possible use ElastiCache Clients with Auto Discovery to support Auto Discovery of new nodes as they are added to the cluster       Lazy Caching  Populate the cache only when an object is requested by the application Cache only contains objects that application requests, keeping the cache size managable Cache expiration is easily handled by deleting the cached object  Write On Through  Cache is updated realtime when the database is updated Advantages  Avoids cache misses Shifts any application delay to the write operation, which maps better with user expectations Simplifies cache expiration (cache is always up to date)   Disadvantages  Cache may be filled with unnecessary objects and may evict more frequently accessed objects out of cache If cache node fails you need to apply lazy cache mechanism to populate the cache    Expiration Date  Always apply TTL for all cache keys, except those updated by write-through caching For rapidly changing data add a TTL of few seconds to minimize the load on database Russian doll caching pattern: Nested records are managed with their own cache key and top-level resource is a collection of those cache keys When not sure, delete a cache key - Lazy Caching should refresh the key when needed  The Thundering Herd  The Thundering Herd effect happens when high number of users request the same piece of data with a cache miss Usually happens in highly concurrent environment This effect can also happen when adding a new cache node - as it has an empty memory Possible Solutions  Prewarm the cache using a script that hits a set of URLs  Prewarming can be automated by triggering the script to run whenever the app receives a cluster reconfiguration event through Amazon SNS   Add a bit of randomness to cache TTLs to mitigate simultaneous expiry event  ttl = 3600 + rand() * 120      Cache (Almost) Everything  Caching should be applied for the heavy queries of database Consider caching other less heavy queries as well, whenever appropriate Monitor cache misses to determine the effectiveness of your cache  Elastic Cache for Redis  Redis data structures cannot be sharded  Redis ElastiCache clusters are always a single node   Primary node can have one or more read replica  Replication group consists of a primary and up to five read replicas Number of replicas attached will affect the performance of primary node One or two read replicas in a different Availability Zone are sufficient for availability   With Multi-AZ enabled will automatically failover  Primary Endpoint is a DNS name of current Redis primary node In event of failover Primary Endpoint will be updated to point to new node   Supports persistence, backup and recovery  Distributing Reads and Writes  Requires configuring the application to write to primary endpoint and read from read replicas endpoint Read workloads can be separated from write workloads Read Replicas may return data slightly out of date from the primary node  There\u0026rsquo;s a short lag between the write operation to be reflected on the read replicas    Reading data from replica?\n Is the value being used only for display purposes? Is the value a cached value, for example a page fragment? Is the value being used on the screen where the user might have just edited it? - using outdated value will look like a bug Is the value being used for application logic? - using outdated value can be risky Are multiple processes using the value simultaneously, such as a lock or queue? - using outdated value can be risky  Mutli-AZ with Auto Failover  AWS ElastiCache will detect a failure of the primary node and transfer the primary endpoint to point into failover instance Failover process can take several minutes All production systems should use multi-az with auto-failover In case of the failover, the read replica selected to be promoted may be slightly behind master Slight data loss may be expected in case of rapidly changing data  Sharding with Redis  Simple keys and counters - support sharding Multidimensional sets, lists and hashes - don\u0026rsquo;t support sharding Redis client has to be configured to shard between redis clusters Horizontal sharding can be combined with split reads and writes  Sharded masters and sharded replicas   Designing the application to support read/write design in future you can add multiple clusters in future  Advanced Datasets with Redis Game Leaderboards\n Redis sorted sets simultaneously guarantee both uniqueness and ordering of elements Commnads start with Z, e.g. ZADD, ZRANGE, ZRANK On insertion element is reranked and assigned a numeric position  Recommendation Engines\n Some recommendation algorithmns, e.g. Slope One, require in-memory access to every item ever rated before by anyone in the system Data should be loaded in the memory to run the algorithm Redis counters can be used to increment / decrement the number of likes or dislikes for a given item Redis hashes can be used to manitain a list of everyone who liked / disliked an item Open source projects like Recommendify and Recommendable use Redis this way Persistence can be used to move keep this data in Redis  INCR \u0026quot;item:89287:likes\u0026quot; HSET \u0026quot;item:89287:ratings\u0026quot; \u0026quot;Susan\u0026quot; 1 INCR \u0026quot;item:89287:dislikes\u0026quot; HSET \u0026quot;item:89287:ratings\u0026quot; \u0026quot;Tommy\u0026quot; -1 Chat and Messaging\n Provides lightweight pub/sub mechanism Well-suited to simple chat and messaging needs In-app messaging, real-time comment streams Use PUBLISH and SUBSCRIBE commands Pub/sub messaging doesn\u0026rsquo;t get persisted to disk  You will loose the data if the cache node fails   Amazon SNS can be considered as a reliable alternative topic-based system  SUBSCRIBE \u0026quot;chat:15\u0026quot; PUBLISH \u0026quot;chat:15\u0026quot; \u0026quot;How are you?\u0026quot; UNSUBSCRIBE \u0026quot;chat:15\u0026quot; Queues\n Redis lists can be used to hold items in a queue When process picks up an item, item is pushed to in-progress queue and then deleted when the work is done Resque open source project (uses by Github) uses Redis as a queue. Redis queue has certain advantages  Very fast speed Once and only once delivery Guaranteed message orderding   ElastiCache for Redis backup and recovery options should be configured with Queue persistence in mind  Client Libraries and Consistent Hashing\n Redis client libraries support most popular programming languages Redis libraries rarely support consistent hashing as advanced types cannot be horizontally sharded Redis cannot be horizontally scaled easily Redis can only scale up to a larger node size, because its data structures must reside in a single memory image in order to perform properly  Monitoring and Tuning Monitoring Cache Efficiency  Use CloudWatch Metrics Watch CPU Usage  CPUUtilization EngineCPUUtilization Evictions  Large number of evictions indicates that your cache is running out of space   CacheMisses  Large number of CacheMissed combined with large number of Evictions indicates that the cache is thrashing due to the lack of memory   BytesUsedForCacheItems  Indicates the total amount of memory used by Memcahced / Redis. Both try to use as much memory as possible.   SwapUsage  In normal usage, neither Redis nor Memcached should be performing swaps.   Currconnections  An increasing number of connections might indicate a problem with your application. This value can be used as a threshold for alarm.     Scaling  For read intensive workloads, consider adding read replicas For write intensive workloads, consider adding more shards to distribute the workloads    Watching for Hotspots  Hotspots are nodes in your cache that receive higher load than other nodes Hotkeys - are cached keys that are access more frequently than others To investigate the hotspots is to track cache key access counts in application log  Will significantly affect performance, so should not be done unless you are very suspicious of hotspots One possible solution is to create a mapping table to remap very hot keys to separate set of cache nodes Another is to add additional layer of smaller caches in front of your main nodes to act as a buffer - gives more flexibility but introduces additional latency   Papers for researching on Hotspot issues:  Relieving Hot Spots on the World Wide Web Characterizing Load Imbalance in Real-World Networked Caches    Memory Optimization Memcached\n Uses slab allocator, allocates memory in fixed chunks When launching ElastiCache cluster, max_cache_memory parameter is set automatically chunk_size and chunk_size_growth_factor parameters work together to control how memory chunks are allocated  Redis\n Redis exposes a number of Redis configuraiton variables that will affect how Redis balances CPU and memory  Redis Backup and Restore  AWS automatically takes snapshots of your Redis Cluster and saves them to AWS S3 Redis backups require more memory to be available for the background Redis backup process For production - enable Redis backup with minimum 7 days retention  Cluster Scaling and Auto Discovery  AWS does not currently support auto-scaling Number of cluster nodes can be changed from AWS console/API During changing the cluster nodes, some of the cache keys will be remapped to new nodes - impacting performance of your application ElastiCache clients support auto-discovery of Memcached nodes Auto-discovery enables your application to auto-locate and connect to the Memcached nodes  Cluster Reconfiguration Events from Amazon SNS\n Your application can be configured to dynamically detect nodes being added or removed by reacting to Events through SNS ElastiCache:AddCacheNodeComplete and ElastiCache:RemoveCacheNodeComplete events are published when nodes are added and removed to the cluster Follow the steps:  Create AWS SNS topic for ElastiCache node additional and removal Modify application code to subscribe to the SNS topic When node is added or removed, re-run auto-discovery code to get the updated cache list Application adds the new list of cache nodes and reconfigured Memcached client accordingly    "
},
{
	"uri": "https://majdarbash.github.io/tags/php/",
	"title": "php",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/php-running-scripts-as-root/",
	"title": "PHP - Running scripts as root",
	"tags": [],
	"description": "",
	"content": "Very simple situation and very simple fix.\nWhether you are using apache of php-fpm, you need to grant your web server with sudo privilege. This can be done by adding the following line to /etc/sudoers file:\napache ALL=NOPASSWD: /usr/local/bin/my_command.sh\nDone !\n"
},
{
	"uri": "https://majdarbash.github.io/tags/programming/",
	"title": "programming",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/proxy/",
	"title": "proxy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/pybrain/",
	"title": "pybrain",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/python/",
	"title": "python",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-exercise-pyglatin-translator/",
	"title": "Python 2.7: Exercise: PygLatin Translator",
	"tags": [],
	"description": "",
	"content": "I was reviewing python tutorials in codeacademy which I really recommend as a great beginning to get yourself familiar with any language. These guys have got some interactive tutorials and exercises which makes it pleasure to start with something new.\nOf course you will need some broader sources and reference informaiton after completing the tutotrials in codeacademy. So I got this exercise from codeacademy - let\u0026rsquo;s see how we can solve it.\nProblem:\nNow let\u0026rsquo;s take what we\u0026rsquo;ve learned so far and write a Pig Latin translator.\nPig Latin is a language game, where you move the first letter of the word to the end and add \u0026ldquo;ay.\u0026rdquo; So \u0026ldquo;Python\u0026rdquo; becomes \u0026ldquo;ythonpay.\u0026rdquo; To write a Pig Latin translator in Python, here are the steps we\u0026rsquo;ll need to take:\n Ask the user to input a word in English. Make sure the user entered a valid word. Convert the word from English to Pig Latin. Display the translation result.  Solution:\ndef convert_to_pig_latin(word): first = word[0]\n# appending the string with the first letter and \u0026quot;ay\u0026quot; new\\_word = word + first + \u0026quot;ay\u0026quot; # removing the first character as it's already appended to the end new\\_word = new\\_word\\[1:len(new\\_word)\\] return new\\_word  input = raw_input(\u0026ldquo;Enter a word: \u0026ldquo;)\ninput should not be blank, and should consists of alpha characters if not (len(input) \u0026gt; 0 and input.isalpha()): print \u0026ldquo;This is not a word!\u0026rdquo; else: translated = convert_to_pig_latin(input) print \u0026ldquo;The word %s is translated to pig latin: %s\u0026rdquo; % (input, translated)\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-file-input-output/",
	"title": "Python 2.7: File Input / Output",
	"tags": [],
	"description": "",
	"content": "Writing to file Following example will populate the output.txt file with the squares of numbers from 1 - 10.\nmy_list = [i**2 for i in range(1,11)]\nGenerates a list of squares of the numbers 1 - 10 f = open(\u0026ldquo;output.txt\u0026rdquo;, \u0026ldquo;w\u0026rdquo;)\nfor item in my_list: f.write(str(item) + \u0026ldquo;\\n\u0026rdquo;)\nf.close()\nYou have probably noticed that we are opening the file in \u0026ldquo;w\u0026rdquo; mode, i.e. for writing. We can use \u0026ldquo;r+\u0026rdquo; flag if we would like to read and write from the file.\nIn this example, we are using with and as syntax. Using this statement python will execute the required operations on the file and will close it automatically:\nwith open(\u0026ldquo;output.txt\u0026rdquo;, \u0026ldquo;w\u0026rdquo;) as file: file.write(\u0026ldquo;File will be automatically closed!\u0026quot;)\nif not file.closed: print \u0026ldquo;Closing file \u0026hellip;\u0026rdquo; file.close() else: print \u0026ldquo;Skipping file closing, it was closed for us!\u0026rdquo;\nReading from file my_file = open(\u0026ldquo;output.txt\u0026rdquo;, \u0026ldquo;r\u0026rdquo;)\nwill output the contents of a single line while the pointer is and moves the pointer to the next line output: one line contents print my_file.readline()\noutput: the contents of output.txt file from the pointer location to the end of file - will move the pointer to the end of file print my_file.read()\nmy_file.close()\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-importing-modules/",
	"title": "Python 2.7: Importing Modules",
	"tags": [],
	"description": "",
	"content": "Modules make your code reusable and sharable amount different files. Module is a file that contains definitions - including variables and functions - that you can use once its imported.\nSome modules are built-in and will expose you to their functions once imported. Example below shows how sqrt function becomes available once the math module is imported:\n# importing math module import math print math.sqrt(25)\nimporting specific functions / variables frmo math module from math import sqrt print sqrt(25)\nimporting all the definitions form match module and unwrapping math. from math import * print sqrt(25)\nOf course, universal importing is not safe. You may fill your code with unnecessary variables and definitions which may conflict by using the same definition names between the modules.\nDisplays what is inside the module (all definitions in form of array of strings):\nimport math everything = dir(math) print everything\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-lambda-and-bitwise-operators/",
	"title": "Python 2.7: Lambda and Bitwise Operators",
	"tags": [],
	"description": "",
	"content": "Lambda functions Using lambda functions we can create functions in runtime, and use them as we go. Here\u0026rsquo;s an example of lambda function and how it is used to filter the array. filter() function takes the lambda function as the first parameter, passes items of the list sequentially and filters the list to retain the items which pass the filtration function passed as first argument.\n# will assign a range of numbers from 0 to 15 to numbers list numbers = range(16)\noutput: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] print numbers\nfiltering only numbers which can be divided by 3 filtered_numbers = filter(lambda x: x % 3 == 0, numbers)\noutput: [0, 3, 6, 9, 12, 15] print filtered_numbers\nBitwise Operators Bitwise operators directly manipulate bits. Python provides certain set of operatos. Example:\nprint 5 \u0026raquo; 4 # Right Shift print 5 \u0026laquo; 1 # Left Shift print 8 \u0026amp; 5 # Bitwise AND print 9 | 4 # Bitwise OR print 12 ^ 42 # Bitwise XOR print ~88 # Bitwise NOT\noutput: 0 output: 10 output: 0 output: 13 output: 38 output: -89 Base 2 number system # convert binary to decimal print 0b1 # output 1 print 0b10 # output :2 print 0b11 # output: 3 print 0b10 + 0b01 # output: 3\nconvert decimal to binary output:0b1111 print bin(15) convert decimal to base-8 output: 017 print oct(15)\nconvert decimal to base-16 / hexedecimal output: 0xe print oct(14)\nConverting string to number # converting string from base 10 number\noutput: 10 print int(\u0026ldquo;10\u0026rdquo;)\nconverting string from base 2 number output: 2 print int(\u0026ldquo;10\u0026rdquo;, 2)\nconverting string from base 16 number output: 204 print int(\u0026ldquo;cc\u0026rdquo;, 16)\nOther Operators # bitwise AND\noutput: 0b001 print bin(0b101 \u0026amp; 0b011)\nbitwise OR output: 0b111 print bin(0b101 | 0b011)\nbitwise XOR output: 0b110 print bin(0b101 ^ 0b011)\nbitwise NOT flips all the bits in a number output: -0b110 print bin(~0b101)\nCheck if the bit #5 is on or off def check_bit5(number): if number \u0026amp; 0b10000 \u0026gt; 0: return \u0026lsquo;on\u0026rsquo; else: return \u0026lsquo;off\u0026rsquo;\noutput: on print check_bit5(0b111101)\noutput: off print check_bit5(0b101101)\nFlipping the nth bit in a number # will flip the nth bit in the number def flip_bit(number, n): mask = 1 \u0026laquo; (n - 1) result = number ^ mask return bin(result)\nwill flip the 3rd bit in the number output: 0b100100 print flip_bit(0b100000, 3)\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-lists-dictionaries/",
	"title": "Python 2.7: Lists &amp; Dictionaries",
	"tags": [],
	"description": "",
	"content": "Lists Lists are the same as arrays in other programming languages. List can be defined using assigment:\nfruits = [\u0026ldquo;banana\u0026rdquo;, \u0026ldquo;apple\u0026rdquo;, \u0026ldquo;strawberry\u0026rdquo;]\nand can be accessed by indices, similar to any other programming language:\n# output: strawberry print fruits[2]\nsubstitution of existing element will replace apple with kiwi in fruits list fruits[1] = \u0026ldquo;kiwi\u0026rdquo;\ndefining an empty list:\nvegetables = []\nappending items to the existing list:\nvegetables.append(\u0026ldquo;cucumber\u0026rdquo;) vegetables.append(\u0026ldquo;eggplant\u0026rdquo;)\nobtaining the length of the list, i.e. number of elements in the list:\nprint len(vegetables)\nList slicing You can extract certain chunks of the list, using \u0026ldquo;: \u0026quot; syntax. You have indicate from and to index, to represent the index range.\n# output: cucumber, eggplant print vegetables[0:2]\noutput: apple, strawberry print fruits[1:3]\nanimals = \u0026ldquo;catdogfrog\u0026rdquo; cat = animals[:3] # slicing elements until the third dog = animals[3:6] # slicing based on range frog = animals[6:] # slicing elements starting from 7th\nSearching through the list You can search through the list using .index(item).\nfruits = [\u0026lsquo;apple\u0026rsquo;, \u0026lsquo;kiwi\u0026rsquo;, \u0026lsquo;banana\u0026rsquo;, \u0026lsquo;peach\u0026rsquo;]\noutput: 1 - i.e. index of element kiwi kiwi_index = fruits.index(\u0026lsquo;kiwi\u0026rsquo;) print \u0026ldquo;Kiwi found at position %s\u0026rdquo; % kiwi_index\ninsert watermelon in the kiwi position, pushing everything down fruits.insert(kiwi_index, \u0026lsquo;watermelon\u0026rsquo;)\noutput: apple, watermelon, kiwi, banana, peach print fruits\nTraversing through the list You can traverse through the list using the for loop.\n# output: 2,4,6,8,10 my_list = [1,2,3,4,5] for number in my_list: print 2 * number\nmy_list = [5,6,3,2,1] for i in range(0, len(my_list)): print my_list[i]\nRemove specific item from the list: furniture = [\u0026lsquo;sofa\u0026rsquo;, \u0026lsquo;chair\u0026rsquo;, \u0026lsquo;table\u0026rsquo;, \u0026lsquo;bed\u0026rsquo;] furniture.remove(\u0026lsquo;chair\u0026rsquo;)\noutput: sofa, table, bed print furniture\nOther list functions # sorting array in ascending order\noutput: 1,2,3,5,8 my_list = [2,5,1,3,8] my_list.sort() print my_list\nusing .pop(index) - returns element from the list and removes it furniture = [\u0026lsquo;sofa\u0026rsquo;, \u0026lsquo;chair\u0026rsquo;, \u0026lsquo;table\u0026rsquo;, \u0026lsquo;bed\u0026rsquo;] item = furniture.pop(2)\noutput: table print item\noutput: sofa, chair, bed print furniture\n# output: list of numbers from 2 to 8, excluding 8\noutput: 2,3,4,5,6,7 list = range(2, 8) print list\nJoining lists\nLists can be joined using summation operator\nn = [1,2,3] m = [7,4,5]\ndef join_lists(x, y): return x + y\noutput: 1,2,3,7,4,5 print join_lists(n, m)\nAlso list can be multiplied by a certain number and that it will join with itself several times\nm = [\u0026ldquo;A\u0026rdquo;]\noutput: [\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;,\u0026ldquo;A\u0026rdquo;] print m * 10\nList elements can be \u0026ldquo;imploded\u0026rdquo; to a string with a specified separator using .join(list) function.\nmy_list = [\u0026ldquo;I\u0026rdquo;, \u0026ldquo;believe\u0026rdquo;, \u0026ldquo;I\u0026rdquo;, \u0026ldquo;can\u0026rdquo;, \u0026ldquo;fly\u0026rdquo;] joint_string = \u0026quot; \u0026ldquo;.join(my_list)\noutput: \u0026ldquo;I believe I can fly\u0026rdquo; print joint_string\nCasting list elements to other data types\nmy_list_int = [1,2,3]\nwill return the same array of int, but casted to str my_list_str = map(str, my_list_int)\nCheck if number is a member of the list:\nimport random my_list = [] my_list.append(random.randint(0,10)) my_list.append(random.randint(0,10)) my_list.append(random.randint(0,10))\nguess_number = int(raw_input(\u0026ldquo;Guess a number between 0 and 10: \u0026ldquo;))\nif guess_number not in range(0,10): print \u0026ldquo;The number is even not between 0 and 10\u0026rdquo; else: if guess_number in my_list: print \u0026ldquo;Congratulations, you found the number!\u0026rdquo; else: print \u0026ldquo;Sorry, you missed it!\u0026rdquo; print \u0026quot; \u0026ldquo;.join(map(str, my_list))\nDictionaries Dictionaries are similar to associative arrays. They consist of key-value pairs.\nscore = {\u0026ldquo;James\u0026rdquo;: 12, \u0026ldquo;Samantha\u0026rdquo;: 43, \u0026ldquo;Andre\u0026rdquo;: 81}\noutput: 12 print score[\u0026ldquo;James\u0026rdquo;]\nIn contrast to lists which are enclosed by [] brackes, dictionaries are enclosed by curly braces {}.\nElements can be appened to dictionaries using dictionary[key] assignment\nscore[\u0026ldquo;Kate\u0026rdquo;] = 34\noutput: 4 print len(score)\nRemoving items from the dictionary:\nscore[\u0026ldquo;Rene\u0026rdquo;] = 29\noutput: 5 print len(score)\ndel score[\u0026ldquo;Rene\u0026rdquo;]\noutput: 4 print len(score)\nTraversing through the dictionary # output: Price of apple is 32.5\noutput: Price of potato is 20 prices = {\u0026ldquo;potato\u0026rdquo;: 20, \u0026ldquo;apple\u0026rdquo;: 32.5} for key in prices: print \u0026ldquo;Price of %s is %s\u0026rdquo; % (key, prices[key])\n"
},
{
	"uri": "https://majdarbash.github.io/python-2.7/python-loops/",
	"title": "Python 2.7: Loops",
	"tags": [],
	"description": "",
	"content": "While Loops count = 0\noutput: 0,1,2,3,4 while count \u0026lt; 5: print count count += 1\ncouple of empty lines print print\ncount = 0 while count \u0026lt; 5: if count == 3: count +=1 continue print count count += 1\noutput: 0,1,2,4 - will skip the number 3 print print\ncount = 0 while True: if count == 8: break print \u0026ldquo;Count %s\u0026rdquo; % count count += 1\nPython supports while \u0026hellip; else structure. In this case the else block will execute when loop condition is evaluated to False - this means the while block never entered or the loop exited normally after the loop condition was evaluated to False. However python will not execute the else block if the loop exit is due to break.\ncount = 0 while count \u0026lt; 5: count += 1 print count else: print \u0026ldquo;Done!\u0026rdquo;\nFor Loops # output: 0,1,2,3 for i in range(4): print i\nLooping through the list fridge = [\u0026ldquo;chicken\u0026rdquo;, \u0026ldquo;tomato\u0026rdquo;, \u0026ldquo;pasta\u0026rdquo;]\noutput: chicken, tomato, pasta for item in fridge: print item\nLooping through the dictionary menu = { \u0026lsquo;fajita\u0026rsquo;: 23.5, \u0026lsquo;chicken alfredo\u0026rsquo;: 45, \u0026lsquo;fish platter\u0026rsquo;: 34 }\nprint \u0026ldquo;Welcome to our restaurant, check our menu:\u0026rdquo; for key in menu: print key + \u0026quot; \u0026quot; + str(menu[key])\nEnumerate function Using for loop to loop through a list you wouldn\u0026rsquo;t know the index of the element you are accessing at each iteration. Enumarate function will help us with this, as it will supply a corresponding index to each element in the list that you pass it.\nchoices = [\u0026lsquo;pasta\u0026rsquo;, \u0026lsquo;pizza\u0026rsquo;, \u0026lsquo;chicken alfredo\u0026rsquo;]\nbasic for loop for item in choices: print \u0026ldquo;Item %s\u0026rdquo; % item\nlooping using enumerate function for index, item in enumerate(choices): print \u0026ldquo;Item %s: %s\u0026rdquo; % (index, item)\nMultiple lists Zip function will create pairs of elemnts when passed two lists. It will stop at the end of shorter list.\nlist1 = [1,2,3,4,5] list2 = [4,6,2]\noutput: 1,4 output: 2,6 output: 3,2 for el1, el2 in zip(list1, list2): print el1, el2\nFor / else For loop may end with else. Else clause will be executed only when the for loop ends normally, without break.\nlist = [3,5,2,1]\nfor number in list: print number else: print \u0026lsquo;all numbers were printed\u0026rsquo;\nDictionary functions: items(), keys(), values() employees = { \u0026ldquo;John\u0026rdquo;: \u0026ldquo;Manager\u0026rdquo;, \u0026ldquo;Bob\u0026rdquo;: \u0026ldquo;Assistant\u0026rdquo;, \u0026ldquo;Kate\u0026rdquo;: \u0026ldquo;Secretary\u0026rdquo; }\noutput: the list of key, value pairs output: [(\u0026lsquo;Bob\u0026rsquo;, \u0026lsquo;Assistant\u0026rsquo;), (\u0026lsquo;John\u0026rsquo;, \u0026lsquo;Manager\u0026rsquo;), (\u0026lsquo;Kate\u0026rsquo;, \u0026lsquo;Secretary\u0026rsquo;)] print employees.items()\noutput: [\u0026lsquo;Bob\u0026rsquo;, \u0026lsquo;John\u0026rsquo;, \u0026lsquo;Kate\u0026rsquo;] print employees.keys()\noutput: [\u0026lsquo;Assistant\u0026rsquo;, \u0026lsquo;Manager\u0026rsquo;, \u0026lsquo;Secretary\u0026rsquo;] print employees.values()\nBuilding Lists # output: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] numbers = range(1,11) print numbers\noutput: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] numbers = [x for x in range(1,11)] print numbers\noutput: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] numbers = [x ** 2 for x in range(1,11)] print numbers\noutput:[4, 16, 36, 64, 100] numbers = [x ** 2 for x in range(1, 11) if x % 2 == 0] print numbers\nSlicing Lists Syntax: [start:end:stride]\nstart - where the slicing starts (inclusive)\nend - where the slicing ends (exclusive)\nstride - space between items in the sliced list (i.e. step)\npositive stride length - traverses the list from left to right\nnegative stride length - traverses the list from right to left\nnumbers = [1,3,4,5,6,7,8,9,10]\noutput: [5, 6, 7, 8, 9, 10] print numbers[3:]\noutput: every second element output: [1, 4, 6, 8, 10] print numbers[::2]\noutput: [5, 6, 7] print numbers[3:6]\noutput: [5, 7, 9] print numbers[3:9:2]\noutput: [10, 9, 8, 7, 6, 5, 4, 3, 1] reversed = numbers[::-1] print reversed\n"
},
{
	"uri": "https://majdarbash.github.io/random/python-for-data-science/",
	"title": "Python for Data Science",
	"tags": ["data-science", "python"],
	"description": "",
	"content": "ipython - Interactive Python Interpreter\nVariables  type(var_name) returns the type of the variable, e.g. float, string, \u0026hellip;  Type Conversion  str(var_name) float(var_name)  Math  Exponentiation 4**2 will give 16 Modulo 18 % 7 will give 4  Generating Output  print(string)  Publish Date: 2019-12-02\n "
},
{
	"uri": "https://majdarbash.github.io/tags/ready/",
	"title": "ready",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/recovery/",
	"title": "recovery",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/reinforced/",
	"title": "reinforced",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/relational-database/",
	"title": "relational-database",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/remove-svn-files-unverison-subversion-working-copy/",
	"title": "Remove .svn files - unverison subversion working copy",
	"tags": ["subversion", "version control system", "vcs", ".svn files"],
	"description": "",
	"content": "In order to remove .svn files you need to execute the following command on your terminal:\nfind . -name .svn -exec rm -rf {};  Publish Date: 2015-01-02\n "
},
{
	"uri": "https://majdarbash.github.io/tags/resiliency/",
	"title": "resiliency",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/review/",
	"title": "review",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/deploybot-com-review/",
	"title": "Review of DeployBot.com",
	"tags": [],
	"description": "",
	"content": "There\u0026rsquo;s tons of deployment tools available these days. Deploybot.com (previously known as Dploy.io) is one of the nice tools I used since a while.\nBasically the tool offered as capability to configure SSH connection to statically defined instances, perform code upload, apply configuration, run before and after install scripts and trigger other deployments. In addition they provide what they call \u0026ldquo;Atomic\u0026rdquo; deployment. This assures zero downtime and code consistency throughout the deployment nodes as the document root is switched from one release to another using softlink.\nRecently deploybot.com launched more tools like running before deployment script in custom build environment. You can actually choose your required environment: PHP, NodeJS etc.. and run your build generation scripts. Afterwards the build will be deployed to the instances directly.\nIf something is not right you can rollback to the previous version. The easy to use interface allows you to view the git commit difference being deployed in each release. You don\u0026rsquo;t need to manually track it - your deployment history can be easily accessed. The nice feature I found is ability to create user accounts with different access levels, e.g. View only, View and Deploy or Full Access.\nPost deployment we usually like to be informed on Slack. There\u0026rsquo;s nothing easier then adding slack webhook into DeployBot.com and you get instance updates.\nWith DeployBot.com you can deploy directly to AmazonS3 buckets and they are expanding their AWS integrations. Unless you are using AWS Auto-Scaling, utilizing tools like Jenkins, CodePipeline and CodeDeploy, DeployBot.com is one of the tools which may be useful to consider\nPublish Date: 2013-04-09\n "
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/route53/",
	"title": "Route53",
	"tags": [],
	"description": "",
	"content": " Route53 gets its name from port 53 of DNS server IPv4 space is 32 bit fields having over 4 billion different addresses. IPv6 solves the depletion issue and has as address space of 128 bits For each public hosted zone Route 53 automatically created NS record and SOA record (Start of Authority Record) NS Record - is used by Top Level Domain servers to direct traffic to the Content DNS server which contains the authoritative DNS records When creating a public hosted zone, Route 53 gives you NS records in different Top-Level Domains. The start of authority (SOA) record identifies the based DNS information about the domain. SOA record components  NS that created the SOA record Email of the administrator The current version of the data file The default number of seconds for the time-to-live file on resource records   Domain to IP request flow  Top Level Domain NS Records SOA   The lower the TTL (time to live), the faster changes to DNS records take to propagate through the internet CNAME records can\u0026rsquo;t be used for naked domain names (zone apex record). It must be either A record or an Alias Given a choice between Alias record and CNAME always choose an Alias record Common DNS Types  SOA Records NS Records A Records CNAMES MX Records PTR Records   You can buy domain names directly with AWS It can take up to 3 days to register depending on the circumstances Routing Policies  Simple Routing\nYou can only have one record with multiple IP addresses. If you specify multiple values in a record, Route 53 returns all values to the user in a random order.\nSimple routing cannot be associated with a health check. Weighted Routing\nTraffic can be split based on different weights assigned. Latency-based Routing\nAllows you to route your traffic based on the lowest network latency for your end user. Failover Routing  Active/passive set up Specifying primary and secondary records Health check is associated with the primary record   Geolocation Routing\nLets you choose where your traffic will be sent based on the geographic location of your users.  This is not a latency based routing - the routes are locked down based on geolocation.   Geoproximity Routing (Traffic Flow Only)\nGeorproximity routing lets Route53 route traffic to your resources based on the geographic location of your users and your resources. You can optionally choose to route more traffic or less to a given resource by specifying a value, know as bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource. To use this you must use Route53 Traffic flow. Multivalue Answer Routing\nExactly the same as Simple Routing, however, allows you to use health checks on each record. Route53 will return values for healthy resources.   Health Checks  You can set health checks on individual record sets If a record set fails a health check it will be removed from Route53 until it passes the health check You can set SNS notifications to alert you if a health check is failed   Health Checks can be created and associated with Route 53 records  FAQs  Route 53 is built using AWS\u0026rsquo;s highly available and reliable infrastructure. Each Amazon Route 53 hosted zone is served by its own set of virtual DNS servers. They are assigned by the system when the hosted zone is created. Amazon Route 53 charges are based on actual usage of the service for Hosted Zones, Queries, Health Checks, and Domain Names. Access to Route53 can be controlled using IAM. You can configure Amazon Route 53 to log information about the queries that Amazon Route 53 receives including date-time stamp, domain name, query type, location etc. to CloudWatch Logs. Amazon Route53 uses anycast network - is a networking and routing technology that helps your end users\u0026rsquo; DNS queries get asnwered from the optimal Route53 location given network conditions. Route53 account is limited to a maximum of 500 hosted zones and 10,000 resource record sets per hosted zone and 50 domains. Multiple hosted zones can be created for domain. In addition to standard record types supported by Route53, alias records are supported, which is Route 53-specific extension to DNS. Alias can be used to map your entires to AWS Resources. Wildcard entries are supported. Zone apex can be mapped to AWS Resources by using alias records. Traffic Flow makes it easy for developers to create policies that route traffic based on constraints like: latency, endpoint health, multivalue; answers, weighted round robin, and geo. In addition to these, Traffic Flow also supports geoproximity based routing with traffic biasing. Traffic Policy is the set of rules that routes end users\u0026rsquo; request to one of the application\u0026rsquo;s endpoints. Policy Record associates the traffic policy with the appropriate DNS name within an Amazon Route 53 hosted zone that you own. You are billed from Traffic Flow per Policy Record. Private DNS should be attached to a VPC Route53 Health checks could verify the expected content of the web server by using the \u0026ldquo;Enable String Matching\u0026rdquo; option. Amazon Route 53’s metric based health checks let you perform DNS failover based on any metric that is available within Amazon CloudWatch, including AWS-provided metrics and custom metrics from your own application. Domains registered are configured to renewal automatically. Route53 provides privacy protection at no additional charge. Route53 registers top-level domains through either Amazon Registar or Gandi. Route53 Resolver is a regional DNS service that provides recursive DNS lookups for names hosted in EC2 as well as public names on the internet. Route 53 is Authoritative and Recursive DNS. Authoritative DNS - contains the final answer to a DNS query. Recursive DNS - forwards the query directly to a specific recursive DNS server.  "
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/s3/",
	"title": "S3",
	"tags": [],
	"description": "",
	"content": " Billing S3 (Simple Storage Service) Usage Patterns Storage Classes S3 Glacier S3 Billing Access \u0026amp; Encryption Versioning Lifecycle Management Tools \u0026amp; Glacier Cross Region Replication Amazon S3 Transfer Acceleration Amazon S3 Notifications CloudFront Snowball Storage Gateway FAQs  Billing  Billing Alarms can be created from CloudWatch. Billing Notifications should be enabled from the Billing Preferences section.  S3 (Simple Storage Service)  Provides developers and IT teams with secure, durable, highly scalable object storage Provides simple web services interface to store and retrieve data S3 is a safe place to store the files S3 and Glacier are not block storages S3 is Object-Based - allows you to upload files Files can be 0 bytes to 5TB Successful uploads will generate an HTTP 200 code Unlimited storage Files are stored in Buckets Objects consist of:  Key (name of the object) Value (the data) Version ID (important for versioning) Metadata (data about data you are storing) Subresources  Access Control List Torrent     Files can be from 0 Bytes to 5TB There is unlimited storage Files are stored in Buckets S3 is a universal namespace. Names must be unique globally. Data Consistency  Read after Write consistency for PUTS of new Objects\nIf you write a new file and read it immediately after, you will be able to view that data Eventual Consistency for overwrite PUTS and DELETEs (can take some time to propagate)\nIf you update an existing file or delete a file and read it immediately, you may get the older version, or you may not. Changes to objects can take a little bit of time to propagate.   Tiered Storage Available Lifecycle Management Versioning Encryption MFA Delete Secure your data using Access Control Lists and Bucket Policies Support BitTorrent peer-to-peer protocol  Allows cost saving when distributing content at high scale   Amazon S3 can be paired with Amazon CloudSearch / DynamoDB or RDS for ease of querying metadata and locating the object reference.  Usage Patterns  Store and distribute static web content and media Host entire static website Data store for computation and large-scale analytics, allowing concurrent access to multiple computing nodes Highly durable, scalable and secure solution for backup and archiving of critical data  Storage Classes  S3 Standard  99.99% availability 99.999999999% durability for S3 information. (11x9s) Stored redundantly across multiple devices in multiple facilities Designed to sustain a loss of 2 facilities concurrently   S3 - IA (Infrequently Accessed)  For data that is accessed less frequently but requires rapid access when needed Lower fee that S3, but you are charged a retrieval fee   S3 One Zone - IA (Infrequently Accessed, was called before RRS - Reduced Redundancy Storage)  Lower-cost option for infrequently accessed data, but do not require the multiple Availability Zone data resilience.   S3 - Intelligent Tiering  Uses machine learning Optimizes costs automatically by moving data to the most cost-effective access tier, without performance impact or operational overhead   S3 Glacier  secure, durable and low-cost storage class for backup and data archiving retrieval times are configurable from minutes to hours retrieval puts a copy of retrieved object in S3 Reduced Redundancy Storage (RRS) for a specified retention period (original object remains in Glacier) expedited, standard and bulk retrievals data is encrypted by default   S3 Glacier Deep Archive  lowest-cost storage class where a retrieval time of 12 hours is acceptable    S3 Glacier  Single Archive Limited to 40TB in size There\u0026rsquo;s no limit on total amount of data you can store in S3 Glacier Vaults can be locked by using lockable policies  You can specify \u0026ldquo;undeletable records\u0026rdquo; or \u0026ldquo;time-based data retention\u0026rdquo; in \u0026ldquo;Glacier Vault Lock\u0026rdquo; policy After policy is locked it becomes immutable and Amazon Glacier enforces the controls to help achieve compliance objectives   Can be integrated with CloudTrail to help control access Can be interfaces using REST web services, or as a storage class in S3 Objects archived to Glacier using S3 Lifecycle policies can be accessed only from S3 API and not from Glacier API Amazon Glacier performs regular systematic data integrity checks and is built to be automatically self-healing  S3 Billing  Storage Number of requests Storage Management Pricing Data Transfer Pricing Transfer Acceleration Cross-Region Replication  Access \u0026amp; Encryption  By default, all newly created buckets are PRIVATE Control access to the buckets using:  Bucket Policies Access Control Lists   Encryption in Transit  SSL/TLS (HTTPS)   Encryption At Rest (Server Side)\n(SSE = Server Side Encryption)  SSE-S3, S3 Managed Keys - AES-256 SSE-KMS, AWS Key Management Service SSE-C, Customer Provided Keys    Versioning  Stores all versions of an object (including all writes and even if you delete an object) Great backup tool Once enabled, Versioning cannot be disabled, only suspended. Integrates with Lifecycle rules Versioning\u0026rsquo;s MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security. Size of the bucket is a sum of all versions of the files stored in the bucket A specific version of the file can be deleted Deletion of a file will place a delete marker  Lifecycle Management Tools \u0026amp; Glacier  Allows you to automate moving your objects between the different storage tiers Can be used in conjunction with versioning Can be applied to current versions and previous versions  Cross Region Replication  Versioning must be enabled on both the source and destination buckets for CRR to work Regions must be unique CRR will not replicate the objects created before the CRR Rule was added Delete markers are not replicated Deleting individual versions or delete markers will not be replicated All subsequently updated files will be replicated automatically  Amazon S3 Transfer Acceleration  takes advantage of CloudFront\u0026rsquo;s globally distributed edge locations can improve upload and access times can be tested using speed comparison tool:\n(http://s3-accelerate-speedtest.s3-accelerate.amazonaws.com)  Using S3 Transfer Acceleration:\n Enable Transfer Acceleration on S3 Bucket Modify Amazon S3 PUT and GET requests to use s3 accelerate endpoint domain name (.s3-accelerate.amazonaws.com) - Regular endpoint will still be accesible Some customers measured performance to exceed 500% percent  Amazon S3 Notifications  Can be issued when certain events happen in your bucket Notifications can be issued to Amazon SQS, SNS Topics and Lambda functions  CloudFront  Edge Location - the location where the content will be cached Origin - the origin of all the files that the CDN will distribute. It can be an S3 Bucket, an EC2 Instance, an Elastic Load Balancer, or Route 53 Distribution - The name that is given to the CDN which consists of a collection of Edge Locations If Edge Location does not have a file in the cache, it will download it from the Origin using optimized networks Objects are cached for the life of the TTL (Time to Live) Edge locations are not just read-only, you can write to them to Types of Distribution supported:  Web Distribution RTMP - Used for Media Streaming   Invalidation  Clears the cache from the Edge Locations   You can invalidate cached objects, but you will be charged  Snowball Petabyte-scale data transporter solution that uses secure appliances to transfer large amounts of data into and out of AWS.\n Snowball  Import to S3 Export from S3 Types  50TB 80TB   Using it can be cheaper than using high-speed internet   Snowball Edge  is a 100TB data transfer device with on-board storage and compute capabilities. Can be used to move large amounts of data into and out of AWS. Applications will continue to run even when they are not able to access the cloud   Snowmobile  Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. Can transfer up to 100 PB per SnowMobile, 45-foot long ruggedized shipping container, pulled by a semi-trailer truck.    Cost Model\n Service Fee (per job) Extra day charges as required (first 10 days of onsite usage are free) Data Transfer  Storage Gateway Connects on-premise software appliance with cloud-based storage to provide seamless and secure integration between an organization\u0026rsquo;s on-premises IT environment and AWS\u0026rsquo;s storage infrastructure.\nThe service enables you to securely store data to the AWS cloud for scalable and cost-effective storage.\nCan be installed as a VM image on a host in a data center. Supports either VMware ESXi or Microsoft Hyper-V hypervisors.\nPhysical appliances are available as well.\nTypes of Storage Gateways:\n File Gateway (NFS)\nFor files: files are stored as objects in your S3 buckets and accessed through an NFS mount point. Ownership, permissions, and timestamps are stored in S3 user-metadata. Volume Gateway (iSCSI)\nAn application can use the disk volumes using iSCSI block protocol.\nData written to the volumes can be asynchronously backed up as point-in-time snapshots of your volumes, and stored in the cloud as Amazon EBS snapshots.\nSnapshots are incremental backups and only changed blocks will be charged.  Stored Volumes\nDAta will be stored locally and asynchronously backed-up to S3 in the form of EBS. (1GB - 16TB volume size) Cached Volumes\nData is stored on AWS S3, while retianing frequently accessed data locally in your storage gateway. This minimizes the need to scale on-premises storage infrastructure, while still providing your applications with low-latency access to their frequently accessed data. (1GB - 32TB volume size)   Tape Gateway (TPL)\nData archiving to AWS Cloud. Lets you leverage your existing tape-based backup application infrastructure to store data on virtual tape cartridges that you create on your tape gateway.\nTape gateway is preconfigured with a media changer and tape drives, which are available to your existing client backup applications as iSCSI devices.  FAQs  The total volume of data is unlimited Individual objects can have a max size of 5Tb Largest object uploaded in a single put is 5Gb For objects larger then 100Mb users should consider using multi-part upload functionality Amazon uses S3 for its developers and a wide variety of projects Amazon S3 is a simple key-based object store. Tags can be added to the objects to organized the data. Pricing components include: storage used, data transfer and data requests Amazon Macie - AI-powered security service that helps you prevent data loss by discovering, classifying, and protecting sensitive data stored in Amazon S3 Amazon S3 uses a combination of Content-MD5 checksums and cyclic redundancy checks (CRCs) to detect data corruption AZs are automatically assigned in Amazon S3 based on the storage class used If the source object is uploaded using the multipart upload feature, then it is replicated using the same number of parts and part size. For example, a 100 GB object uploaded using the multipart upload feature (800 parts of 128 MB each) will incur request cost associated with 802 requests (800 Upload Part requests + 1 Initiate Multipart Upload request + 1 Complete Multipart Upload request) when replicated. You will incur a request charge of $0.00401 (802 requests x $0.005 per 1,000 requests) and a charge of $2.00 ($0.020 per GB transferred x 100 GB) for inter-region data transfer. After replication, the 100 GB will incur storage charges based on the destination region.  "
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/exam-overview/",
	"title": "SAA Exam Overview",
	"tags": [],
	"description": "",
	"content": " Intended for people performing a Solutions Architect role. Valid for 2 years. Question Domains  Design Resilient Architectures Define Performant Solutions Specify Secure Applications and Architectures Design Cost-optimized Architectures Define Operationally Excellent Architectures   Multiple choice questions  single selection multiple selection    Design Resilient Architectures  Choose reliable/resilient storage Determine how to design decoupling mechanisms using AWS services Determine how to design a multi-tier arhictecture solution Determine how to design high availability and/or fault tolerant solutions  High Availability vs Fault Tolerance\n High Availability means that system is up and available but it might perform in degraded state Fault Tolerant is a higher bar - it means that the user does not experience any impact of the fault - the SLA is met  RTO / RPO\n RTO - Recovery Time Objective  how long does it take for system to be back online   RPO - Recovery Point Objective  how much data is lost if the system fails    Design Performant Architectures   Choose performant storage and databases\n  Apply caching to improve performance\n  Design solutions for elasticity and scalability\n  EBS SSD volumes offer better performance then HDD\n  Static content can be offloaded to S3 instead of keeping them on web servers\n  Data Stored on EBS is automatically replicated within an AZ\n  Databases\n Amazon RDS  Complex transactions or complex queries   Amazon DynamoDB  Massive read/write rates Scalability   Amazon Redshift    Useful for analytic queries      Caching\n CloudFront Edge Locations ElasticCache  Memcached  Multithreading, Low maintenance, Horizontal scaling   Redis  Support for data structures, Persistence, Read replicas/failover, Cluster mode/sharded clusters      CloudFront\n Can be used to serve dynamic content with TTL of 0 Improves security, integrates with AWS WAF and AWS Shield Advanced Can serve static content and cache response on the Edge nodes  AutoScaling\n Launch Configuration  EC2 instance type and AMI   Auto Scaling Group  Defines Launch Configuration Min, max and the desired size of the ASG May reference an ELB Health check type   Auto Scaling Policy  Scaling In / Out Uses CloudWatch alarms take an Auto Scaling action   CloudWatch  Monitors CPU, Network, Queue Size CloudWatch Logs, Metrics (Default / Custom)    Secure Architectures Shared Responsibility Model  AWS Responsibility\n(Security of the Cloud)  AWS Global Infrastructure AWS Foundation Services (Compute, Storage, Database, Networking)   Customer Responsibility\n(Their Security in the Cloud)  Client Side Encryption, Server Side Encryption, Network Traffic Protection Operating System, Network and Firewall Configuration Platform, Applications, Identity and Access Management Customer Content    Principle of Least Privilege Granting the minimum access and permissions to a person / system required to perform a certain task\n AWS IAM  central permission management in AWS can be integrated with Active Directory and AWS Directory Service using SAML identity federation    VPC Security  Subnets  Private Subnet\nRoute Table does not have an entry to the Internet Gateway, internet is accessed using the NAT Gateway Public Subnet\nRoute Table has an entry to the Internet Gateway   Security Groups (Apply to ENIs)  Use security group membership to grant access to members of the security group   Network ACLs (Apply to Subnets) VPC Connections  Internet Gateway: Connect to the internet Virtual private gateway: Connect to VPN AWS Direct Connect: Dedicated pipe VPC Peering: Connect to other VPCs NAT gateways: Allow internet traffic from private subnets   Securing Data Tier  Securing data in transit  SSL over web VPN for IPSec IPSec over AWS Direct Connect Import/Export/Snowball  AWS API calls use HTTPS/SSL by default   S3 supports ACL and policies   Securing data at rest  Server-side encryption  Amazon S3-Managed Keys (SSE-S3) KMS-Managed Keys (SSE-KMS) Customer Provided Keys (SSE-C)   Client-side encryption  KMS managed master encryption keys (CSE-KMS) Customer managed master encryption keys (CSE-C)       Storing keys  Key Management Service  Customer software-based key management Integrated with many AWS services Use directly from application Integrates with EBS, S3, RDS, RedShift, Elastic Transcoder, Workmail, EMR   AWS CloudHMS  Hardware based key management Use directly from application FIPS 140-2 compliance      Design Cost-Optimized Architectures  AWS Pricing  Pay as you go Pay less when you reserve Pay even less per unit using more   Fundamental Pricing Characteristics  Compute Pricing  Duration of using an instance Machine configuration Purchase type Number of instances Load balancing Detailed monitoring Auto Scaling Elastic IP Operating system and software packages   Storage Pricing  S3  Storage class Storage Requests Data transfer   EBS  Volumes Input/output operations per second (IOPS) Snapshots Data transfer     Data Transfer Pricing   Serverless Architecture  Reducing cost spent through using Lambda, S3, DynamoDB and API Gateway CloudFront Pricing  Traffic distribution Number of requests Data transfer out      Operational Excellence  Cycle  Prepare Operate Evolve   Design Principles  Perform operations with code Annotate documentation Make frequent, small, reversible changes Refine operations procedures frequently Anticipate failure Learn from all operational failures   Services  AWS Config\nTracks resources such as EBS volumes and EC2 instances, verifies that resources comply with configuration rules AWS CloudFormation\nConverts Yaml and Json templates into cloud resources AWS Trusted Advisor\nChecks account for best practices on security, reliability, performance, cost and service limits AWS Inspector\nChecks EC2 instances for security vulnerabilities VPC Flow Logs\nLogs network traffic AWS Cloud Trail\nLogs API calls AWS CloudWatch\nTracks metrics and triggers alarms when metrics are exceeded    Test Axioms\n Design Resilient Architectures  Expect \u0026lsquo;Single AZ\u0026rsquo; will never be a right answer Using AWS managed services should always be preferred Fault tolerant and high availability are not the same thing Expect that everything will fail at some point and design accordingly   Design Performant Architectures  If data is unstructured, Amazon S3 is the storage solution Use caching to strategically improve performance Know when and why to use Auto Scaling Choose the instance and database type that makes the most sense for your workload and performance need   Secure Architectures  Lock down the root user Security groups only allow. Network ACLs allow explicit deny. Prefer IAM Roles to access keys   Cost-optimized Architectures  Reserve resources to save costs Any unused CPU time is a waste of money Use the most cost-effective data storage service and class Determine the most cost-effective EC2 pricing model and instance type for each workload   Operational Excellence  IAM roles are easier and safer than keys and passwords Monitor metrics across the system Automate responses to metrics where appropriate Provide alerts for anomalous conditions    "
},
{
	"uri": "https://majdarbash.github.io/tags/school-timetabling-problem/",
	"title": "school timetabling problem",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/security/",
	"title": "security",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/aws-security-anipatterns-reinvent/",
	"title": "Security Anti-Patterns: Mistakes to Avoid (re:Invent 2017)",
	"tags": [],
	"description": "",
	"content": " Account Structure Anti-Patterns Network Design Anti-Patterns InfoSec Auditing Example: Amazon RDS At-Rest Encryption Audit Software Delivery Anti-Patterns CFN-NAG  Anti-Pattern: A common response to a recurring problem that is usually ineffective and risks being high counterproductive.\nRisks associated with Security Anti-Patterns\n Lack of SecOps agility  Slow threat assessments Can\u0026rsquo;t patch fast enough Reactive security posture   Lack of business agility  Slow to onboard new customers Hard to practice true DevOps Rogue dev projects    Types of Security Anti-Patterns\n Account Structure Network Design InfoSec Auditing Software Delivery  Account Structure Anti-Patterns  MFA tied to person\u0026rsquo;s mobile phone Root login: one person\u0026rsquo;s inbox If someone leaves the company, this could put the company in trouble No one should log into the account root! Use IAM only! Anti-Pattern: AWS Account Overcrowding  Hard to manage permissions Ambiguous responsibility boundaries Blast radius    Network Design Anti-Patterns  Trusted IP Access w/o Client Auth  Routing is not security Doesn\u0026rsquo;t identity end users Not defense in depth Not highly scalable Best Practice: Implement Authentication and Authorization   Network Egress Backhauling  Some companies send traffic to the data center for traffic inspection proxy Requires AWS Direct Connect Can be solved with having restricted egress via exit VPC  Exit VPC: Pool of EC2 instances running host based filtering controls Can consider other AWS partners providing third-party solutions      InfoSec Auditing  Anti-Pattern: Security Questionnaires  Point-in-time: not continuous Not based on standards No independent verification Not highly scalable   Best Practice: Attestations Instead of Questionnaires  SOC 2, PCI DSS, HIPAA, etc\u0026hellip; Standardized Controls Third-party SQAs verify compliance Recertification cadence   Best Practice: Align with the Standard Controls  Higher priority for overlapping controls among different compliances   Anti-Pattern: Manual Technical Auditing  Manual technical audits Not highly scalable Inconsistent process Typically reactive   Best Practice: Continuous Automated Auditing  DevSecOps: security as code  Proactive controls enforced by code Continuous evidence-based auditing   Continuous detective controls  Amazon CloudWatch Logs + Alarams Amazon Inspectors for EC2 Amazon Macie for Amazon S3 AWS Trusted Advisor AWS Config rules Cloud Conformity Cloud Custodian evident.io Dome9 cfn-nag \u0026hellip; and many more!     Anti-Pattern: Not Using AWS Native Managed Services  Methodology sprawl: audit complications + patch drift   Best Practice: Consistency and Compliance from AWS-Managed Services Best Practice: Train Your Technical Auditors  AWS Auditor Learning Path AWS Tech Essentials Goal: DevSecOps    Example: Amazon RDS At-Rest Encryption Audit import boto3 ec2 = boto3.client(\u0026#39;ec2\u0026#39;) regions = ec2.describe_regions() # Lambda invoked by a CloudWatch Scheduled Event def handler(event, context): # scan each aws region for reg in regions[\u0026#39;Regions\u0026#39;]: # check each RDS instance in region rds = boto3.client(\u0026#39;rds\u0026#39;, region_name = reg[\u0026#39;RegionName\u0026#39;]) try: dbis = rds.describe_db_instances()[\u0026#39;DBInstances\u0026#39;] for dbi in dbis: print \u0026#39;{} {} {}\u0026#39;.format( reg[\u0026#39;RegionName\u0026#39;], dbi[\u0026#39;DBInstanceIdentifier\u0026#39;], dbi[\u0026#39;StorageEncrypted\u0026#39;] ) Software Delivery Anti-Patterns  Anti-Pattern: Over-the-Wall Software Delivery  Dev/QA/Ops are kept in separate teams Manual handoff processes Ci/CD logistically blocked Tight controls and guardrails Post-deployment security checks Infrequent release cycles Infrequent patch rollouts Best-Practice adjust SDLC to include DevSecOps throughout the organization Security tests should be included in every stage Change review process DevSecOps monitoring and alerts      CFN-NAG The cfn-nag tool looks for patterns in CloudFormation templates that may indicate insecure infrastructure. Roughly speaking, it will look for:\n IAM rules that are too permissive (wildcards) Security group rules that are too permissive (wildcards) Access logs that aren\u0026rsquo;t enabled Encryption that isn\u0026rsquo;t enabled Password literals  "
},
{
	"uri": "https://majdarbash.github.io/tags/seo/",
	"title": "seo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/server/",
	"title": "server",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/serverless/",
	"title": "Serverless",
	"tags": [],
	"description": "",
	"content": "Lambda  Compute service to upload and run your code AWS Lambda takes care of provisioning and managing the underlying infrastructure Lambda scales out (not up) automatically Usage  Event-driven compute service which runs code in response to events. Events could be internal AWS events. Compute service to run your code in response to HTTP requests using AWS API Gateway or API calls made using AWS SDKs.   Lambda functions are independent, 1 event = 1 function Lambda is serverless Lambda functions can trigger other lambda functions, 1 event can trigger multiple functions Architectures can get extremely complicated, AWS X-ray allows you to debug what is happening Lambda can do things globally, you can use it to back up S3 buckets to other S3 buckets, etc Lambda is cost effective No servers, no maintenance is required  Lambda is the Ultimate Extraction Layer  Data Centres Hardware Assembly Code/Protocols High-Level Languages Operating Systems Application Layer/AWS APIs AWS Lambda  Languages supported by Lambda  Node.js Java Python C# Go PowerShell  Pricing  Number of Requests Duration\n(from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms)  How to Build Alexa Skill  Skill Service  AWS Lambda   Skill Interface  Invocation Name Intent Schema Slot Type Utterances    Cloud Architecture  Traditional\nELB -\u0026gt; EC2 Instances -\u0026gt; RDS Serverless\nAPI Gateway -\u0026gt; Lambda -\u0026gt; DynamoDB  "
},
{
	"uri": "https://majdarbash.github.io/tags/service/",
	"title": "service",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/shell/",
	"title": "shell",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/software-development/",
	"title": "software development",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/solution-architect/",
	"title": "solution architect",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/squid/",
	"title": "squid",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/subversion/",
	"title": "subversion",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/supervised/",
	"title": "supervised",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/svn-commands/",
	"title": "SVN commands",
	"tags": ["terminal", "shell", "commands", "subversion", ".svn"],
	"description": "",
	"content": "Adding all un-versioned files under the current directory to the SVN:\ncommand 1:\nsvn add `svn status .|grep \u0026quot;^?\u0026quot;|awk '{print $2}'` command 2:\nsvn add . --force Adding specific files to subversion:\nsvn add file1 file2 folder1 path/to/folder2 Commit specific folder in svn:\nsvn ci -m \u0026quot;commit message\u0026quot; file1 file2 folder1 path/to/folder2  Publish Date: 2014-10-12\n "
},
{
	"uri": "https://majdarbash.github.io/tags/symfony/",
	"title": "symfony",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-assets/",
	"title": "Symfony 3: Assets",
	"tags": ["symfony", "assets", "webpack", "encore", "ready"],
	"description": "",
	"content": " Composer manages PHP dependencies NPM or Yarn would be used for Frontend assets  Webpack\nWebpack is a node.js library which can bundle your javascript and CSS files into bundles. You can ready detailed documentation about it on https://webpack.js.org.\nEncore\nEncore is a recommended library which is built on top of webpack. Encore is also written in node.js. This library makes it simpler to integrate webpack into your Symfony application.\n1) Installing the Dependency\ncomposer require symfony/webpack-encore-pack yarn add @symfony/webpack-encore --dev After running this command the following files will be changed:\n./composer.json: \u0026ldquo;symfony/webpack-encore-pack\u0026rdquo; dependency added ./package.json: dev dependencies to @symfony/webpack-encore are added ./webpack.config.js: we will define the build process parameters in this file\n2) Install bootstrap and compiling assets\nyarn add bootstrap@4.0.0 yarn add holderjs yarn add popper.js@1.12.9 yarn add jquery@3.3.1 This command will automatically add yarn.lock file which will be used to lock the versions\nduring when running yarn install command.\n3) Add the installed asset dependencies to the entries of webpack.config.js\nIn addition let\u0026rsquo;s add app.js and app.css files as well.\n./assets/css/app.css (will contain custom styling css) ./assets/js/app.js (will contain custom js code)\nNow let\u0026rsquo;s adjust the webpack.config.js to include all the files required:\n# ./webpack.config.js var Encore = require('@symfony/webpack-encore'); Encore // directory where compiled assets will be stored .setOutputPath('public/build/') // public path used by the web server to access the output path .setPublicPath('/build') .addEntry('js/app', [ './node_modules/jquery/dist/jquery.slim.js', './node_modules/popper.js/dist/popper.min.js', './node_modules/bootstrap/dist/js/bootstrap.min.js', './node_modules/holderjs/holder.min.js', './assets/js/app.js' ]) .addStyleEntry('css/app', [ './node_modules/bootstrap/dist/css/bootstrap.min.css', './assets/css/app.css' ]) .cleanupOutputBeforeBuild() .enableSourceMaps(!Encore.isProduction()) // enables hashed filenames (e.g. app.abc123.css) .enableVersioning(Encore.isProduction()) ; module.exports = Encore.getWebpackConfig(); 4) Compile the assets\n./node_modules/.bin/encore dev # compiling the assets and watch files for changes ./node_modules/.bin/encore dev --watch 5) Including styles and javascript files in the template\n# ./templates/base.html.twig \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;{% block title %}Welcome!{% endblock %}\u0026lt;/title\u0026gt; {% block stylesheets %} \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;{{ asset('build/css/app.css') }}\u0026quot;/\u0026gt; {% endblock %} \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {% block body %}{% endblock %} {% block javascripts %} \u0026lt;script src=\u0026quot;{{ asset('build/js/app.js') }}\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; {% endblock %} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-basics/",
	"title": "Symfony 3: Basics",
	"tags": [],
	"description": "",
	"content": "Installation sudo mkdir -p /usr/local/bin sudo curl -LsS https://symfony.com/installer -o /usr/local/bin/symfony sudo chmod a+x /usr/local/bin/symfony Checking requirements php ./bin/symfony_requirements "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-configuration/",
	"title": "Symfony 3: Configuration",
	"tags": [],
	"description": "",
	"content": "# Will print the bundle names and aliases php bin/console config:dump-reference # Dumps configuration options for specific bundle php bin/console config:dump-reference [extension alias] php bin/console config:dump-reference framework "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-controllers-routing-views/",
	"title": "Symfony 3: Controllers, Routing, Views",
	"tags": ["twig", "template inheritance", "symfony", "service"],
	"description": "",
	"content": "This example demonstrates routing using annotation as well as different ways to pass arguments to the controller.\n\u0026lt;?php // src/Service/TestService.php namespace App\\Service; class TestService { public function message($name) { return \u0026quot;Hello $name\u0026quot;; } } \u0026lt;?php // src/Controller/TestController namespace App\\Controller; use Symfony\\Component\\HttpFoundation\\Request; use Symfony\\Component\\HttpFoundation\\Response; use Symfony\\Component\\Routing\\Annotation\\Route; use App\\Service\\TestService; use Symfony\\Bundle\\FrameworkBundle\\Controller\\AbstractController; class TestController extends AbstractController { /** * @var TestService */ private $testService; /** * TestController constructor. */ public function __construct(TestService $testService) { $this-\u0026gt;testService = $testService; } /** * @Route(\u0026quot;/test1\u0026quot;, name=\u0026quot;test1\u0026quot;) * @param Request $request * @return Response */ public function test1(Request $request) { return new Response($this-\u0026gt;testService-\u0026gt;message( $request-\u0026gt;get('name') )); } /** * @Route(\u0026quot;/test2/{name}\u0026quot;, name=\u0026quot;test2\u0026quot;) * @param $name * @return Response */ public function test2($name) { return new Response($this-\u0026gt;testService-\u0026gt;message($name)); } } Annotation configuration can be done from:\n# config/routes/annotations.yaml controllers: resource: ../../src/Controller/ type: annotation Route annotation set on the class level. Applies defined annotation as prefix to all inner class annotations.\n\u0026lt;?php namespace App\\Controller; use Symfony\\Bundle\\FrameworkBundle\\Controller\\AbstractController; use Symfony\\Component\\HttpFoundation\\Request; use Symfony\\Component\\Routing\\Annotation\\Route; /** * @Route(\u0026quot;/blog\u0026quot;) * Class BlogController * @package App\\Controller */ class BlogController extends AbstractController { public function __construct() { } /** * @Route(\u0026quot;/\u0026quot;, name=\u0026quot;blog_index\u0026quot;) * @param Request $request * @return \\Symfony\\Component\\HttpFoundation\\Response */ public function index(Request $request) { return $this-\u0026gt;render('base.html.twig', ['message' =\u0026gt; 'index page']); } /** * @Route(\u0026quot;/hello\u0026quot;, name=\u0026quot;blog_hello\u0026quot;) * @param Request $request * @return \\Symfony\\Component\\HttpFoundation\\Response */ public function hello(Request $request) { return $this-\u0026gt;render('base.html.twig', ['message' =\u0026gt; 'hello page']); } } The index action in example above will be accessible using /blog/ url, while hello action will require /blog/hello.\nThis example shows the crud for the posts which uses session to store data.\n * @Route(\u0026quot;/blog\u0026quot;) * Class BlogController * @package App\\Controller */ class BlogController extends AbstractController { /** * @var \\Twig_Environment */ private $twig; /** * @var SessionInterface */ private $session; /** * @var RouterInterface */ private $router; public function __construct(\\Twig_Environment $twig, SessionInterface $session, RouterInterface $router) { $this-\u0026gt;twig = $twig; $this-\u0026gt;session = $session; $this-\u0026gt;router = $router; } /** * @Route(\u0026quot;/\u0026quot;, name=\u0026quot;blog_index\u0026quot;) */ public function index() { $html = $this-\u0026gt;twig-\u0026gt;render('blog/index.html.twig', [ 'posts' =\u0026gt; $this-\u0026gt;session-\u0026gt;get('posts') ]); return new Response($html); } /** * @Route(\u0026quot;/add\u0026quot;, name=\u0026quot;blog_add\u0026quot;) */ public function add() { $posts = $this-\u0026gt;session-\u0026gt;get('posts'); $posts[uniqid()] = [ 'title' =\u0026gt; 'Random title' . rand(0, 1000), 'text' =\u0026gt; 'Random text' . rand(0, 1000), ]; $this-\u0026gt;session-\u0026gt;set('posts', $posts); return new RedirectResponse($this-\u0026gt;router-\u0026gt;generate('blog_index')); } /** * @Route(\u0026quot;/show\u0026quot;, name=\u0026quot;blog_show\u0026quot;) */ public function show($id) { $posts = $this-\u0026gt;session-\u0026gt;get('posts'); if (!$posts || !isset($posts['id'])) { throw new NotFoundHttpException('Post not found'); } $html = $this-\u0026gt;twig-\u0026gt;render('blog/post.html.twig', [ 'id' =\u0026gt; $id, 'post' =\u0026gt; $posts[$id] ]); return new Response($html); } } Template Inheritance: Twig parent template can be inherited and reused.\nbase.html.twig\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;{% block title %}Welcome!{% endblock %}\u0026lt;/title\u0026gt; {% block stylesheets %}{% endblock %} \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {% block body %} {{ message }} {% endblock %} {% block javascripts %}{% endblock %} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; # child.html.twig {% extends 'base.html.twig' %} {% block stylesheets %} {{ parent() }} \u0026lt;link type=\u0026quot;text/css\u0026quot; href=\u0026quot;{{ asset('css/child.css') }}\u0026quot; rel=\u0026quot;stylesheet\u0026quot;/\u0026gt; {% endblock %} {% block title %}{{ page_title }}{% endblock %} {% block body %} here we go {% block body %} "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-data-fixtures/",
	"title": "Symfony 3: Data fixtures",
	"tags": ["symfony", "ready", "fixtures", "data fixtures", "doctrine"],
	"description": "",
	"content": "1) Installation\ncomposer require --dev doctrine/doctrine-fixtures-bundle 2) Fixtures class\n\u0026lt;?php # ./src/DataFixtures/PostFixtures.php namespace App\\DataFixtures; use App\\Entity\\Post; use Doctrine\\Bundle\\FixturesBundle\\Fixture; use Doctrine\\Common\\Persistence\\ObjectManager; class PostFixtures extends Fixture { public function load(ObjectManager $manager) { $plan = new Post(); $plan-\u0026gt;setName('My first post'); $plan-\u0026gt;setDescription('This is my first post'); $manager-\u0026gt;persist($plan); $manager-\u0026gt;flush(); } } 3) Loading fixtures\nphp bin/console doctrine:fixtures:load php bin/console doctrine:fixtures:load --purge-with-truncate "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-error-handling/",
	"title": "Symfony 3: Error Handling",
	"tags": [],
	"description": "",
	"content": "throw new NotFoundHttpException('Post not found');\nThis exception will be handled using default exception handler, which will try to locate a template from twig-bundle/Resources/views/Exception/error...html.twig.\nIt will first try to locate a template using the following order:\nerror[error_code].html.twig error.json.twig error.html.twig These templates can be customized when mimic the structure of the bundle templates.\n{# templates/bundles/TwigBundle/Exception/error404.html.twig #} {% extends 'base.html.twig %} {% block body %} \u0026lt;h1\u0026gt;Page not found.\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;{{ status_text }}, {{status_code}}\u0026lt;/p\u0026gt; {% endblock %} "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-flash-messages/",
	"title": "Symfony 3: Flash messages",
	"tags": ["twig", "symfony", "ready", "templates", "flash messages", "error", "notice"],
	"description": "",
	"content": "1) Setting the message\n\u0026lt;?php namespace App\\Controller; use Symfony\\Bundle\\FrameworkBundle\\Controller\\Controller; use Symfony\\Component\\HttpFoundation\\Session\\Flash\\FlashBagInterface; class FlashBagController extends Controller { /** * @var FlashBag */ private $flashBag; public function __construct(FlashBagInterface $flashBag) { $this-\u0026gt;flashBag = $flashBag; } /** * @Route('/index', name=\u0026quot;flash_index\u0026quot;) */ public function index() { return $this-\u0026gt;render('flash/index.html.twig'); } /** * @Route('/setFlash') */ public function setFlashMessage() { $this-\u0026gt;flashBag-\u0026gt;add('notice', 'Here is the flash message.'); $this-\u0026gt;redirectToRoute('flash_index'); } } 2) Reading the message in the template\n{# ./templates/flash/index.html.twig #} {% for message in app.flashes('notice') %} \u0026lt;div class=\u0026quot;alert alert-info\u0026quot;\u0026gt; {{ message }} \u0026lt;/div\u0026gt; {% endfor %} "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-forms/",
	"title": "Symfony 3: Forms",
	"tags": ["symfony", "ready", "form", "form class", "validation", "formBuilder"],
	"description": "",
	"content": "The code-snippets in this article are obtained from: https://symfony.com/doc/current/forms.html\n1) Installation\ncomposer require symfony/form 2) Building form in controller\n\u0026lt;?php // src/Entity/Task.php namespace App\\Entity; class Task { protected $task; protected $dueDate; public function getTask() { return $this-\u0026gt;task; } public function setTask($task) { $this-\u0026gt;task = $task; } public function getDueDate() { return $this-\u0026gt;dueDate; } public function setDueDate(\\DateTime $dueDate = null) { $this-\u0026gt;dueDate = $dueDate; } } \u0026lt;?php // src/Controller/DefaultController.php namespace App\\Controller; use App\\Entity\\Task; use Symfony\\Bundle\\FrameworkBundle\\Controller\\Controller; use Symfony\\Component\\HttpFoundation\\Request; use Symfony\\Component\\Form\\Extension\\Core\\Type\\TextType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\DateType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\SubmitType; class DefaultController extends Controller { public function new(Request $request) { // creates a task and gives it some dummy data for this example  $task = new Task(); $task-\u0026gt;setTask(\u0026#39;Write a blog post\u0026#39;); $task-\u0026gt;setDueDate(new \\DateTime(\u0026#39;tomorrow\u0026#39;)); $form = $this-\u0026gt;createFormBuilder($task) -\u0026gt;add(\u0026#39;task\u0026#39;, TextType::class) -\u0026gt;add(\u0026#39;dueDate\u0026#39;, DateType::class) -\u0026gt;add(\u0026#39;save\u0026#39;, SubmitType::class, array(\u0026#39;label\u0026#39; =\u0026gt; \u0026#39;Create Task\u0026#39;)) -\u0026gt;getForm(); $form-\u0026gt;handleRequest($request); if ($form-\u0026gt;isSubmitted() \u0026amp;amp;\u0026amp;amp; $form-\u0026gt;isValid()) { // $form-\u0026gt;getData() holds the submitted values  // but, the original `$task` variable has also been updated  $task = $form-\u0026gt;getData(); // ... perform some action, such as saving the task to the database  // for example, if Task is a Doctrine entity, save it!  // $entityManager = $this-\u0026gt;getDoctrine()-\u0026gt;getManager();  // $entityManager-\u0026gt;persist($task);  // $entityManager-\u0026gt;flush();  return $this-\u0026gt;redirectToRoute(\u0026#39;task_success\u0026#39;); } return $this-\u0026gt;render(\u0026#39;default/new.html.twig\u0026#39;, array(\u0026#39;form\u0026#39; =\u0026gt; $form-\u0026gt;createView(),)); } } {# templates/default/new.html.twig #} {{ form_start(form) }} {{ form_widget(form) }} {{ form_end(form) }} {# form_start - renders the start tag of the form, with enctype form_widget - fields, validation errors form_end - eng tag of the form + automatic CSRF protection #} \u0026lt;!-- templates/default/new.html.php --\u0026gt; \u0026lt;?php echo $view[\u0026#39;form\u0026#39;]-\u0026gt;start($form) ?\u0026gt; \u0026lt;?php echo $view[\u0026#39;form\u0026#39;]-\u0026gt;widget($form) ?\u0026gt; \u0026lt;?php echo $view[\u0026#39;form\u0026#39;]-\u0026gt;end($form) ?\u0026gt; {% endraw %} 3) Validation\ncomposer require symfony/validator Annotation is used for validation\n\u0026lt;?php // src/Entity/Task.php  namespace App\\Entity; use Symfony\\Component\\Validator\\Constraints as Assert; /** * Class Task * @property string $task * @property \\DateTime $dueDate * @package App\\Entity */ class Task { /** * @Assert\\NotBlank() */ protected $task; /** * @Assert\\NotBlank() * @Assert\\Type(\u0026#34;\\DateTime\u0026#34;) */ protected $dueDate; /** * @return string */ public function getTask(): string { return $this-\u0026gt;task; } /** * @param string $task */ public function setTask($task): void { $this-\u0026gt;task = $task; } /** * @return \\DateTime */ public function getDueDate(): \\DateTime { return $this-\u0026gt;dueDate; } /** * @param \\DateTime $dueDate */ public function setDueDate(\\DateTime $dueDate): void { $this-\u0026gt;dueDate = $dueDate; } } {# templates/default/new.html.twig #} {{ form_start(form, {\u0026#39;attr\u0026#39;: {\u0026#39;novalidate\u0026#39;: \u0026#39;novalidate\u0026#39;}}) }} {{ form_widget(form) }} {{ form_end(form) }} Symfony Field Types https://symfony.com/doc/current/reference/forms/types.html\nPassing options to Fields\n-\u0026gt;add(\u0026#39;dueDate\u0026#39;, DateType::class, array( \u0026#39;widget\u0026#39; =\u0026gt; \u0026#39;single_text\u0026#39;, \u0026#39;required\u0026#39; =\u0026gt; true // applies client-side validation if enabled, \u0026#39;label\u0026#39; =\u0026gt; \u0026#39;Due Date\u0026#39;, )) -\u0026gt;add(\u0026#39;task\u0026#39;, null, array(\u0026#39;attr\u0026#39; =\u0026gt; array(\u0026#39;maxlength\u0026#39; =\u0026gt; 4))) Creating Form Class\n\u0026lt;?php // src/Form/TaskType.php  namespace App\\Form; use App\\Entity\\Task; use Symfony\\Component\\Form\\AbstractType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\CheckboxType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\SubmitType; use Symfony\\Component\\Form\\FormBuilderInterface; use Symfony\\Component\\OptionsResolver\\OptionsResolver; class TaskType extends AbstractType { public function buildForm(FormBuilderInterface $builder, array $options) { $builder -\u0026gt;add(\u0026#39;task\u0026#39;) -\u0026gt;add(\u0026#39;dueDate\u0026#39;, null, [\u0026#39;widget\u0026#39; =\u0026gt; \u0026#39;single_text\u0026#39;]) -\u0026gt;add(\u0026#39;agreeTerms\u0026#39;, CheckboxType::class, [\u0026#39;mapped\u0026#39; =\u0026gt; false]) -\u0026gt;add(\u0026#39;save\u0026#39;, SubmitType::class); } public function configureOptions(OptionsResolver $resolver) { // indicates the type of class which holds data  $resolver-\u0026gt;setDefaults([ \u0026#39;data_class\u0026#39; =\u0026gt; Task::class ]); } } \u0026lt;?php // src/Controller/TaskController.php  namespace App\\Controller; use App\\Form\\TaskType; use Symfony\\Component\\Routing\\Annotation\\Route; use App\\Entity\\Task; use Symfony\\Bundle\\FrameworkBundle\\Controller\\Controller; use Symfony\\Component\\Form\\Extension\\Core\\Type\\DateType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\SubmitType; use Symfony\\Component\\Form\\Extension\\Core\\Type\\TextType; use Symfony\\Component\\HttpFoundation\\Request; class TaskController extends Controller { /** * @Route(\u0026#34;/task/new2\u0026#34;) */ public function newFromFormClass(Request $request) { $task = new Task(); $task-\u0026gt;setTask(\u0026#34;Here is the new task\u0026#34;); $task-\u0026gt;setDueDate(new \\DateTime(\u0026#39;tomorrow\u0026#39;)); $form = $this-\u0026gt;createForm(TaskType::class, $task); $form-\u0026gt;handleRequest($request); if ($form-\u0026gt;isSubmitted() \u0026amp;amp;\u0026amp;amp; $form-\u0026gt;isValid()) { $task = $form-\u0026gt;getData(); echo $form-\u0026gt;get(\u0026#39;agreeTerms\u0026#39;)-\u0026gt;getData(); return $this-\u0026gt;redirectToRoute(\u0026#39;task_success\u0026#39;); } return $this-\u0026gt;render(\u0026#39;task/new.html.twig\u0026#39;, [ \u0026#39;form\u0026#39; =\u0026gt; $form-\u0026gt;createView() ]); } } "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-overview/",
	"title": "Symfony 3: Overview",
	"tags": [],
	"description": "",
	"content": "Symfony is a PHP framework for creating websites and web applications. Symfony provides you with a set of reusable components: Validators, HTTP Kernel, etc. Symfony Embraces best software development practices.\nSymfony was first released in 2005. The framework releases have got LTS - Long Term Support and with the help of the huge community and documentaion you are not alone! What is noticably great is that symfony uses best practices for application development.\nOne of the prominent default symfony components: Twig templating engine, Doctrine 2 (database ORM). You can use any other bundles (libraries) in symfony.\nLet\u0026rsquo;s get some terminology here:\nConsole\nThe Symfony framework provides lots of commands through the bin/console script (e.g. the well-known bin/console cache:clear command). These commands are created with the Console component. You can also use it to create your own commands.\nEvent Dispatcher\nDuring the execution of a Symfony application, lots of event notifications are triggered. Your application can listen to these notifications and respond to them by executing any piece of code.\nBundle\nA bundle is similar to a plugin in other software, but even better. The core features of Symfony framework are implemented with bundles (FrameworkBundle, SecurityBundle, DebugBundle, etc.) They are also used to add new features in your application via third-party bundles.\nService\nYour application is full of useful objects: a \u0026ldquo;Mailer\u0026rdquo; object might help you send emails while another object might help you save things to the database. Almost everything that your app \u0026ldquo;does\u0026rdquo; is actually done by one of these objects. And each time you install a new bundle, you get access to even more!\nService Container\nThis is where your services live. Service container will centralize the way services are constructed.\n"
},
{
	"uri": "https://majdarbash.github.io/symfony/symfony-services/",
	"title": "Symfony 3: Services",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s get some terminology straight.\nService is an object which performs specific function - sending email, caching, etc. Service container will control how services are constructured and are used in Dependency Injection.\nBased on DI practices objects will be passed, which makes the code cleaner and writing tests easier.\n# Displays full list of defined services php bin/console debug:container services.yaml Contains service configuration. Here you will find paths defined to be used as services. In addition you can use services_[env].yaml for environment specific configuration.\nService is a class which represents the business logic. To utilize Dependency Injection, Interfaces classes are passed as arguments to the class constructors. In this aspect symfony will choose appropriate Service which implements an interface and inject this dependency.\n# Displays what service is associated with the interface php bin/console debug:autowiring LoggerInterface # Displays all services and corresponding interfaces php bin/console debug:autowiring # Displays information about the service php bin/console debug:container monolog.logger Auto-Wire Autowiring is responsible for injecting dependencies in your services. By default you will find autowiring enabled (services.yaml). If autowiring is disabled you will have to inject dependencies manually through defining them in services.yaml as:\n[Class name]: [Injected Dependency Name]\n# Injects Greeting service to BlogController App\\Controller\\BlogController: [\u0026#39;@App\\Service\\Greeting\u0026#39;] # Injects Logger service to Greeting service (constructor) App\\Service\\Greeting: [\u0026#39;@monolog.logger\u0026#39;] Auto-Configure When set to true symfony automatically adds tags to classes implementing certain interfaces. In addition symfony automatically registers your services as commands, event subscribers, etc.\nSample services.yaml file:\n# This file is the entry point to configure your own services. # Files in the packages/ subdirectory configure your dependencies. # Put parameters here that don\u0026#39;t need to change on each machine where the app is deployed # https://symfony.com/doc/current/best_practices/configuration.html#application-related-configuration parameters: locale: \u0026#39;en\u0026#39; services: # default configuration for services in *this* file _defaults: autowire: true # Automatically injects dependencies in your services. autoconfigure: true # Automatically registers your services as commands, event subscribers, etc. public: false # Allows optimizing the container by removing unused services; this also means # fetching services directly from the container via $container-\u0026gt;get() won\u0026#39;t work. # The best practice is to be explicit about your dependencies anyway. # makes classes in src/ available to be used as services # this creates a service per class whose id is the fully-qualified class name App\\: resource: \u0026#39;../src/*\u0026#39; exclude: \u0026#39;../src/{Entity,Migrations,Tests,Kernel.php}\u0026#39; # controllers are imported separately to make sure services can be injected # as action arguments even if you don\u0026#39;t extend any base controller class App\\Controller\\: resource: \u0026#39;../src/Controller\u0026#39; tags: [\u0026#39;controller.service_arguments\u0026#39;] # add more service definitions when explicit configuration is needed # please note that last definitions always *replace* previous ones Public / Private Services By default, services are private (public: false).\nIf you try to fetch the service using the container, service should be defined as public. This example is demonstrated as:\n/** * @required */ public function setContainer(ContainerInterface $container = null){ $container-\u0026gt;get(MyService::class); } In such example MyService should be a public service. The better way is just to use dependency injection which will make it possible to inject private services.\nTags Service tags are a way to tell Symfony or other third-party bundles that your service should be registered in specific way. For example, if tagged in with specific way we can fetch all services which are responsible for sending mail, based on the common tag used. Tags can be explicitly defined in services.yaml file under services block.\nservices: Swift_SmtpTransport: tags: [\u0026#39;app.mail_transport\u0026#39;] Swift_SendmailTransport: tags: [\u0026#39;app.mail_transport\u0026#39;] public function process(ContainerBuilder $container) { $taggedServices = $container-\u0026gt;findTaggedServiceIds(\u0026#39;app.mail_transport\u0026#39;); } Another way is when symfony defines the tags based on autoconfigure option being set to true. In such case, command extending from Console\\Command will automatically be tagged with console.command tag, tag makes it available under php bin/console command.\n\u0026lt;?php namespace App\\Command; \u0026lt;/pre\u0026gt; \u0026lt;pre\u0026gt;use Symfony\\Component\\Console\\Command\\Command; class UtilsCommand extends Command { protected function configure(){ $this-\u0026gt;setName(\u0026#39;app:utils\u0026#39;) -\u0026gt;setDescription(\u0026#39;Utils to be run in console\u0026#39;) -\u0026gt;addArgument(\u0026#39;name\u0026#39;, InputArgument::REQUIRED); } .. } # UtilsCommand is tagged with console.command php bin/console debug:container \u0026#39;App\\Command\\UtilsCommand\u0026#39; # app:utils is displayed based on share console.command tag php bin/console Manual service wiring, parameter binding Example below demonstrates wiring $message argument in the Greeting service.\n\u0026lt;?php namespace App\\Service; use Psr\\Log\\LoggerInterface; class Greeting { /** * @var LoggerInterface */ private $logger; /** * @var string */ private $message; public function __construct(LoggerInterface $logger, string $message) { $this-\u0026gt;logger = $logger; $this-\u0026gt;message = $message; } public function greeting(string $name): string { $this-\u0026gt;logger-\u0026gt;info(\u0026#34;{$this-\u0026gt;message}$name\u0026#34;); return \u0026#34;Hello $name\u0026#34;; } }\u0026lt;/pre\u0026gt; \u0026lt;p\u0026gt;Wiring service:\u0026lt;/p\u0026gt; \u0026lt;pre\u0026gt;# config/services.yaml services: App\\Service\\Greeting: arguments: $message: \u0026#39;hello from service\u0026#39; Inject our service using parameter:\n# config/services.yaml parameters: locale: \u0026#39;en\u0026#39; hello_message: \u0026#39;hello from service\u0026#39; services: App\\Service\\Greeting: arguments: $message: \u0026#39;%hello_message%\u0026#39; Binding (in this case all the parameters in any service will be wired automatically):\n# config/services.yaml parameters: locale: \u0026#39;en\u0026#39; hello_message: \u0026#39;hello from service\u0026#39; services: _defaults: autowire: true autoconfigure: true public: false bind: $message: \u0026#39;%hello_message%\u0026#39;\u0026lt;/pre\u0026gt; \u0026lt;p\u0026gt;Another great feature allow wiring based on type:\u0026lt;/p\u0026gt; \u0026lt;pre\u0026gt;# config/services.yaml services: _defaults: autowire: true autoconfigure: true public: false bind: App\\Service\\SomeInterface: \u0026#39;@some_service\u0026#39; "
},
{
	"uri": "https://majdarbash.github.io/symfony/symfonycode-structure/",
	"title": "Symfony 3:[Code Structure]",
	"tags": [],
	"description": "",
	"content": "myproject/ config/ src/ Controller/ Entity/ Form/ Migrations/ Model/ Repository/ Service/ [components] templates/ Description:\nEntity/: Refers to the Persistence Layer\nExample:\nLet\u0026rsquo;s assume we have an entity Product, consisting of id and name.\n1) Start by creating Product directory\nA bundle is simply a structured set of files within a directory that implement a single feature. Each directory contains everything related to that feature, including PHP files, templates, stylesheets, JavaScripts, tests and anything else. Every aspect of a feature exists in a bundle and every feature lives in a bundle.\n"
},
{
	"uri": "https://majdarbash.github.io/symfony/symfonytwig-reference/",
	"title": "Symfony 3:[Twig]: Reference",
	"tags": ["twig", "symfony", "ready", "template", "filter", "twig inheritance", "for loop", "twig filter"],
	"description": "",
	"content": "Template file name\n[template_name].html.twig {# comment #} Template block\n{% block [block_name] %} [block_content] {% endblock %} Extending twig template\n{% extends [template_name] %} Output a variable\n{{ [variable_name] }} For loop\n{% for post in posts %} {{ post.title }}:\u0026amp;nbsp;{{ post.text }} {% endfor %} Generate URL from annotated route name\n{{ path([route_name]) }} Twig Filters\n# Uppercase conversion: {{ post.title|upper }} # Date conversion: {{ post.data|date('H:i:s') }} Environment variables\n{{ app.environment }} {{ app.user }} {{ app.request }} {{ app.session.isStarted() }} {{ app.debug }} Example: Extending Twig - Creating Filter \u0026lt;?php namespace App\\Twig; use Twig\\Extension\\AbstractExtension; use Twig\\TwigFilter; class AppExtension extends AbstractExtension { public function getFilters() { return [ new TwigFilter('price', [$this, 'priceFilter']) ]; } public function priceFilter($number) { return ' The price filter added above will be used as:\n{{ 1029|price }} Extension is automatically configured and \u0026lsquo;twig.extension\u0026rsquo; tag is added based on inheritance from AbstractExtension class (and autoconfigure: true in services.yaml)\nGlobal Variables in Twig\n# config/packages/twig.yaml twig: paths: ['%kernel.project_dir%/templates'] debug: '%kernel.debug%' strict_variables: '%kernel.debug%' globals: message: '%hello_message%' Here %hello_message% is defined as parameter in services.yaml and is not available as global variable \u0026ldquo;message\u0026rdquo; within twig templates.\n. number_format($number, 2, '.', ','); } } \nThe price filter added above will be used as:\nExtension is automatically configured and \u0026lsquo;twig.extension\u0026rsquo; tag is added based on inheritance from AbstractExtension class (and autoconfigure: true in services.yaml)\nGlobal Variables in Twig\nHere %hello_message% is defined as parameter in services.yaml and is not available as global variable \u0026ldquo;message\u0026rdquo; within twig templates.\n"
},
{
	"uri": "https://majdarbash.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/template/",
	"title": "template",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/template-inheritance/",
	"title": "template inheritance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/templates/",
	"title": "templates",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/terminal/",
	"title": "terminal",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/testing/",
	"title": "testing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/the-netflix-simian-army/",
	"title": "The Netflix Simian Army: reliability, security, resiliency and recoverability",
	"tags": [],
	"description": "",
	"content": " Along with price and scalability, redundancy and fault-tolerance are possibly the most important triggers driving cloud migration. The cloud architecture should allow failure without affecting the availability of the entire system. We want to be able to test the failure scenarios.  Chaos Monkey  Randomly disables production instances Testing ability to survive the failure without overall impact on the service Leads to building automatic recovery mechanism to deal with system failures  Latency Monkey  Induces artificial delays to RESTful client-server communication layer to simulate service degradation. Measures if upstream services respond appropriately. Simulate a node or an entire service downtime without physically bringing these instances down.  Conformity Monkey  Finds instances that don\u0026rsquo;t adhere to best-practices and shut them down.  Doctor Monkey  Detecting unhealthy instances using health checks and other external signs of health. Removes unhealthy instances from service.  Janitor Monkey  Searches for unused resources and disposes them.  Security Monkey  Finds security violations and vulnerabilities and terminates the offending instances.  10-18 Monkey (Localization / Internalization)  Detects configuration and run time problems in instances serving customers in different multiple geographic regions.  Chaos Gorilla  Simulates an outage of an entire Amazon availability zone. Services should re-balance to the functional AZs without user-visible impact or manual intervention.  The Simian Army project on Github has retired and the functionality has been moved to other Netflix projects. Check the Simian Army Github page to find more details about hte new projects.\n https://github.com/Netflix/SimianArmy https://github.com/netflix/chaosmonkey https://github.com/spinnaker/swabbie  "
},
{
	"uri": "https://majdarbash.github.io/tags/travis/",
	"title": "travis",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/travis-ci/",
	"title": "Travis CI Review",
	"tags": ["testing", "travis", "ci", "continuous integration", "software development", "review"],
	"description": "",
	"content": "Is Travis CI a good CI tool? Travis CI is a very popular and widely used tool. I was personally using Travis for quite a while and had a good experience.\nThere are few things you should consider before starting with Travis. It\u0026rsquo;s very easy to start with Travis. I cannot imagine a faster integration with a CI tool.\nAll you have to do in order to start is to add your github repo to travis. Hence, you can do this after creating Travis account by authenticating Travis app to access your github repo. From Travis CI settings you will choose if you want travis to run tests for every push and for which branches.\nAll the configurations of the test environment are specified in travis.yml file.\nThe key factor in the pricing model of Travis is the number of concurrent jobs. In terms of price Travis CI is not considered to be the most effective solution, as you will have to spend around 129 / month USD for 2 concurrent jobs (i.e. 2 builds running at the same time: https://travis-ci.com/plans).\nIf you are building an Open Source project, you can still use Travis CI for free, which will navigate you to travis-ci.org (free version of travis-ci.com).\nTravis.yml A sample travis.yml file, would contain information like below:\n# php language is used language: php\nthis will enable artifacts addon, which will be sent to you with every email addons: artifacts: true\nusing php 5.5 version php:\n 5.5  before_script:\nhere you list the commands to be run before the build usually you might want to install some packages and prepare your build for running notifications:\nin this section you would specify different methods how you want to be notified you can choose slack, email and other methods after_failure:\nin this section you might choose to do a certain action after_failure (e.g. upload artifacts, etc\u0026hellip;) As you can see the installation script will take around 30 minutes to setup. However you may not feel that you have a full control of the testing environment configuration, as you will be bound to certain underlying structure provided by Travis CI. Sometimes when the test fails you would like to login to the test instance and investigate the exact cause of the failure - this is another thing you would not be able to enjoy with Travis.\nResources Check out here to find out more information about Travis CI:\nhttp://travis-ci.com\nhttps://travis-ci.com/plans\nhttps://docs.travis-ci.com/user/customizing-the-build/\n"
},
{
	"uri": "https://majdarbash.github.io/tags/twig/",
	"title": "twig",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/twig-filter/",
	"title": "twig filter",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/twig-inheritance/",
	"title": "twig inheritance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/types-of-learning/",
	"title": "types of learning",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/ubuntu/",
	"title": "ubuntu",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/ubuntu-server/",
	"title": "ubuntu server",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/ubuntu-server-clock-synchronisation-using-ntpd/",
	"title": "Ubuntu Server clock synchronisation using NTP",
	"tags": ["ubuntu", "server", "clock"],
	"description": "",
	"content": "NTP is Network Time Protocol. This protocol is used for clock synchronisation between computer systems. NTP is one of the oldest protocols which are still used on the Internet.\nThe scenario below explains the use of NTP:\nWhile the server is running, the system clock will gradually slow or run faster which will lead to clock misalignment. Whether you are running multiple systems over a load balancer, or just multiple systems in parallel, it\u0026rsquo;s very important to make sure that they are all on the same clock, interpreting and processing the records with the same current time. As simple as it sounds before NTP this was a very challenging task.\nThe concept of NTP is to synchronise all computer clocks over some reference point. Having NTPd - being a Network Time Protocol daemon running on all the systems, you will assure that all your system clocks are synchronised against one reference point.\nNTPd installation on Ubuntu / Ubuntu Server 14.04 sudo ntpdate -s ntp.ubuntu.com; sudo apt-get install ntp; sudo service ntp reload; sudo service apache2 reload;\nIf you would like to execute this algorithm for several nodes over the network, you can do as follows:\nsudo ssh -i key.pem ubuntu@machine1 \u0026lsquo;sudo ntpdate -s ntp.ubuntu.com; sudo apt-get install ntp; sudo service ntp reload; sudo service apache2 reload;\u0026rsquo; ;\nsudo ssh -i key.pem ubuntu@machine2 \u0026lsquo;sudo ntpdate -s ntp.ubuntu.com; sudo apt-get install ntp; sudo service ntp reload; sudo service apache2 reload;\u0026rsquo; ;\nsudo ssh -i key.pem ubuntu@machine3 \u0026lsquo;sudo ntpdate -s ntp.ubuntu.com; sudo apt-get install ntp; sudo service ntp reload; sudo service apache2 reload;\u0026rsquo; ;\n"
},
{
	"uri": "https://majdarbash.github.io/random/create-user-using-terminal-ubuntu/",
	"title": "Ubuntu Terminal - adding user with SSH access",
	"tags": [],
	"description": "",
	"content": "Objective In the following example I\u0026rsquo;m doing the following\n create a new user using ubuntu terminal add my public key to authorized keys add user to sudoers (to be able to use sudo) enable ssh access for this user  Scripts # creating user adduser [username]\nadding user to sudo (administrative privelleges) visudo\nsearch for a line \u0026ldquo;root ALL=(ALL:ALL) ALL\u0026rdquo; and add a new line there [username] ALL=(ALL:ALL) ALL\ndelete a user deluser [username]\nwhen deleting the user, the line added in visudo should be also deleted delete home directory of the user along with the account deletion deluser \u0026ndash;remove-home [username]\n# adding user without password, with ssh access and sudo\n1. adding user without password adduser \u0026ndash;disable-password \u0026ndash;gecos \u0026quot;\u0026rdquo; [username]\n2. creating ssh user and supplying the public key mkdir /home/[username]/.ssh touch -f /home/[username]/.ssh/authorized_keys\necho \u0026ldquo;[public key contents]\u0026rdquo; \u0026raquo; /home/[username]/.ssh/authorized_keys\necho \u0026ldquo;AllowUsers [username]\u0026rdquo; \u0026raquo; /etc/ssh/sshd_config sudo service ssh reload\n3. giving sudo access echo \u0026ldquo;[username] ALL=(ALL) NOPASSWD:ALL\u0026rdquo; \u0026raquo; /etc/sudoers.d/[username]\n"
},
{
	"uri": "https://majdarbash.github.io/tags/unsupervised/",
	"title": "unsupervised",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/validation/",
	"title": "validation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/vcs/",
	"title": "vcs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/version-control-system/",
	"title": "version control system",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws-csaa/vpcs/",
	"title": "VPCs",
	"tags": [],
	"description": "",
	"content": "VPC Components  Virtual Private Cloud Subnet Internet Gateway NAT Gateway Virtual Private Gateway Peering Connection VPC Endpoints Egress-only Internet Gateway  VPC (Virtual Private Cloud)  VPC (Virtual Private Cloud) is a virtual private network dedicated to your AWS account. Logically isolated from other virtual networks in AWS Cloud VPC is defined on Region level VPC Tenancy  Default Dedicated   By default, VPC is created with Route Table, Network ACL and Security Group Availability Zones a, b and c are randomized per account Default VPC  All Subnets in default VPC have a route out to the internet Each EC2 instance has both a public and private IP address    Subnet  Subnet is defined on AZ level IP Ranges which can be assigned to Subnets  10.0.0.0-10.255.255.255 (/8 prefix) - largest network 172.16.0.0-172.31.255.255 (/12 prefix) 192.168.0.0-192.168.255.255 (/16 prefix)   5 IP addresses are reserved\n(first four and the last one)  Network address (10.0.0.0) Reserved by AWS for VPC router (10.0.0.1) DNS server IP (10.0.0.2) For future use (10.0.0.3) Network broadcast address - is not supported in VPC, so reserved (10.0.0.255)   Allowed block size is between /16 and /28 netmask AWS Resources can be launched into Subnets Public IPs can be auto-assigned to resources in Subnet  IGW (Internet Gateway)  IGW is a virtual router providing VPC connectivity to the Internet IGW can be attached to VPC You can have only one IGW per VPC  RTB (Route Table)  Route Tables belong to a VPC Route Tables can be associated with Subnets Subnets which are not associated with any Route Table will be associated with the main Route Table It\u0026rsquo;s a better practice to keep the main Route Table as private and create the public Route Tables on demand Route\u0026rsquo;s status \u0026ldquo;blackhole\u0026rdquo; is a route that goes nowhere  VPC Peering  Allows you to connect one VPC with another via a direct network route using private IP addresses Instances behave as if they were on the same private network You can peer VPC\u0026rsquo;s with other AWS accounts as well as with other VPCs in the same account No Transitive Peering - VPC Peering should be defined between each of VPCs that should be connected  NAT Instances, NAT Gateways  NAT = Network Address Translations NAT Instances, NAT Gateways - can provide connectivity to the Internet for private resources, without the need of being public, i.e. having a public IP NAT Instance:  NAT Instance is a single point of failure NAT instance should be in a public subnet The amount of traffic that NAT instances can support depends on the instance size Launching NAT instance  Use EC2 instance with community AMI Disable Source / Destination check\nEach EC2 instance performs source/destination checks by default. This means that the instance must be the source or destination of any traffic it sends or receives. However, a NAT Instance must be able to send and receive traffic when the source or destination is not itself. Therefore, you must disable these checks on the NAT Instance. Add a route in Route Table to provide the Internet connectivity:\n0.0.0.0/0 -\u0026gt; ENI of the NAT Instance (Elastic Network Interface)     NAT Gateway  Created within the Subnet Redundant within the Availability Zone Elastic IP is assigned to the NAT Gateway The Route should be added to Route Table.\n0.0.0.0/0 -\u0026gt; NAT Gateway Starts with a throughput of 5Gbps and scales up to 45Gbps Not associated with Security Groups For Availability Zone-independent architecture, create a NAT gateway in each AZ and configure routing to ensure that resources use the NAT gateway in the same AZ    Network ACLs (Network Access Control Lists)  Network ACLs are created in VPCs Default NACL, created with VPC allows all inbound and outbound traffic By default each custom NACL denies all inbound and outbound traffiic, until you add rules Network ACLs are associated with several Subnets. When associating a network ACL with a Subnet, the previous Subnet\u0026rsquo;s association is removed. Subnet without NACL association will be automatically associated with the default NACL in VPC Recommended adding rules numbers as increments of 100 NACL Rules are applied starting from the lower numbered rule Each Subnet can be associated with only one NACL Inbound and Outbound Rules Ephemeral Ports are allocated automatically and typically used by TCP, UDP as port assignment for the client end of a client-server communication to a well-known port on a server Ephemeral ports should be Allowed in NACL\u0026rsquo;s Outbound Rules for NAT Gateway to work  NAT Gateway uses Ephemeral ports 1024-65535   Network Access Control Lists (ACLs) and Security Groups provide security on different levels Security Groups are stateful\n(Return traffic is automatically allowed, regardless of any rules) Network Access Control Lists are Stateless\n(Return traffic must be explicitly allowed by rules) You can block IP address in NACL - this cannot be done in SGs (Security Groups)  VPC Flow Logs  Enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data is stored using Amaazon CloudWatch Logs. Flow log is created from VPC list by selecting the VPC and choosing \u0026ldquo;Create flow log\u0026rdquo; from the actions You can filter traffic type in Flow Log: Accepted, Rejected or All Traffic Flow Log can send traffic to S3 bucket or CloudWatch Log  Destination Log Group has to be created from CloudWatch / Logs   You cannot enable flow logs for VPCs that are peered with your VPC unless the peer VPC is in your account You cannot tag a flow log After creating a flow log you cannot change its configuration Not all IP Traffic is monitored  Traffic to Amazon DNS server is not logged Traffic generated by a Windows Instance for Amazon Windows license activation Traffic to and from 169.254.169.254 for instance metadata DHCP traffic Traffic to the reserved IP addresses for the default VPC router    Bastion Host Special purpose computer on a network specifically designed and configured to withstand attacks.\n Community AMIs are available for Bastion Hosts A Bastion is used to securely administer EC2 instances (Using SSH or RDP). Bastions are sometimes called Jump Boxes.  Direct Connect AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS.\nIt\u0026rsquo;s a direct connection through dedicated lines from the Customer location to Direct Connect Location in AWS Data Centers.\n Direct Connect directly connects your data center to AWS Useful for high throughput workload Or if you need a stable and reliable secure connection  VPC Endpoints  VPC Endpoint enables you to privately connect your VPC supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT devices, VPN connection or AWS Direct Connect Connection. Traffic between your VPC and the other service does not leave the Amazon network. Two types of VPC Endpoints  Interface Endpoints Gateway Endpoints  S3 DynamoDB     VPN Endpoint can be added from VPC/Endpoints in the management console Endpoints are added in the VPC, with routes being added to the Route Tables.  FAQs  There are no additional charges for creating and using the VPC itself If you connect your VPC to your corporate data center using the optional hardware VPN connection, pricing is per VPN connection-hour Data transfer charges are not incurred when accessing AWS services like Amazon S3 via your VPC\u0026rsquo;s Internet gateway An Internet gateway is horizontally-scaled, redundant and highly available. It imposes no bandwidth constraints. Traffic between two EC2 instances communicating using public IP addresses in the same AWS Region stays within the AWS network. Traffic between two EC2 instances located in 2 different regions with VPC peering stays with the AWS network. Without VPC Peering connection between VPCs traffic is not guaranteed to stay within the AWS network. Site-to-Site VPN connection connects your VPC to your data center. This can be done using IPSec VPN connection. Internet Gateway is not required to establish a Site-to-Site VPN connection. Amazon VPCs support 5 IP ranges: one primary and four secondary IPv4. Each of these ranges can be between /28 and /16 size (in CIDR notation). For IPv6, the VPC is a fixed size of /56 (in CIDR notation). A VPC can have both IPv4 and IPv6 blocks attached to it. Size of the VPC can be changed by expanding your existing VPC and adding the secondary IPv4 ranges. You can create 200 subnets per VPC - the limit can be adjusted through a request. The minimum size of the subnet is /28 (14 IPs v4). For IPv6 the subnet size is fixed to be /64. You can assign one or more secondary private IP address to an Elastic Network Interface (ENI) or an EC2 instance in Amazon VPC. Multiple Elastic IPs (EIP) addresses can be assigned to VPC-based Amazon EC2 instance. Each EIP address will be associated with a unique private IP address on the instance. This is applicable only for IPv4. EIPs for IPv6 are not supported at this time. Multicast and broadcast are not supported by Amazon VPC. If the instances reside in subnets in different Availability Zones, you will be charged $0.01 per GB for data transfer. Elastic Network Interfaces (ENIs) can be attached or detached from an EC2 instance while it\u0026rsquo;s running. Total number of ENIs attached depends on the instance type. ENIs and instances should be in the same AZ and VPC. VPC peers can be done between VPCs in different regions as well as VPCs belonging to different AWS accounts. AWS Direct Connect cannot be used to access VPCs peered with. VPC peering traffic within the region is not encrypted - traffic remains private and isolated. Inter-Region VPC peering traffic is encrypted. Transitive peering relationships are not supported: A-B and B-C peering does not imply A-C peering. Network Load Balancers, AWS PrivateLink, and Elastic File System cannot be used over Inter-Region VPC Peering. AWS PrivateLink and VPC Endpoints is the same thing Bring Your Own IP (BYOIP) enables customers to move all or part of their existing publicly routable IPv4 address space to AWS. They will be able to create EIPs from IP space and associate them with AWS resources. Sometimes it\u0026rsquo;s done for IP reputation, regulation and compliance reasons. Limits  5 VPCs per AWS account per region 200 subnets 5 VPC EIP addresses per AWS account per region 1 Internet Gateway per VPC    "
},
{
	"uri": "https://majdarbash.github.io/tags/vsftpd/",
	"title": "vsftpd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/aws/web-application-hosting-whitepaper/",
	"title": "Web Application Hosting in the AWS Cloud Whitepaper (2019)",
	"tags": ["aws-beginner", "aws", "web-hosting", "overview"],
	"description": "",
	"content": " Benefits of using AWS over a traditional hosting model Classic web application architecture on AWS Cloud Key Components of AWS Web Hosting Architecture Key Considerations When Using AWS for Web Hosting  Benefits of using AWS over a traditional hosting model  A cost effective alternative to oversized fleets needed to handle peaks  Traditional hosting: provision servers to handle peak capacity  Unused cycles are wasted outside of peak periods   AWS auto-scaling based on actual traffic trends will result in less wasted capacity and a greater reduction in cost   A scalable solution to handling unexpected traffic peaks  Traditional hosting: inability to respond in time to unexpected traffic spikes   An On-demand solution for test, load, beta and pre-production environments  Traditional hosting: a lot of expensive hardware sits unused for long periods of time    Classic web application architecture on AWS Cloud  Load Balancing with Elastic Load Balancing (ELB) / Application Load Balancing (ALB) Firewalls with Security Groups Caching with Amazon ElastiCache Managed Database with Amazon RDS DNS Services with Amazon Route 53 Edge Caching with Amazon CloudFront Edge Security for Amazon CloudFront with AWS WAF DDoS Protection with AWS Shield Static Storage and Backup with Amazon S3  Key Components of AWS Web Hosting Architecture  Network Management (VPC, Subnets, NACLs, etc\u0026hellip;) Content Delivery (Amazon CloudFront) Managing Public DNS (Route53) Host Security (EC2 Security Groups) Load Balancing Across Clusters (ALB /ELB / Software load-balancing packages, e.g. Zeus, HAProxy, NGINX Plus) Finding Other Hosts and Services (Dynamic addresses and DNS entries for the resources, Elastic IPs) Caching within the Web Application (Amazon ElastiCache) Database Configuration, Backup and Failover (NoSQL Database - DynamoDB, RDS, and Self-Managed on EC2) Storage and Backup of Data and Assets (Amazon S3, Amazon EBS volumes) Automatically Scaling the Fleet (Auto Scaling Groups - ASGs, integrated with ELBs/ALBs) Additional Security Features (AWS Shield, AWS DDoS Response Team - DRT to mitigate large-scale and sophisticated attacks against your resources, AWS WAF for CloudFront / ALB) Failover with AWS (Multi-AZ, Multi-Region)  Key Considerations When Using AWS for Web Hosting  No more physical Network Appliances Firewalls everywhere Consider the availability of multiple data centers Treat hosts as ephemeral and dynamic Consider a serverless architecture (AWS Lambda, Amazon API Gateway)  "
},
{
	"uri": "https://majdarbash.github.io/tags/web-server/",
	"title": "web server",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/web-hosting/",
	"title": "web-hosting",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/webpack/",
	"title": "webpack",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/random/how-captcha-works/",
	"title": "What is Captcha? How does it work?",
	"tags": [],
	"description": "",
	"content": "In this article I\u0026rsquo;m explaining the generic concept of captcha in technical and non-technical terms, what are the possible threats of insecure captcha and how to make sure the captcha is secure enough.\nCAPTCHA is defined as a Completely Automated Public Turing test. The main purpose of captcha is to distinguish automated requests from natural human behavior.\nBy using the visual ability of people to distinguish patterns in the images, we can distinguish humans from the computers and bad bots which are trying to flood our databases. Captcha should strong enough and should not be solvable by any OCR or image processing system. This will make us confident that even OCR or any other alternative image processing system will not be able to solve our captcha: i.e any automated bad bot won\u0026rsquo;t be able to \u0026ldquo;pretend\u0026rdquo; to be human.\nThe concept behind the captcha is that the server knows what is passed to the client, by saving the captcha in a session or database. The expected input is then embedded to a runtime generated image and passed to the client.\nWhile the user sees the captcha and submits what he sees to the server in his consequent request, the server compares the user input with the stored expected input. In case they don\u0026rsquo;t match the captcha verification fails and the request is rejected.\nBy using this simple mechanism, gaining advantage of human image processing capabilities over most sophisticated image processing softwares, you will be able to provide the level of security required to protect your database and web assets of being misused.\nIn addition to captcha it would be useful to limit the number of requests expected from the client at a period of time to make it more difficult and time consuming for potential attackers to play around with your website.\n"
},
{
	"uri": "https://majdarbash.github.io/tags/whitepaper/",
	"title": "whitepaper",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/wordpress/",
	"title": "wordpress",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://majdarbash.github.io/tags/x-forwarded-for/",
	"title": "X-Forwarded-For",
	"tags": [],
	"description": "",
	"content": ""
}]